{
  "questions": [
    {
      "header": "What is systems design?",
      "content": [
        {
          "type": "text",
          "value": "The process of establishing system aspects such as modules, architecture, components and their interfaces, and data for a system based on specified requirements is known as system design. It is the process of identifying, creating, and designing systems that meet a company's or organization's specific objectives and expectations. \n\nSystems design is more about system analysis, architectural patterns, APIs, design patterns, and glueing it all together than it is about coding. Because your application will be able to handle the architectural load, designing your system adequately for the requirements of your application will eliminate unnecessary costs and maintenance efforts, as well as provide a better experience for your end-users."
        }
      ],
      "answers": [
        "Systems design involves defining the architecture, modules, interfaces, and data for a system to satisfy specified requirements",
        "Systems design solely focuses on the aesthetic aspects of user interfaces to enhance usability, ignoring backend complexities.",
        "System design strictly avoids scalability considerations, assuming that initial deployment capabilities will suffice long-term.",
        "It mandates the use of only the latest, cutting-edge technology stacks, regardless of project requirements or team expertise.",
        "Effective systems design is achievable with a superficial understanding of only one design pattern and no architectural principles.",
        "Systems design dismisses the need for trade-offs, aiming to deliver all features with maximum performance and minimum cost always.",
        "It exclusively involves using non-relational databases for all types of data persistence, regardless of the data's nature or structure.",
        "Designing systems prioritizes low availability and minimal fault tolerance to simplify the development and maintenance processes.",
        "Systems design is a linear process with no need for feedback, as initial requirements are assumed to be perfect and unchanging.",
        "Documentation is considered unnecessary in systems design, as the focus is on rapid development and deployment over future maintainability."
      ]
    },
    {
      "header": "How should systems design interview questions typically be handled?",
      "content": [
        {
          "type": "bullets",
          "values": [
            "1. Clarify Requirements. Start by asking clarifying questions to understand the scope and key requirements of the system you are being asked to design. Determine the system's goals, the number of users it needs to support, the types of users, and any specific functionalities or constraints. Understanding these requirements upfront ensures that you and the interviewer are aligned on the expectations for the system's design.",
            "2. Define System Interfaces. Outline the system's interfaces by defining how users or other systems will interact with it. Specify the input data the system will receive and the expected output. Clearly defining the APIs or user interfaces helps in setting clear boundaries for the system's functionalities.",
            "3. High-Level Design. Sketch a high-level architecture of the system, focusing on the main components and how they interact with each other. This overview should include critical elements like databases, servers, caches, load balancers, and any external services. The goal is to provide a bird's eye view of how the system fits together. Propose this design and go through a few concrete use cases to get buy-in from the interviewer.",
            "4. Deep Dive into Key Components. Choose one or a few components of the system that are crucial for meeting the requirements and delve into their detailed design. Discuss how these components will be implemented, how they will scale, and how they will handle failures. This is your opportunity to showcase your knowledge of various technologies and design patterns.",
            "5. Data Management. Discuss how data will be stored, retrieved, and managed. Consider the choice between SQL and NoSQL databases based on the system's needs, how data consistency will be ensured, and how the system will handle large volumes of scalable data.",
            "6. Scalability and Performance. Explain how your design will scale to accommodate growth in users or data volume. Discuss concepts like horizontal vs. vertical scaling, caching strategies, database sharding, and load balancing. Highlight how your design addresses potential performance bottlenecks.",
            "7. Reliability and Fault Tolerance. Address how the system will ensure high availability and reliability. Discuss strategies for handling system failures, data backups, redundancy, and disaster recovery plans.",
            "8. Security Considerations. Identify potential security risks and how they will be mitigated. Discuss authentication, authorization, data encryption, and secure communication protocols.",
            "9. Wrap-up and Trade-offs. Conclude your design by summarizing the key points and discussing any trade-offs you made in your design choices. It's important to communicate why certain decisions were made and how they balance the system's needs in terms of performance, scalability, and reliability.",
            "10. Review and Feedback. Be open to feedback and questions from the interviewer. They may point out aspects you haven't considered or suggest alternatives. Engaging in a constructive discussion shows your ability to adapt and consider different perspectives."
          ]
        },
        {
          "type": "subheader",
          "value": "Good questions to ask:"
        },
        {
          "type": "bullets",
          "values": [
            "What specific features are we going to build?",
            "How many users does the product have?",
            "Is the product intended exclusively for the web or a mobile app, or both?",
            "How fast does the company anticipate to scale up? What are the anticipated scales in 3 months, 6 months, and a year?",
            "What is the company's technology stack? What existing services you might leverage to simplify the design?"
          ]
        },
        {
          "type": "text",
          "value": "Do not under any circumstances attempt to immediately dive into a concrete solution without proposing a high-level design first."
        }
      ],
      "answers": [
        "Start by clarifying the scope and key requirements of the system to ensure alignment with the interviewer",
        "Immediately propose a detailed database schema as the first step to ground the conversation in specifics.",
        "Focus exclusively on the user interface design, ensuring that the visual aspects are prioritized over system architecture.",
        "Assume scalability and performance are secondary concerns that can be addressed after the initial deployment.",
        "Suggest using only the latest and most complex technologies to impress the interviewer, regardless of the project's needs.",
        "Advocate for a single, large-scale solution to all potential bottlenecks, ignoring modular or incremental improvements.",
        "Propose a system design that prioritizes innovation over reliability, suggesting cutting-edge but untested failure handling strategies.",
        "Overlook security considerations initially, planning to add them as an afterthought once the system is fully designed.",
        "Conclude the discussion without inviting questions or feedback, to appear more confident and decisive.",
        "Dismiss any feedback from the interviewer as outside the scope of the design challenge, to maintain focus on your original solution."
      ]
    },
    {
      "header": "What is CAP theorem?",
      "content": [
        {
          "type": "text",
          "value": "CAP Theorem states that it is not possible for a distributed computer system to simultaneously provide all three of the following guarantees, only two at a time:"
        },
        {
          "type": "bullets",
          "values": [
            "1. Consistency (all nodes see the same data at the same time, even with concurrent updates)",
            "2. Availability (a guarantee that every request receives a response about whether it was successful or failed)",
            "3. Partition tolerance (the system continues to operate despite arbitrary message loss or failure of part of the system)"
          ]
        },
        {
          "type": "text",
          "value": "This theorem has created the base for modern distributed computing approaches. The most high volume traffic companies (e.g. Amazon, Google, Facebook) use this as a basis for deciding their application architecture. It's important to understand that only two of these three conditions can be guaranteed to be met by a system."
        },
        {
          "type": "image",
          "alt": "CAP",
          "path": "../InterviewQuestions/images/systems_design/cap.png"
        }
      ],
      "answers": [
        "CAP theorem states it's impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance.",
        "It suggests that systems can prioritize user interface design over consistency, availability, and partition tolerance for better performance.",
        "CAP theorem indicates that a system's ability to scale horizontally is independent of its consistency and partition tolerance capabilities.",
        "According to the theorem, distributed systems can switch between consistency, availability, and partition tolerance dynamically as needed.",
        "The theorem asserts that consistency and partition tolerance are less critical for systems handling non-sensitive data, focusing instead on availability.",
        "It presumes that modern distributed databases can achieve all three guarantees through innovative synchronization and replication techniques.",
        "CAP theorem has been extended to include a fourth element, security, making it the CAPS theorem, applicable to all distributed systems.",
        "The theorem implies that the choice between consistency, availability, and partition tolerance is permanent and cannot be revised in a system's lifecycle.",
        "According to CAP, the impact of network latency on a system's architecture is negligible, focusing instead on data consistency and system availability.",
        "The theorem is only a theoretical concept and has no practical application in the design and implementation of real-world distributed systems."
      ]
    },
    {
      "header": "What is a rate limiter?",
      "content": [
        {
          "type": "text",
          "value": "In a network system, a rate limiter is used to control the rate of traffic sent by a client or a service. In the HTTP world, a rate limiter limits the number of client requests allowed to be sent over a specified period. If the API request count exceeds the threshold defined by the rate limiter, all the excess calls are blocked. Here are a few examples:"
        },
        {
          "type": "bullets",
          "values": [
            "A user can write no more than 2 posts per second.",
            "You can create a maximum of 10 accounts per day from the same IP address.",
            "You can claim rewards no more than 5 times per week from the same device."
          ]
        },
        {
          "type": "image",
          "alt": "rate_limiter",
          "path": "../InterviewQuestions/images/systems_design/rate_limiter.png"
        },
        {
          "type": "subheader",
          "value": "Benefits"
        },
        {
          "type": "bullets",
          "values": [
            "Prevent resource starvation caused by Denial of Service (DoS) attack",
            "Reduce cost. Limiting excess requests means fewer servers and allocating more resources to high priority APIs. Rate limiting is extremely important for companies that use paid third party APIs that charge per-call.",
            "Prevent servers from being overloaded. To reduce server load, a rate limiter is used to filter out excess requests caused by bots or users' misbehavior."
          ]
        },
        {
          "type": "subheader",
          "value": "Design Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "You can implement a rate limiter on either the client or server-side. But client is generally an unreliable place to enforce rate limiting because client requests can easily be forged by malicious actors. Moreover, we might not have control over the client implementation.",
            "When you implement everything on the server-side, you have full control of the algorithm. However, your choice might be limited if you use a third-party gateway.",
            "Cloud microservices have become widely popular and rate limiting is usually implemented within a component called API gateway. API gateway is a fully managed service that supports rate limiting, SSL termination, authentication, IP whitelisting, servicing static content, etc. Importatnt to know if the API gateway is middleware that supports rate limiting.",
            "If you have already used microservice architecture and included an API gateway in the design to perform authentication, IP whitelisting, etc., adding a rate limiter to the API gateway might be best."
          ]
        },
        {
          "type": "subheader",
          "value": "Rate Limiter Algorithms"
        },
        {
          "type": "bullets",
          "values": [
            "Token bucket: A container that has pre-defined capacity. Tokens are put in the bucket at preset rates periodically. Once the bucket is full, no more tokens are added and are considered 'overflow' that is dropped. Like leaking bucket has two parameters - bucket size and refill rate. Easy to understand and implement, memory efficient, and allows for short traffic bursts. But might be difficult to tune properly.",
            "Leaking bucket: The leaking bucket algorithm is similar to the token bucket except that requests are processed at a fixed rate. It is usually implemented with a first-in-first-out (FIFO) queue. When a request arrives, the system checks if the queue is full. If it is not full, the request is added to the queue. Otherwise, the request is dropped. Requests are polled and processed at regular intervals. Memory efficient given the limited queue size. This is also memory efficient, and requests are processed at a fixed rate therefore it is suitable for use cases where a stable outflow rate is needed. A burst of traffic fills up the queue with old requests, and if they are not processed in time, recent requests will be rate limited. Like token bucket, has only two parameters - bucket size, outflow rate. Once again may be difficult to tune properly.",
            "Fixed window counter: This algorithm divides the timeline into fixed size time 'windows' and assign a counter for each window. Each request increments the counter by one. Once the counter reaches the pre-defined threshold, new requests are dropped until a new time window starts. Memory efficient. Easy to understand. Resetting available quota at the end of a unit time window fits certain use cases. Spike in traffic at the edges of a window could cause more requests than the allowed quota to go through, so may have issues with many traffic bursts.",
            "Sliding window log: The sliding window log algorithm fixes the issue with fixed window edges. The algorithm keeps track of request timestamps. Timestamp data is usually kept in cache, such as sorted sets of Redis. When a new request comes in, remove all the outdated timestamps. Outdated timestamps are defined as those older than the start of the current time window. Add a timestamp of the new request to the log. If the log size is the same or lower than the allowed count, the request is accepted. Otherwise, it is rejected. Rate limiting implemented by this algorithm is very accurate. In any sliding window, requests will not exceed the rate limit. The algorithm consumes a lot of memory because even if a request is rejected, its timestamp might still be stored in memory.",
            "Sliding window counter: The sliding window counter algorithm is a hybrid approach that combines the fixed window counter and sliding window log. The algorithm can be implemented by two different approaches. It smooths out spikes in traffic because the rate is based on the average rate of the previous window and is memory efficient. It only works for a not-so-strict look back window. It is an approximation of the actual rate because it assumes requests in the previous window are evenly distributed. However, this problem may not be as bad as it seems. According to experiments done by Cloudflare, only 0.003% of requests are wrongly allowed or rate limited among 400 million requests."
          ]
        },
        {
          "type": "text",
          "value": "\n\nHigh-level architecture\nWe need a counter to keep track of how many requests are sent from the same user, IP address, etc. If the counter is larger than the limit, the request is disallowed. Using the database is not a good idea due to slowness of disk access. In-memory cache is typically chosen because it is fast and supports time-based expiration strategy. For instance, Redis is a popular option to implement rate limiting. It is an inmemory store that offers two commands: INCR (increases the stored counter by 1) and EXPIRE (sets a timeout for the counter, if the timeout expires the counter is automatically deleted)."
        },
        {
          "type": "text",
          "value": "\n\nIn cases where the client exceeds the rate limit, the server responds with a 429 Too Many Requests status code. Additionally, the response includes the X-Ratelimit-Retry-After header, providing the cooldown period necessary before subsequent requests can be made without triggering throttling. Clients can determine if they are being throttled and track the number of allowable requests before throttling through specific HTTP response headers returned by the rate limiter.:"
        },
        {
          "type": "bullets",
          "values": [
            "X-Ratelimit-Remaining: Indicates the number of remaining requests allowed within the current rate limit window.",
            "X-Ratelimit-Limit: Specifies the total number of requests a client can make per time window.",
            "X-Ratelimit-Retry-After: Tells the client how many seconds to wait before making a new request to avoid being throttled."
          ]
        },
        {
          "type": "subheader",
          "value": "A Detailed Design Deep Dive"
        },
        {
          "type": "image",
          "alt": "rate_limiter2",
          "path": "../InterviewQuestions/images/systems_design/rate_limiter2.png"
        },
        {
          "type": "bullets",
          "values": [
            "Rules are stored on the disk. Workers frequently pull rules from the disk and store them in the cache.",
            "When a client sends a request to the server, the request is sent to the rate limiter middleware first.",
            "Rate limiter middleware loads rules from the cache. It fetches counters and last request timestamp from Redis cache. Based on the response, the rate limiter decides: 1) if the request is not rate limited, it is forwarded to API servers. 2) if the request is rate limited, the rate limiter returns 429 too many requests error to the client. In the meantime, the request is either dropped or forwarded to the queue."
          ]
        },
        {
          "type": "subheader",
          "value": "Pitfalls of Rate Limiting in a Distributed Environment"
        },
        {
          "type": "text",
          "value": "There are two challenges: \n1) Race condition - If two requests concurrently read the counter value before either of them writes the value back, each will increment the counter by one and write it back without checking the other thread. Both requests, which are really threads, believe they have the correct counter value. However, the correct counter value should be +1)\n2) Synchronization - As the web tier is stateless, clients can send requests to a different rate limiter. If no synchronization happens, rate limiter 1 does not contain any data about client 2. Thus, the rate limiter cannot work properly. One possible solution is to use sticky sessions that allow a client to send traffic to the same rate limiter. This solution is not advisable because it is neither scalable nor flexible. A better approach is to use centralized data stores like Redis."
        },
        {
          "type": "subheader",
          "value": "Performance Optimization"
        },
        {
          "type": "text",
          "value": "First, multi-data center setup is crucial for a rate limiter because latency is high for users located far away from the data center. Most cloud service providers build many edge server locations around the world. For example, as of 5/20 2020, Cloudflare has 194 geographically distributed edge servers. Traffic is automatically routed to the closest edge server to reduce latency.\nSecond, synchronize data with an eventual consistency model. The eventual consistency model seeks to achieve high availability that informally guarantees that, if no new updates are made to a given data item, eventually all accesses to that item will return the last updated value. This is suitable for applications where the system can tolerate some degree of inconsistency for a period, such as social networking feeds, collaborative editing platforms, and distributed file systems."
        },
        {
          "type": "subheader",
          "value": "Review"
        },
        {
          "type": "text",
          "value": "Hard vs soft rate limiting. Hard: The number of requests cannot exceed the threshold. Soft: Requests can exceed the threshold for a short period.\n Rate limiting at different levels. This json mostly describes application level rate limiting (layer 7, the highest). It is possible to apply rate limiting at other layers. For example, you can apply rate limiting by IP addresses using Iptables (layer 3).\n Note: The Open Systems Interconnection model (OSI model) has 7 layers: Layer 1: Physical layer, Layer 2: Data link layer, Layer 3: Network layer, Layer 4: Transport layer, Layer 5: Session layer, Layer 6: Presentation layer, Layer 7: Application layer.\nAvoid being rate limited. Design your client with best practices: Use client cache to avoid making frequent API calls, understand the limit and do not send too many requests in a short time frame, include code to catch exceptions or errors so your client can gracefully recover from exceptions. Add sufficient back off time to retry logic."
        }
      ],
      "answers": [
        "A system or mechanism used to control the rate of operations or requests to ensure optimal performance and avoid overloading resources.",
        "Rate limiters prevent users from accessing any network resources once their allocated bandwidth is exceeded, to ensure fair usage.",
        "Rate limiters primarily use artificial intelligence to predict and adjust the limits in real-time based on user behavior patterns.",
        "Implementing a rate limiter is optional in modern systems as advanced networking hardware automatically manages traffic flow efficiently.",
        "Rate limiting can be bypassed by premium users in any system, ensuring they receive uninterrupted service regardless of their usage level.",
        "Rate limiters only monitor and restrict HTTP/HTTPS traffic, having no effect on other protocols such as FTP or SSH.",
        "The primary function of a rate limiter is to enhance the security of a network by preventing unauthorized data exfiltration attempts.",
        "Rate limiters are typically deployed at the application layer only, with no need to consider network or transport layer traffic management.",
        "In distributed systems, rate limiting is managed independently by each service without any need for centralized control or coordination.",
        "Once a rate limiter is configured, it cannot be adjusted or bypassed, making it critical to set limits carefully to avoid hindering legitimate traffic."
      ]
    },
    {
      "header": "What is SSL?",
      "content": [
        {
          "type": "text",
          "value": "SSL, or Secure Sockets Layer, is a cryptographic protocol designed to provide secure communications over a computer network. It's most commonly used on the internet to secure connections between web servers and clients (browsers), ensuring that all data transmitted is encrypted and safe from eavesdropping. The use of SSL certificates, issued by Certificate Authorities (CAs), helps in verifying the authenticity of websites, adding an additional layer of trust for online transactions and information exchange."
        }
      ],
      "answers": [
        "SSL is a protocol for encrypting internet traffic and verifying server identity, ensuring secure online transactions.",
        "SSL is a software package that speeds up the loading time of websites by compressing data before transmission.",
        "SSL stands for Server Software Licensing, a method for web servers to ensure only authorized users can access certain web pages.",
        "SSL is a type of malware designed to steal personal information by posing as legitimate security software.",
        "SSL is an outdated method for creating websites, replaced by modern programming languages like HTML5 and JavaScript."
      ]
    },
    {
      "header": "What is SSL Termination?",
      "content": [
        {
          "type": "text",
          "value": "SSL termination is the process of decrypting SSL/TLS encrypted traffic at the load balancer before it reaches the web server. This setup enables the web server to focus on delivering content rather than encryption, enhancing performance and speed. It's particularly beneficial in environments where high traffic is expected, as it efficiently distributes the load across multiple servers. SSL termination also simplifies SSL certificate management, as only the load balancer needs to have the certificate installed, not every server in the network."
        }
      ],
      "answers": [
        "SSL termination is the process of decrypting SSL/TLS encrypted traffic at the load balancer, enhancing performance and simplifying SSL certificate management.",
        "SSL termination is a security measure that blocks all SSL/TLS encrypted traffic, ensuring only unencrypted data enters the network for easier monitoring.",
        "SSL termination increases the encryption strength of SSL/TLS traffic by re-encrypting data multiple times at the load balancer.",
        "SSL termination requires individual SSL certificates for each server behind the load balancer to maintain a high security standard.",
        "SSL termination is a deprecated technology, replaced entirely by blockchain for securing web traffic."
      ]
    },
    {
      "header": "What is TLS?",
      "content": [
        {
          "type": "text",
          "value": "TLS, or Transport Layer Security, is an encryption protocol that provides security over a computer network. Websites use TLS to secure all communications between their servers and web browsers, ensuring that personal information, such as credit card numbers and login credentials, are transmitted securely. TLS is the updated, more secure version of SSL, and it uses stronger encryption algorithms and provides enhanced security features. Implementing TLS involves obtaining a TLS certificate from a trusted Certificate Authority (CA) and configuring the server to use the certificate for secure communications."
        }
      ],
      "answers": [
        "TLS is a protocol for encrypting data sent over the internet, ensuring secure communications between web servers and clients.",
        "TLS encryption is specifically designed for securing file transfers, rather than web communications.",
        "TLS is an optional security enhancement that can be disabled for improved website performance.",
        "TLS certificates have a fixed encryption key that remains the same for the duration of the certificate's validity.",
        "TLS is a firewall technology used to block malicious internet traffic and protect internal network resources."
      ]
    },
    {
      "header": "What are HTTP Status Codes?",
      "content": [
        {
          "type": "text",
          "value": "HTTP status codes are issued by a server in response to a client's request made to the server. These codes are divided into five categories:"
        },
        {
          "type": "bullets",
          "values": [
            "1xx (Informational): The request was received, continuing process",
            "2xx (Successful): The request was successfully received, understood, and accepted",
            "3xx (Redirection): Further action needs to be taken in order to complete the request",
            "4xx (Client Error): The request contains bad syntax or cannot be fulfilled",
            "5xx (Server Error): The server failed to fulfill an apparently valid request"
          ]
        },
        {
          "type": "text",
          "value": "Common HTTP Status Codes:"
        },
        {
          "type": "bullets",
          "values": [
            "200 OK: The request has succeeded",
            "201 Created: The request has been fulfilled, resulting in the creation of a new resource",
            "301 Moved Permanently: The URL of the requested resource has been changed permanently",
            "400 Bad Request: The server cannot or will not process the request due to an apparent client error",
            "401 Unauthorized: Authentication is required and has failed or has not yet been provided",
            "403 Forbidden: The server understood the request, but is refusing to fulfill it",
            "404 Not Found: The requested resource could not be found but may be available in the future",
            "429 Too Many Requests: The user has sent too many requests in a given amount of time (\"rate limiting\")",
            "500 Internal Server Error: An unexpected condition was encountered",
            "503 Service Unavailable: The server is currently unable to handle the request due to a temporary overloading or maintenance of the server"
          ]
        }
      ],
      "answers": [
        "HTTP status codes provide standardized indications of the outcomes of HTTP requests, helping identify issues and the state of the requested resource.",
        "HTTP status codes above 500 indicate successful data transfer and encryption between the client and server.",
        "HTTP status codes in the 1xx range are used to indicate that a request has been permanently redirected to a new location.",
        "HTTP status codes starting with 4 indicate server-side errors that prevent the server from processing requests.",
        "HTTP status codes are dynamically generated by web browsers to optimize page loading speeds based on user connection quality."
      ]
    },
    {
      "header": "What is load balancing? Why is it important in systems design?",
      "content": [
        {
          "type": "text",
          "value": "Load balancers are systems or devices that distribute incoming network traffic across multiple servers. This distribution ensures that no single server bears too much demand. By spreading the requests across multiple servers, load balancers reduce individual server load, increase the capacity and reliability of applications, and ensure their availability even under high traffic conditions. Load balancers can provide additional security features like SSL termination, where SSL encryption and decryption occur at the load balancer level instead of on individual servers."
        },
        {
          "type": "image",
          "alt": "load balancing",
          "path": "../InterviewQuestions/images/systems_design/load_balancing.png"
        },
        {
          "type": "subheader",
          "value": "How load balancers work"
        },
        {
          "type": "bullets",
          "values": [
            "Traffic Distribution: When a client sends a request to a server, the load balancer receives the request first. Based on its configuration, it selects a server from the pool of available servers to forward the request to.",
            "Health Checks: Load balancers periodically check the health of all servers in the pool to ensure they can handle requests. If a server fails a health check, it is temporarily removed from the pool until it becomes healthy again.",
            "Session Persistence: Some applications require that a user's session be maintained with the same server for the duration of their visit. Load balancers can use various methods to ensure session persistence or 'stickiness'."
          ]
        },
        {
          "type": "subheader",
          "value": "Types of Load Balancing"
        },
        {
          "type": "bullets",
          "values": [
            "Round Robin: Requests are distributed across the servers sequentially.",
            "Least Connections: Requests are sent to the server with the fewest active connections.",
            "IP Hash: The client's IP address is used to determine which server receives the request, ensuring session persistence.",
            "Least Response Time: Requests are sent to the server with the shortest average response time."
          ]
        },
        {
          "type": "subheader",
          "value": "Deployment Models"
        },
        {
          "type": "bullets",
          "values": [
            "Hardware Load Balancers: Physical devices specifically designed for load balancing with built-in optimizations and features.",
            "Software Load Balancers: Applications running on general-purpose hardware, offering flexibility and integration with cloud environments.",
            "Cloud-Based Load Balancers: Provided as part of cloud services, these are fully managed by the cloud provider, offering scalability and integration with cloud resources.",
            "Load balancers are crucial in modern web infrastructure, enabling the efficient management of network resources, improving user experience, and ensuring the resilience and reliability of web applications and services."
          ]
        }
      ],
      "answers": [
        "Load balancing distributes network traffic across multiple servers to ensure no single server becomes overloaded, increasing application reliability and availability.",
        "Load balancing optimizes resource use, improves response times, and maximizes throughput, leading to a more efficient network infrastructure.",
        "Load balancing supports failover, automatically rerouting traffic from failing or overloaded servers to healthy ones to maintain service continuity.",
        "Load balancing enables scalable applications by allowing more servers to be added or removed based on demand, ensuring the system can handle increases in traffic.",
        "Load balancers can provide additional security features such as SSL termination and protection against certain types of attacks.",
        "Load balancing supports session persistence, ensuring that a user's session is maintained with the same server for the duration of their visit for applications requiring it.",
        "Load balancing techniques, like round robin, least connections, and IP hash, offer different methods of distributing traffic based on specific needs.",
        "Deployment models for load balancing include hardware load balancers, software load balancers, and cloud-based load balancers, each offering unique benefits.",
        "In cloud environments, load balancers facilitate auto-scaling by distributing traffic across instances and integrating seamlessly with cloud services.",
        "Load balancing is fundamental in achieving high availability and disaster recovery, ensuring systems remain operational even during outages or maintenance."
      ]
    },
    {
      "header": "What is scalability? How does horizontal scaling differ from vertical scaling?",
      "content": [
        {
          "type": "text",
          "value": "Scalability in systems design refers to the ability of a system, network, or process to handle a growing amount of work, or its potential to be enlarged to accommodate that growth. For a system to be scalable, it must be able to increase its total throughput under an increased load when resources are added. Scalability is a crucial aspect of modern systems and applications, especially those with variable workloads or rapidly growing user bases. \n\nThere are two primary methods to scale a system: horizontal scaling and vertical scaling, each with its own strategies and implications for system design."
        },
        {
          "type": "subheader",
          "value": "Horizontal Scaling"
        },
        {
          "type": "text",
          "value": "Horizontal scaling (or in/out scaling) involves adding more machines or instances to your pool of resources to manage an increased load. It's about scaling out (or in) by increasing the number of nodes in the system, such as adding more servers to a cluster to handle more traffic. This method can significantly increase the system's capacity to handle more requests or data processing tasks. Horizontal scaling is often easier to implement with modern cloud-based services since it usually involves adding more instances of the same type of resource rather than upgrading existing ones. It's highly effective for distributed systems and applications designed to work over multiple machines, like web applications using microservices architecture."
        },
        {
          "type": "subheader",
          "value": "Horizontal Scaling Advantages"
        },
        {
          "type": "bullets",
          "values": [
            "Enhanced fault tolerance and redundancy, as the failure of a single node doesn't necessarily bring down the entire system.",
            "Flexibility in scaling, as resources can be added or removed easily based on demand.",
            "Potentially unlimited scaling, constrained only by the infrastructure provider's capabilities or the architecture's ability to distribute workloads efficiently."
          ]
        },
        {
          "type": "subheader",
          "value": "Vertical Scaling"
        },
        {
          "type": "text",
          "value": "Vertical scaling (scaling up/down), on the other hand, means upgrading the existing machines' capacity within your system. This could involve adding more CPUs, memory, or storage to a server to increase its capability to handle more load. Vertical scaling is often referred to as scaling up (or down) because you're essentially making a single node in the system more powerful. It is a straightforward approach to scaling since it involves enhancing the existing servers' capabilities rather than adding more servers to the system."
        },
        {
          "type": "subheader",
          "value": "Vertical Scaling Advantages"
        },
        {
          "type": "bullets",
          "values": [
            "Simplicity, as it usually involves fewer complexities related to the system architecture changes.",
            "Lower initial complexity for applications not designed to run on multiple machines, since there's no need to manage communication or data consistency across multiple nodes."
          ]
        },
        {
          "type": "text",
          "value": "However, vertical scaling has its limits - there's a maximum to how much you can upgrade a single machine, and at some point, it becomes cost-prohibitive or technically unfeasible to scale up further. Additionally, it introduces a single point of failure; if the upgraded server goes down, the entire service can be affected."
        }
      ],
      "answers": [
        "Scalability is the system's ability to handle increased loads by adding resources, with horizontal scaling adding more machines and vertical scaling upgrading existing ones.",
        "Horizontal scaling refers to decreasing the number of transactions a system can handle to increase individual transaction speed.",
        "Vertical scaling is achieved by spreading out requests evenly over time to prevent peak load times, rather than enhancing server capacity.",
        "In horizontal scaling, the primary method of managing increased load is to reduce the quality of service to maintain performance.",
        "Vertical scaling can indefinitely increase a system's capacity without any limitations, making it the preferred method for long-term growth.",
        "Horizontal scaling increases the complexity of systems by requiring each new machine to perform a different function from the others.",
        "Vertical scaling involves adding more servers but does not require any additional networking infrastructure or configuration.",
        "Horizontal scaling is typically more expensive and less effective than vertical scaling due to the cost of additional hardware.",
        "Vertical scaling, unlike horizontal scaling, improves fault tolerance by ensuring that if one server fails, others are unaffected.",
        "Systems cannot be both horizontally and vertically scaled at the same time; it must be one approach or the other."
      ]
    },
    {
      "header": "What is latency, throughput, and availability of a system?",
      "content": [
        {
          "type": "text",
          "value": "In systems design, latency, throughput, and availability are key performance indicators (KPIs) that measure the efficiency, reliability, and overall performance of a system. "
        },
        {
          "type": "subheader",
          "value": "Latency"
        },
        {
          "type": "text",
          "value": "Latency refers to the time it takes for a system to respond to a request. It's usually measured in milliseconds (ms) and indicates the delay between a user initiating a request and receiving a response. Low latency is crucial for interactive applications where a quick response time is essential for a good user experience, such as online gaming, real-time communications, and high-frequency trading systems."
        },
        {
          "type": "subheader",
          "value": "Throughput"
        },
        {
          "type": "text",
          "value": "Throughput is the amount of work or data processed by a system in a given amount of time. It's often measured in transactions per second (tps), requests per second (rps), or data bits per second (bps). Throughput is an indicator of the capacity of a system to handle load and perform tasks. High throughput is essential for systems that need to process large volumes of data or serve many users simultaneously, such as video streaming services, large-scale web applications, and data processing pipelines."
        },
        {
          "type": "subheader",
          "value": "Availability"
        },
        {
          "type": "text",
          "value": "Availability measures the proportion of time a system is operational and accessible to users. It's typically expressed as a percentage of uptime in the context of the total time considered (usually a year). For example, a system with 99.9% availability is down for no more than 8.76 hours over the course of a year. High availability is critical for systems that provide essential services where downtime can result in significant financial loss, safety issues, or user dissatisfaction. Strategies to improve availability include redundant systems, failover mechanisms, and robust error handling."
        },
        {
          "type": "subheader",
          "value": "Importance in Systems Design:"
        },
        {
          "type": "text",
          "value": "Latency and Throughput: There's often a trade-off between latency and throughput that needs to be balanced based on the application's requirements. For instance, optimizing for low latency might mean processing fewer requests at a time to ensure quick responses, while maximizing throughput might involve batch processing that introduces delays.\nAvailability: Designing for high availability often involves redundancy, geographic distribution of resources, and the ability to quickly recover from failures. This can add complexity and cost to the system design but is necessary for mission-critical applications."
        }
      ],
      "answers": [
        "Latency is the delay in system response to a request, throughput is the rate of processing data, and availability is the system's operational uptime.",
        "Latency measures the system's ability to handle high volumes of data, critical for data analysis applications.",
        "Throughput is determined solely by the speed of the user's internet connection and is independent of the system's configuration.",
        "A system's availability is considered high if it can process requests from multiple users simultaneously without queuing.",
        "Improving latency directly increases a system's throughput because faster responses mean more data can be processed over time.",
        "High availability guarantees that a system will have no downtime, making it immune to failures or maintenance periods.",
        "Throughput optimization involves reducing the quality of service to decrease the amount of data processed, improving speed.",
        "Availability is primarily enhanced by upgrading hardware specifications, such as faster processors and more memory.",
        "The concept of latency is applicable only to networked systems and does not affect standalone or offline applications.",
        "Systems with high throughput inherently have low latency, as processing more data per second means quicker individual responses."
      ]
    },
    {
      "header": "What is performance and scalability, how are they related to each other?",
      "content": [
        {
          "type": "subheader",
          "value": "Performance"
        },
        {
          "type": "text",
          "value": "Performance refers to how efficiently a system processes requests and carries out its operations under a given workload. It is often measured in terms of latency (the time it takes to respond to a request) and throughput (the number of requests a system can handle per unit of time). High performance means that the system is capable of executing tasks and responding to requests quickly and efficiently, leading to a better user experience."
        },
        {
          "type": "subheader",
          "value": "Scalability"
        },
        {
          "type": "text",
          "value": "Scalability, on the other hand, is the ability of a system to maintain or improve its performance when the workload increases. This can involve handling more users, data, transactions, or requests without significant drops in performance. Scalability can be achieved through various means, such as adding resources (hardware or software), optimizing existing resources, or changing the system's architecture. Scalability is often categorized into two types:"
        },
        {
          "type": "bullets",
          "values": [
            "Horizontal scalability (scaling out/in): Adding more machines or instances to a pool to distribute the load more broadly.",
            "Vertical scalability (scaling up/down): Adding more power (e.g., CPU, RAM) to an existing machine to handle more load."
          ]
        },
        {
          "type": "text",
          "value": "Performance and scalability are deeply interconnected. A system's scalability directly impacts its ability to maintain performance under increasing loads. Here's how they relate:"
        },
        {
          "type": "bullets",
          "values": [
            "Foundation for Scalability: Good performance is the starting point for scalability. If a system is not performant at a low scale, simply scaling it up or out may not address the root causes of performance issues. Therefore, optimizing performance is often a prerequisite for effective scalability.",
            "Measuring Success: The success of scalability efforts is often measured by the system's performance under increased loads. If a system can handle more users, data, or transactions while maintaining or improving performance metrics like latency and throughput, it is considered scalable.",
            "Complementary Goals: While performance focuses on efficiency under a specific workload, scalability focuses on the capacity to handle workload increases. Improving one can benefit the other; for example, optimizing code for better performance can reduce the resources required per operation, making it easier to scale the system.",
            "Trade-offs and Balancing Acts: Achieving high performance and scalability may require trade-offs. For instance, adding caching can improve performance but might complicate scaling out if cache coherence becomes an issue. Similarly, scaling out a database can improve scalability but might introduce latency due to data distribution and synchronization issues."
          ]
        },
        {
          "type": "text",
          "value": "In essence, while performance is about how well a system operates at a given scale, scalability is about how well it adapts to increased demands. A well-designed system aims to excel in both areas, ensuring that it not only performs efficiently under current conditions but can also grow to meet future demands without compromising on performance."
        }
      ],
      "answers": [
        "Performance measures system efficiency in processing requests, while scalability is the ability to maintain performance under increased load.",
        "Performance improvements alone can automatically scale a system to handle any amount of increased load without further modifications.",
        "Scalability is only relevant for large-scale systems and has minimal impact on small to medium-sized applications.",
        "Horizontal scaling decreases the overall system performance by introducing more complexity and communication overhead.",
        "A system's performance at low scale is unrelated to its scalability; systems often scale better when starting from a less optimized state.",
        "Scalability efforts are solely measured by the cost-effectiveness of adding resources, rather than performance under load.",
        "Performance optimizations often lead to decreased scalability, as faster systems reach their maximum capacity more quickly.",
        "A system becomes scalable by default when equipped with the maximum available computational resources, regardless of its architecture.",
        "High performance is achieved by maximizing resource utilization, which contradicts the principles of scalability.",
        "Systems designed for scalability do not require performance optimization, as scalability ensures efficient operation under all conditions."
      ]
    },
    {
      "header": "What is consistent hashing? How do you design a consistent hash system?",
      "content": [
        {
          "type": "text",
          "value": "To achieve horizontal scaling in a distributed system, it is important to send and receive requests and data efficiently and evenly across a cluster of nodes (such as caches, databases, or web servers), even as nodes are added or removed. Consistent hashing is a commonly used technique to achieve this goal."
        },
        {
          "type": "text",
          "value": "\n\nIf you have n cache servers, a common way to balance the load is to use the following hash method: `serverIndex = hash(key) % N`, where N is the size of the server pool. This approach works well when the size of the server pool is fixed and data distribution is relatively even, but very problematic when servers are added or removed, since the same hash will return incorrect an server:"
        },
        {
          "type": "image",
          "alt": "consistent hashing",
          "path": "../InterviewQuestions/images/systems_design/consistent_hashing.png"
        },
        {
          "type": "text",
          "value": "Instead, we need a design that does not depend directly on the number of servers, so that when adding or removing servers, the number of keys that need to be relocated is minimized. MIT found a solution to this problem in 1997 called consistent hashing.\n\n Consistent hashing involves the use of an abstract circle, or 'ring.' Imagine we mapped the hash output range onto the edge of a circle. That means that the minimum possible hash value, zero, would correspond to an angle of zero, the maximum possible value (some big integer we'll call INT_MAX) would correspond to an angle of 2(pi) radians (or 360 degrees). To find out which server to ask for a given key, we need to locate the key on the ring and move clockwise (or counter-clockwise depending on the system configuration) direction until we find a server. From a programming perspective, what we would do is keep a sorted list of server values (which could be angles or numbers in any real interval), and walk this list (or use a binary search) to find the first server with a value greater than, or equal to, that of the desired key. If no such value is found, we need to wrap around, taking the first one from the list."
        },
        {
          "type": "image",
          "alt": "consistent hashing",
          "path": "../InterviewQuestions/images/systems_design/consistent_hashing2.png"
        },
        {
          "type": "text",
          "value": "\nWhat's the benefit of this ring approach? Imagine server C is removed. To account for this, we must remove labels C0 ... C9 from the circle. This results in the object keys formerly adjacent to the deleted labels now being randomly labeled Ax and Bx, reassigning them to servers A and B.\n\nBut what happens with the other object keys, the ones that originally belonged in A and B? Nothing! That's the beauty of it: The absence of Cx labels does not affect those keys in any way. So, removing a server results in its object keys being randomly reassigned to the rest of the servers, leaving all other keys untouched: \nThis is how consistent hashing solves the rehashing problem. In general, only k/N keys need to be remapped where k is the number of keys and N is the number of servers (more specifically, the maximum of the initial and final number of servers). "
        },
        {
          "type": "text",
          "value": "These artificial 'labels' on the key ring are known as virtual nodes. Virtual nodes allow each physical node to be represented by any location on the hash ring. In general, the number of virtual nodes for a server is proportional to the server's capacity at a given moment in time, servers with higher capacity are assigned more virtual nodes. This reduces the likelihood of hotspots where one server is significantly more loaded than others.\n\nRequired Features"
        },
        {
          "type": "bullets",
          "values": [
            "Distribution uniformity to ensure keys are evenly spread across the available nodes or buckets.",
            "Scalability to easily add or remove nodes without significantly disrupting the key distribution.",
            "Replication support to enhance data availability and fault tolerance.",
            "Flexibility to accommodate various types of workloads and data distributions.",
            "Minimal overhead for key lookup and node selection to maintain high performance."
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Hot spots, where some nodes receive significantly more load than others, leading to uneven load distribution.",
            "Handling node failures and ensuring the consistent hashing mechanism quickly adapts to changes in the node set.",
            "Maintaining data consistency and availability during node addition or removal.",
            "Complexity in implementing a consistent hashing mechanism that accurately reflects the theoretical model."
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Use virtual nodes (vnodes) to improve distribution uniformity and ease the handling of node addition/removal.",
            "Implement automatic data replication and rebalancing mechanisms to maintain data availability and consistency.",
            "Design a robust failure detection and recovery process to quickly adapt to changes in the cluster's state.",
            "Simplify the consistent hashing algorithm's implementation by leveraging existing libraries and frameworks that have been tested and optimized."
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Determining the optimal number of virtual nodes to balance between load distribution and management overhead.",
            "Choosing the right hashing function that minimizes collisions and provides a good distribution of keys.",
            "Integrating with other system components, such as load balancers and caching layers.",
            "Assessing the impact of consistent hashing on application-specific requirements, such as query latency and data locality."
          ]
        },
        {
          "type": "text",
          "value": "\n\nThere are clients for several systems, such as Memcached and Redis, that include support for consistent hashing out of the box. Some types of NoSQL databases rely on distributed hashing as well."
        }
      ],
      "answers": [
        "Consistent hashing minimizes redistribution of keys when nodes are added or removed, improving system scalability and performance.",
        "Virtual nodes increase the complexity of node management, requiring additional computational resources for each node in a distributed system.",
        "Data replication and rebalancing negatively impact system performance due to the additional overhead introduced by consistency mechanisms.",
        "Failure detection and recovery mechanisms significantly slow down system responses as they require constant monitoring and checking.",
        "Existing libraries for consistent hashing often lead to increased system vulnerability due to common security flaws within the libraries.",
        "The choice of hashing function has little impact on the overall performance and scalability of distributed systems.",
        "The number of virtual nodes in a system should be minimized to reduce the complexity of the consistent hashing algorithm.",
        "Integration with load balancers and caching layers complicates the implementation of consistent hashing, often leading to inefficiencies.",
        "Consistent hashing simplifies the system architecture, making it unnecessary to consider application-specific performance requirements."
      ]
    },
    {
      "header": "What is partitioning and what is sharding? How do they differ?",
      "content": [
        {
          "type": "text",
          "value": "Sharding and partitioning are both techniques used to distribute data across multiple databases or tables to improve scalability, performance, and manageability of large-scale database systems. While they share a common goal of dividing data to manage it more effectively, there are distinctions in their approaches and use cases."
        },
        {
          "type": "subheader",
          "value": "Partitioning"
        },
        {
          "type": "text",
          "value": "Partitioning typically refers to dividing a database or table into smaller segments or partitions, often within a single database server. The main goal of partitioning is to improve manageability, performance, and efficiency of data retrieval. Data can be partitioned in various ways, such as by range (e.g., date ranges), list (e.g., category), or hash (based on a hash function applied to a partition key).\n\nKey characteristics of partitioning include:"
        },
        {
          "type": "bullets",
          "values": [
            "Partitions are usually contained within a single database system, making it easier to manage than sharded setups.",
            "It is primarily used to improve query performance, especially for read-heavy operations, by reducing the amount of data scanned during query execution.",
            "Partitioning can be transparent to the application, as the database management system (DBMS) can handle data distribution and retrieval across partitions without requiring application-level changes."
          ]
        },
        {
          "type": "subheader",
          "value": "Sharding"
        },
        {
          "type": "text",
          "value": "Sharding is a type of database partitioning that spreads data across multiple servers or database instances, effectively breaking up a large database into smaller, more manageable pieces called shards. Each shard holds a subset of the data, and the collection of shards make up the entire dataset. Sharding is primarily used to scale out applications horizontally, allowing for more data to be handled by adding more servers. It can significantly improve performance and reduce the load on any single server, as operations can be executed in parallel across shards.\n\nKey characteristics of sharding include:"
        },
        {
          "type": "bullets",
          "values": [
            "Shards are distributed across different physical or virtual servers, often using a shard key to determine how data is distributed.",
            "It is primarily used to improve performance and scalability of write-heavy database operations by distributing the load.",
            "Sharding can be complex to implement and manage, as it often requires changes to application logic to handle data distribution and aggregation from multiple shards."
          ]
        },
        {
          "type": "subheader",
          "value": "Differences Between Sharding and Partitioning"
        },
        {
          "type": "bullets",
          "values": [
            "Scope of Distribution: Sharding distributes data across multiple servers or database instances, while partitioning divides data within a single database or table.",
            "Purpose and Use Case: Sharding is used to scale databases horizontally by adding more servers to handle increased load, particularly for write operations. Partitioning is used to improve the manageability and query performance within a single database, especially for read operations.",
            "Complexity and Management: Sharding tends to be more complex to implement and manage, as it requires careful planning of data distribution strategies and might necessitate application changes. Partitioning is often managed by the DBMS, requiring less direct intervention and being more transparent to applications.",
            "Performance Optimization: Both techniques aim to optimize performance, but they do so in different ways and for different types of operations (e.g., sharding for writes, partitioning for reads)."
          ]
        }
      ],
      "answers": [
        "Sharding distributes data across multiple servers to improve scalability and performance for write-heavy operations, while partitioning divides data within a single server to enhance query performance and manageability for read-heavy operations.",
        "Partitioning aims to distribute computational load evenly across a network, often leading to increased latency due to network congestion.",
        "Sharding reduces the security of a database system by spreading data across multiple servers, making it more vulnerable to attacks.",
        "The primary goal of partitioning is to decrease database storage requirements by compressing data within partitions.",
        "Sharding simplifies database management by automatically synchronizing data across shards, eliminating the need for manual intervention.",
        "Partitioning involves manually updating application logic to direct queries to the correct partition, increasing development complexity.",
        "Sharding decreases the overall cost of database management by reducing the need for high-performance hardware.",
        "The use of partitioning is limited to relational databases and is not applicable to NoSQL or in-memory database systems.",
        "Sharding is a recent technological advancement and is not supported by older database management systems.",
        "Partitioning enhances data security by encrypting each partition with a different encryption key, though this is not its primary purpose."
      ]
    },
    {
      "header": "What is caching, what are some update strategies associated with caching?",
      "content": [
        {
          "type": "text",
          "value": "Caching is a technique used to store copies of data or computational results in a temporary storage area (the cache), so future requests for that data can be served faster. The cache is typically faster to access than the original data source (like a database or the internet), making caching an effective way to speed up data retrieval, reduce latency, and improve the overall performance of a system. Caching can be implemented at various levels, including hardware (like CPU caches), software (like web browser caches), and within application architectures (like web application caching layers)."
        },
        {
          "type": "text",
          "value": "\n\nUpdate Strategies for Caching\nThe effectiveness of a cache depends not just on its ability to store data, but also on how it is kept up-to-date with the underlying data source. Several strategies are employed to manage cache updates, each with its trade-offs in terms of consistency, performance, and complexity:"
        },
        {
          "type": "text",
          "value": "\n\n1. Write-Through Cache\nIn a write-through cache strategy, data is written into the cache and the corresponding database or storage system simultaneously. This approach ensures that the cache always contains the most recent data, providing strong consistency between the cache and the storage. However, write operations may be slower, as they only complete once the data has been written to both the cache and the database."
        },
        {
          "type": "text",
          "value": "\n\n2. Write-Around Cache\nWith write-around caching, data is written directly to the persistent storage, bypassing the cache. This method can prevent the cache from being filled with data that might not be read again, conserving cache space for more frequently accessed data. The drawback is that subsequent reads of newly written data will not benefit from the cache and will have to access the slower backend storage, potentially increasing read latency for those items."
        },
        {
          "type": "text",
          "value": "\n\n3. Write-Back Cache\nIn a write-back (or write-behind) strategy, data is initially written to the cache alone and then written to the storage system at a later time. This approach can significantly speed up write operations since they complete as soon as the data is written to the cache. The downside is the risk of data loss if the cache data has not been persisted to the storage system before a failure occurs. Additionally, it introduces complexity in ensuring data consistency between the cache and the storage."
        },
        {
          "type": "text",
          "value": "\n\n4. Cache Invalidation\nCache invalidation is the process of removing outdated data from the cache. This can be challenging to implement correctly, as it requires the system to know when data has changed in a way that makes the cached version obsolete. Strategies for cache invalidation include:\n- Time-based expiration, where cache entries are invalidated after a specified duration.\n- Event-driven invalidation, where updates to the data source trigger cache invalidation."
        },
        {
          "type": "text",
          "value": "\n\n5. Cache Refreshing\nCache refreshing involves updating the cache with the latest data at fixed intervals or on demand. This strategy can be used to ensure that the cache reflects recent changes in the data source, even if not every write operation triggers an immediate cache update."
        },
        {
          "type": "text",
          "value": "\n\nChoosing the right cache update strategy involves balancing the need for data consistency, the performance impact of cache operations, and the complexity of implementing and managing the caching logic. The best approach depends on the specific requirements and characteristics of the application, such as the acceptable staleness of data, read-to-write ratio, and the criticality of operations."
        }
      ],
      "answers": [
        "Caching stores copies of data or computational results temporarily to speed up future data retrieval, reduce latency, and improve system performance.",
        "A Write-Through Cache accelerates all forms of data operations by caching every request, regardless of frequency or recency.",
        "Write-Around Caching strategy enhances the cache's performance for both read and write operations by storing all data directly in the cache first.",
        "In the Write-Back Cache approach, data is immediately persisted to storage before being cached, ensuring no write operation latency.",
        "Cache Invalidation continuously updates cached data in real-time, mirroring every change in the database instantly without any delay.",
        "Cache Refreshing involves manually clearing and repopulating the cache at regular intervals, typically every few minutes, to ensure data freshness.",
        "Selecting a caching strategy is primarily a function of the storage capacity of the cache, with larger caches favoring more complex invalidation strategies."
      ]
    },
    {
      "header": "What is consistency?",
      "content": [
        {
          "type": "text",
          "value": "Consistency in the context of distributed systems refers to how a system guarantees that data remains accurate, synchronized, and reliable across multiple nodes or components, despite potential challenges like network failures, concurrent access, or data replication delays. Achieving consistency in such environments is crucial for maintaining the integrity and dependability of data operations. There are several patterns and models designed to manage consistency, each balancing the trade-offs between availability, consistency, and partition tolerance, famously summarized by the CAP theorem."
        },
        {
          "type": "subheader",
          "value": "Consistency Models"
        },
        {
          "type": "bullets",
          "values": [
            "1. Strong Consistency (Immediate Consistency). Strong consistency ensures that any read operation retrieves the most recent write operation's result, guaranteeing that all users see the same data at the same time. This model is simple to understand and reason about from a developer's perspective but can be challenging to achieve in distributed systems without impacting availability or performance due to the need for synchronous updates across all nodes.",
            "2. Eventual Consistency. Eventual consistency is a weaker consistency model where the system guarantees that if no new updates are made to a given data item, eventually all accesses to that item will return the same value. It is a common model in distributed systems that prioritize availability and partition tolerance over immediate consistency, allowing for temporary discrepancies between nodes with the understanding that they will become consistent over time.",
            "3. Causal Consistency. Causal consistency ensures that causally related updates (where one operation depends on the result of another) are seen by all nodes in the same order, while concurrent updates (independent from each other) may be seen in a different order on different nodes. This model provides a balance between strict consistency requirements and the flexibility needed in distributed environments.",
            "4. Read-Your-Writes Consistency. This consistency pattern guarantees that a process that writes data will always read its own writes. It's particularly important in user-facing applications where users expect to see the results of their actions immediately reflected in the system.",
            "5. Session Consistency. Session consistency is a variation of read-your-writes consistency extended to a session context, where a system ensures consistency within the context of a user session. It guarantees that within a session, all reads will reflect writes that happened in the same session, even if they might not be immediately visible outside the session.",
            "6. Monotonic Read Consistency. Monotonic read consistency ensures that if a read operation returns a particular value, any subsequent read operations will never return an older value. This model helps prevent regressions in data seen by the client but does not guarantee immediate visibility of the latest writes.",
            "7. Monotonic Write Consistency. This pattern ensures that writes from a single source are applied in the order they were issued, which is crucial for maintaining the logical sequence of operations. It does not, however, guarantee immediate consistency across the system."
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "text",
          "value": "The choice of consistency model has significant implications for system design, particularly in balancing the trade-offs between consistency (C), availability (A), and partition tolerance (P) as outlined by the CAP theorem. Systems often need to prioritize two out of these three properties, leading to different consistency patterns being more suitable for some applications than others. The decision largely depends on the specific requirements of the application, such as the need for real-time data accuracy versus the need for high availability and fault tolerance."
        }
      ],
      "answers": [
        "Strong Consistency ensures any read operation retrieves the most recent write operation's result, guaranteeing simultaneous data visibility across all users.",
        "Eventual Consistency prioritizes immediate system availability over data accuracy, often resulting in faster write operations but slower read operations.",
        "Causal Consistency allows for immediate visibility of all writes across the system, regardless of the network latency or failures.",
        "Read-Your-Writes Consistency can significantly reduce system performance due to the overhead of tracking each operation's causal relationships.",
        "Session Consistency requires each user session to be handled by the same server node to ensure data consistency, limiting the system's scalability.",
        "Monotonic Read Consistency ensures that once a write operation is performed, all subsequent operations are paused until the write is globally visible.",
        "Monotonic Write Consistency simplifies transaction management by allowing operations to execute in any order without affecting the system's consistency.",
        "Choosing a consistency model is solely based on system performance considerations, with availability and partition tolerance being secondary concerns."
      ]
    },
    {
      "header": "What is a CDN?",
      "content": [
        {
          "type": "text",
          "value": "A Content Delivery Network (CDN) is a distributed network of servers designed to deliver web content and pages to users based on their geographic locations, the origin of the webpage, and a content delivery server. CDNs are used to efficiently distribute the load of delivering content, reduce bandwidth costs, improve web page load times, and increase content availability and redundancy."
        },
        {
          "type": "subheader",
          "value": "How CDNs Work"
        },
        {
          "type": "text",
          "value": "CDNs store a cached version of its content in multiple geographical locations, known as \"points of presence\" (PoPs). Each PoP contains a number of caching servers responsible for content delivery to visitors within its proximity. When a user requests a webpage or other content, the request is redirected to the closest server geographically, rather than fetching the content from the website's central server. This significantly reduces the distance the data travels, decreasing latency, improving site speed, and providing a better user experience."
        },
        {
          "type": "subheader",
          "value": "Key Features of CDNs"
        },
        {
          "type": "bullets",
          "values": [
            "Reduced Latency: By serving content from locations closer to the user, CDNs reduce the time it takes for data to travel, thus reducing latency.",
            "Scalability: CDNs can handle sudden spikes in traffic by distributing the load across multiple servers, making websites more scalable and reliable.",
            "Content Availability and Redundancy: CDNs can reroute traffic in case of server failure, enhancing content availability and providing redundancy.",
            "Optimized Bandwidth: Through caching and other optimizations, CDNs can reduce the amount of data transferred, lowering hosting costs and improving website load times.",
            "Security: CDNs can improve security by providing DDoS protection, improving security certificates management, and other security features to safeguard websites against attacks."
          ]
        },
        {
          "type": "subheader",
          "value": "Types of CDN Content"
        },
        {
          "type": "bullets",
          "values": [
            "Web objects (text, graphics, and scripts)",
            "Downloadable objects (media files, software, documents)",
            "Applications (e-commerce, portals)",
            "Live streaming media, on-demand video, and social networks",
            "And more"
          ]
        },
        {
          "type": "subheader",
          "value": "Use Cases"
        },
        {
          "type": "text",
          "value": "CDNs are used across various types of websites and applications, especially those requiring high availability and performance, such as:"
        },
        {
          "type": "bullets",
          "values": [
            "E-commerce sites, to ensure fast loading times during high traffic periods.",
            "News and media websites, to handle the distribution of large video files and streams.",
            "Online gaming, to reduce latency for a better user experience.",
            "Software distribution, for efficient delivery of large files to a global audience."
          ]
        }
      ],
      "answers": [
        "A CDN reduces latency by serving content from the closest server to the user's geographic location, enhancing site speed and user experience.",
        "By using multiple servers, CDNs prevent server overload by equally distributing user requests, ensuring no single server's performance is compromised.",
        "CDNs primarily reduce website operating costs by decreasing the need for expensive web hosting solutions.",
        "Optimizing bandwidth through CDNs can significantly increase a website's ranking on search engine results pages (SERPs) due to improved load times.",
        "Security enhancements provided by CDNs include automatic encryption of all transferred data, making the internet a safer place by default.",
        "The versatility of CDNs allows for the instantaneous update of content across all servers, ensuring all users access the most current version of the website.",
        "CDNs are particularly beneficial for websites that do not have international traffic, as they can significantly reduce the complexity of global content delivery."
      ]
    },
    {
      "header": "What is leader election?",
      "content": [
        {
          "type": "text",
          "value": "Leader election is a coordination mechanism used in distributed systems to ensure that among multiple nodes or processes, one node is designated as the \"leader\" at any given time. This leader node is responsible for managing specific tasks, such as coordinating access to a shared resource, making decisions on behalf of the cluster, or handling read and write requests to ensure data consistency.\n\nThe need for leader election arises in scenarios where having multiple nodes perform the same operation could lead to conflicts, inconsistencies, or inefficiencies. By electing a leader, the system ensures that only one node is in charge of critical operations, simplifying the management of distributed tasks and reducing the likelihood of errors."
        },
        {
          "type": "subheader",
          "value": "How Leader Election Usually Works"
        },
        {
          "type": "bullets",
          "values": [
            "Initiation: The election process starts when there's no current leader, perhaps due to the system starting up or the previous leader failing or stepping down.",
            "Election: Nodes in the system communicate with each other using a predefined protocol to nominate and elect a new leader. This can be based on node IDs, timestamps, random choice, or other criteria that ensure a fair and decisive election.",
            "Declaration: Once a leader is elected, the new leader announces its role to the other nodes, and those nodes acknowledge the leader's authority.",
            "Operation: The leader then performs its designated tasks, while other nodes may take on worker roles or standby in a ready state to take over leadership if needed.",
            "Monitoring and Failover: The system continuously monitors the health of the leader. If the leader fails or becomes unresponsive, the election process is reinitiated to select a new leader."
          ]
        },
        {
          "type": "subheader",
          "value": "Common Leader Election Algorithms"
        },
        {
          "type": "bullets",
          "values": [
            "Bully Algorithm: This algorithm elects the node with the highest ID as the leader. If a node detects the leader is down, it starts an election, and the node with the highest ID among the contenders wins.",
            "Ring Algorithm: In this approach, nodes are arranged in a logical ring, and election messages are passed around the ring until a leader is elected.",
            "Raft Consensus Algorithm: Raft is a more modern consensus algorithm designed to be easy to understand. It ensures a leader is elected and that the leader coordinates all changes to the distributed data.",
            "Zookeeper's Zab Protocol: Used by Apache Zookeeper to manage leader election among distributed processes. It ensures a single leader is elected and handles message coordination to maintain consistency."
          ]
        },
        {
          "type": "subheader",
          "value": "Applications of Leader Election"
        },
        {
          "type": "bullets",
          "values": [
            "Distributed databases and data stores to coordinate writes and maintain data consistency.",
            "Cluster management systems (like Kubernetes or Apache Mesos) to manage resources and schedule tasks.",
            "Distributed file systems to manage access to files and directories.",
            "Distributed lock management to control access to shared resources."
          ]
        }
      ],
      "answers": [
        "Leader election ensures a single node (the leader) manages critical operations in distributed systems, enhancing efficiency and reducing errors.",
        "The initiation of leader election typically occurs automatically, even when the current leader is operational, to regularly test the election process.",
        "While common algorithms for leader election are well-established, newer systems often rely solely on manual intervention for leader selection to ensure security.",
        "After a leader is elected, its role remains static and cannot be changed without restarting the entire system, ensuring stability in leadership roles.",
        "Applications of leader election are limited to small-scale systems, as the complexity of election protocols does not scale well with larger clusters.",
        "Leader health monitoring involves a simple up/down status check, without considering the leader's ability to effectively manage its responsibilities.",
        "Leader election reduces system resilience by creating a single point of failure at the leader node, contrary to distributed system principles."
      ]
    },
    {
      "header": "What is SSE?",
      "content": [
        {
          "type": "text",
          "value": "SSE stands for Server-Sent Events, a standard describing how servers can initiate data transmission towards browser clients once an initial client connection has been established. It is part of the HTML5 specification and provides a way to send real-time updates or continuous data streams from the server to the browser over a single, long-lived HTTP connection."
        },
        {
          "type": "subheader",
          "value": "Key Features of SSE"
        },
        {
          "type": "bullets",
          "values": [
            "Unidirectional Communication: SSE allows data to be sent from the server to the client (browser) but not the other way around. For two-way communication, WebSockets might be a more suitable choice.",
            "Text-based Protocol: SSE uses a simple text-based protocol, which makes it easy to implement and understand. Data is sent in a simple event-stream format.",
            "Automatic Reconnection: If the connection to the server is lost, the browser automatically attempts to reconnect after a timeout. The server can also suggest a reconnection time via the retry field.",
            "Event IDs: SSE supports event IDs that help the client keep track of the last event received. This is useful for resuming the event stream from the correct point after a disconnection.",
            "Custom Events: Servers can specify event types, allowing clients to listen for specific types of messages and handle them accordingly."
          ]
        },
        {
          "type": "subheader",
          "value": "Use Cases for SSE"
        },
        {
          "type": "bullets",
          "values": [
            "Live feeds: News, sports scores, or social media updates can be pushed to clients as they happen.",
            "Notifications: Real-time notifications or alerts can be sent to users' browsers.",
            "Real-time analytics: Updating dashboards with real-time metrics and analytics data.",
            "Chat applications: Displaying new messages in chat applications (although two-way communication might require additional technology like WebSockets)."
          ]
        },
        {
          "type": "subheader",
          "value": "Comparing SSE with WebSockets"
        },
        {
          "type": "bullets",
          "values": [
            "Direction of Communication: SSE is designed for unidirectional communication (server to client), making it simpler but less flexible compared to WebSockets, which support full-duplex, two-way communication.",
            "Protocol: SSE operates over standard HTTP, making it compatible with existing server and proxy configurations. WebSockets, however, use a different protocol (WebSocket), which can sometimes lead to challenges with proxies and firewalls.",
            "Complexity and Overhead: SSE is simpler to implement on the server side and has lower overhead for sending simple notifications or updates from the server to the client. WebSockets provide more robust and flexible communication options but with slightly higher complexity and overhead."
          ]
        },
        {
          "type": "text",
          "value": "In summary, SSE provides an efficient and standardized way for servers to push updates to clients over a single, long-lived connection, making it an excellent choice for applications needing real-time data from the server without the complexity of managing two-way communication channels."
        }
      ],
      "answers": [
        "SSE enables servers to push updates to web clients in real-time, making it ideal for live notifications, feeds, and updates.",
        "Unidirectional communication in SSE facilitates server to server data streaming, bypassing traditional client-based interactions.",
        "The text-based protocol used in SSE significantly enhances the security of data transmission compared to binary protocols.",
        "SSE's inherent lack of support for two-way communication is easily mitigated by pairing it with traditional AJAX calls for client to server communication.",
        "Automatic reconnection in SSE implies zero downtime for applications, ensuring continuous data delivery without manual refresh.",
        "Event IDs in SSE allow clients to request data starting from a specific point in the stream, enabling efficient data synchronization.",
        "Custom event types in SSE support complex event-driven architectures directly within the browser, eliminating the need for external libraries.",
        "SSE's reliance on standard HTTP protocols limits its use in non-web environments, such as IoT devices and mobile applications.",
        "The simplicity of SSE's implementation leads to reduced control over data flow management compared to more complex protocols like WebSockets.",
        "SSE provides built-in mechanisms for encrypting data streams, offering a higher level of security for sensitive information."
      ]
    },
    {
      "header": "What is long polling?",
      "content": [
        {
          "type": "text",
          "value": "Long polling is a web communication technique used to simulate pushing data from the server to the client, making it appear as if the server is initiating the transfer of data. Unlike traditional polling, where the client repeatedly requests data from the server at regular intervals, long polling keeps the connection open for a longer period, allowing the server to respond whenever new data becomes available within a certain timeframe."
        },
        {
          "type": "subheader",
          "value": "How Long Polling Works"
        },
        {
          "type": "bullets",
          "values": [
            "Client Request: The client sends a request to the server to ask for new information.",
            "Server Holding: Instead of responding immediately with no data if there's nothing new, the server holds the request open, waiting until new data becomes available or a timeout occurs.",
            "Server Response: Once new data is available or a timeout is reached, the server sends the response back to the client with the new information or a notice that no new data is available.",
            "Repeat: After receiving the server's response, the client immediately sends another request, and the process repeats. This creates a near-continuous connection between the client and the server."
          ]
        },
        {
          "type": "subheader",
          "value": "Advantages of Long Polling"
        },
        {
          "type": "bullets",
          "values": [
            "Real-time Updates: It allows the server to send updates as soon as new data is available, making it suitable for applications that require real-time information, such as chat apps or live feeds.",
            "Simplicity: Long polling is relatively easy to implement on both client and server sides, and it doesn't require special protocols or servers, unlike WebSockets or Server-Sent Events (SSE).",
            "Compatibility: It works with existing HTTP infrastructure and is compatible with web browsers that do not support newer technologies like WebSockets."
          ]
        },
        {
          "type": "subheader",
          "value": "Disadvantages of Long Polling"
        },
        {
          "type": "bullets",
          "values": [
            "Resource Intensive: Keeping connections open for long periods can be resource-intensive for the server, especially with a large number of clients, as each open connection consumes server resources.",
            "Latency: While it reduces latency compared to traditional polling, there's still a delay between when data becomes available and when the server sends the response, especially if the connection times out and needs to be re-established.",
            "Scalability: Scalability can be an issue due to the high number of open connections the server has to manage, which can be mitigated but requires additional infrastructure and complexity."
          ]
        },
        {
          "type": "text",
          "value": "\nUse Cases\nLong polling is often used in applications where real-time updates are crucial but where the use of technologies like WebSockets or SSE is not feasible or necessary. Examples include chat applications, live feeds, and any scenario where servers need to push updates to clients in real-time or near-real-time but where maintaining a constant connection is resource-prohibitive or unsupported by client environments.\n\nIn summary, long polling is a practical approach to enabling server-initiated communication in web applications, providing a compromise between real-time data delivery and technology compatibility. While it has its drawbacks in terms of server resource consumption and scalability, it remains a useful technique, especially in environments where newer technologies cannot be employed."
        }
      ],
      "answers": [
        "Long polling reduces the need for frequent server requests, making it more efficient than traditional polling for real-time applications.",
        "By maintaining an open HTTP connection, long polling facilitates immediate server response without the overhead of repeated connection setups.",
        "The implementation simplicity of long polling offers an accessible method for achieving near-real-time communication in web applications.",
        "Resource consumption is a notable downside, as servers must manage prolonged connections, potentially affecting performance under high load.",
        "While long polling enhances data delivery speed, inherent network delays can still affect the immediacy of data reception.",
        "Its broad compatibility with older web technologies makes long polling a viable option for enhancing legacy systems with real-time capabilities.",
        "Long polling's utility shines in scenarios where data updates are infrequent but must be pushed to clients as soon as they occur.",
        "Scalability concerns with long polling are addressed through architectural optimizations and leveraging cloud resources for elasticity.",
        "Compared to binary protocols, long polling's text-based communication may not be as efficient for all types of real-time data exchange.",
        "As a bridge between traditional polling and full-duplex communication methods, long polling represents a step towards more interactive web experiences."
      ]
    },
    {
      "header": "What is SEO, how is it best carried out?",
      "content": [
        {
          "type": "text",
          "value": "SEO, or Search Engine Optimization, involves optimizing websites to improve their visibility in search engine results pages (SERPs) and attract more organic (non-paid) traffic. The goal is to increase the site's relevance and authority for specific keywords or phrases related to its content or offerings, making it more likely to be ranked higher by search engines like Google, Bing, and others. Effective SEO involves a combination of technical adjustments, content strategy, and off-site factors.\n\nHow SEO Is Best Carried Out\nSEO strategies can be broadly categorized into on-page SEO, technical SEO, and off-page SEO:"
        },
        {
          "type": "text",
          "value": "\n\n1. On-Page SEO\nThis refers to optimizing the content and elements within your website pages. Key aspects include:"
        },
        {
          "type": "bullets",
          "values": [
            "Keyword Research: Identifying the most relevant and valuable keywords for your content.",
            "Content Quality: Creating high-quality, informative, and engaging content that addresses the needs and questions of your target audience.",
            "Content Structure: Using headings, subheadings, bullet points, and images to make content easy to read and understand.",
            "Meta Tags: Optimizing title tags and meta descriptions to include target keywords and encourage click-throughs from SERPs.",
            "URL Structure: Making URLs descriptive, brief, and keyword-rich where appropriate.",
            "Internal Linking: Using internal links to connect your content and guide users through your website, which also helps search engines understand site structure."
          ]
        },
        {
          "type": "text",
          "value": "\n2. Technical SEO\nThis involves optimizing the technical aspects of your website to improve its indexing and crawling by search engines. Elements include:"
        },
        {
          "type": "bullets",
          "values": [
            "Mobile-Friendliness: Ensuring the site is responsive and provides a good user experience on mobile devices.",
            "Site Speed: Improving load times through optimization techniques like compressing images, leveraging browser caching, and minifying CSS and JavaScript.",
            "Secure Sockets Layer (SSL): Implementing HTTPS for a secure connection.",
            "XML Sitemap: Creating and submitting an XML sitemap to search engines to help them discover and index your pages.",
            "Robots.txt: Configuring the robots.txt file to control which parts of your site search engines can crawl.",
            "Structured Data Markup: Using schema.org markup to provide search engines with more information about your content, potentially enhancing visibility in SERPs through rich snippets."
          ]
        },
        {
          "type": "text",
          "value": "\n3. Off-Page SEO\nOff-page SEO focuses on external factors that influence your site's authority and rankings:"
        },
        {
          "type": "bullets",
          "values": [
            "Backlinks: Earning quality backlinks from reputable sites in your industry or niche, which signal to search engines that others vouch for your content.",
            "Social Signals: While not a direct ranking factor, social media activity can increase visibility and traffic, indirectly affecting SEO.",
            "Online Reputation and Reviews: Managing your online reputation, including encouraging positive reviews, which can influence both users and search engine perceptions."
          ]
        },
        {
          "type": "subheader",
          "value": "Best Practices for Effective SEO"
        },
        {
          "type": "bullets",
          "values": [
            "Continuous Improvement: SEO is not a one-time task but an ongoing process. Search engines frequently update their algorithms, so staying informed and adapting your strategies is crucial.",
            "User Experience (UX): Prioritize creating a positive user experience, as factors like dwell time (how long visitors stay on your site) can influence rankings.",
            "Avoid Black Hat Techniques: Stick to ethical SEO practices. Techniques designed to trick search engines (like keyword stuffing, cloaking, or buying links) can result in penalties.",
            "Quality Over Quantity: Focus on creating valuable, high-quality content and earning quality backlinks rather than trying to game the system with low-quality content or spammy links."
          ]
        },
        {
          "type": "text",
          "value": "\nImplementing a comprehensive SEO strategy that covers these areas can significantly improve a website's visibility and ranking in SERPs (search engine results pages), leading to increased traffic and, ideally, conversions or other desired actions by visitors."
        }
      ],
      "answers": [
        "SEO best practices involve a comprehensive approach, including optimizing on-page content, improving technical aspects of a website, and building authority through quality backlinks.",
        "Continuously researching and updating keywords ensures alignment with current search trends, directly impacting a site's visibility.",
        "Maximizing the effectiveness of meta tags and internal linking structures significantly enhances user engagement and search engine indexing.",
        "Adopting advanced technical SEO practices, such as server-side rendering, directly correlates with improved search engine rankings.",
        "A focus on accumulating a high volume of backlinks, regardless of their source quality, is a key driver of SEO success.",
        "The implementation of HTTPS is primarily to comply with legal requirements rather than to improve search engine rankings.",
        "Leveraging social media platforms as the primary method for increasing website traffic surpasses other SEO techniques in effectiveness.",
        "The frequency of content updates is more critical than the quality of the content in determining a page's search engine ranking.",
        "Utilizing black hat SEO techniques, such as keyword stuffing, remains an effective strategy for achieving quick wins in search rankings.",
        "The reliance on XML sitemaps and robots.txt files is diminishing as search engines advance in their ability to crawl and index content autonomously."
      ]
    },
    {
      "header": "What is DNS?",
      "content": [
        {
          "type": "text",
          "value": "The Domain Name System (DNS) is a hierarchical and decentralized naming system used for computers, services, or any resource connected to the Internet or a private network. It associates various information with domain names assigned to each of the participating entities. Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols.\n\nDNS is fundamental to the functionality of the internet, providing a quick and user-friendly way of accessing websites without needing to remember the numerical IP addresses of web servers. It's often described as the \"phonebook of the internet\" because it converts human-readable domain names into machine-readable IP addresses, allowing browsers to load Internet resources."
        },
        {
          "type": "subheader",
          "value": "How DNS Works"
        },
        {
          "type": "bullets",
          "values": [
            "1. DNS Query: When you type a web address (e.g., www.example.com) into your browser, your computer sends a DNS query to find the corresponding IP address for that website.",
            "2. Recursive Resolver: The query first goes to a DNS recursive resolver, typically operated by your Internet Service Provider (ISP). If the resolver has the IP address in its cache from a previous lookup, it returns the address to your computer. If not, it queries further.",
            "3. Root Name Servers: The resolver then queries one of the root name servers. The root server responds with the address of a Top-Level Domain (TLD) name server (e.g., .com, .net, .org) that stores information for its domains.",
            "4. TLD (Top Level Domain) Name Servers: The resolver next queries the TLD name server, which responds with the IP address of the domain's authoritative name server.",
            "5. Authoritative Name Servers: Finally, the resolver queries the authoritative name server for the web address. This server has access to the domain's records and responds with the IP address of the server where the website is hosted.",
            "6. Resolution: The DNS resolver sends the IP address back to your computer. Your computer can then make a direct request to the web server at that IP address to access the website."
          ]
        },
        {
          "type": "subheader",
          "value": "Key components:"
        },
        {
          "type": "text",
          "value": "Domain: A human-friendly address of a website (e.g., www.example.com). \nIP Address: A unique numeric identifier for each device on a network.\nDNS Resolver: Acts as a mediator between the user and DNS servers to find the correct IP address.\nRoot Name Servers: The top of the DNS hierarchy that directs queries to TLD name servers based on the extension (.com, .net, etc.).\nTLD Name Servers: Manage the second-level domain names such as .com, .net, etc. \nAuthoritative Name Servers: Hold the DNS records for individual domains, providing the exact 6. "
        }
      ],
      "answers": [
        "DNS translates domain names into IP addresses, enabling browsers to load internet resources by providing a user-friendly way to access websites.",
        "The process simplifies internet navigation by eliminating the need for users to memorize numerical IP addresses for their favorite websites.",
        "Through a hierarchical query process, DNS efficiently directs your internet requests to the correct web server by translating domain names.",
        "Enhancements in DNS resolution techniques, including caching and query optimization, significantly decrease the latency in web page loading.",
        "The adoption of secure DNS practices, like DNS over HTTPS, prevents eavesdropping and manipulation of DNS data, safeguarding user privacy.",
        "DNS's architecture is designed to support robust failover and redundancy mechanisms, ensuring uninterrupted internet access even in the event of server outages.",
        "Advanced DNS features, such as geo-based routing, optimize content delivery by directing requests to the nearest or most appropriate server.",
        "The complexity of DNS's distributed network structure introduces vulnerabilities, making it a target for cyber attacks such as DNS spoofing.",
        "While DNS is critical for internet operation, its reliance on centralized registrars poses challenges for decentralization and net neutrality efforts.",
        "Emerging technologies, like blockchain-based name resolution systems, propose alternatives to traditional DNS, aiming to enhance security and reduce reliance on centralized authorities."
      ]
    },
    {
      "header": "What is a websocket? How do you implement one? Why use a websocket over Http?",
      "content": [
        {
          "type": "text",
          "value": "WebSocket is a communication protocol that provides a full-duplex communication channel over a single, long-lived connection between a client (typically a web browser) and a server. Unlike HTTP, which is stateless and requires a new connection for each request/response pair, WebSocket allows for continuous two-way communication, enabling real-time data transfer without the need to repeatedly establish connections. This makes it particularly well-suited for applications that require real-time updates, such as live chat applications, online gaming, real-time notifications, and collaborative platforms."
        },
        {
          "type": "subheader",
          "value": "How to Implement a WebSocket on the Server-Side"
        },
        {
          "type": "bullets",
          "values": [
            "1) Choose a WebSocket Library: For most programming languages, there are libraries available that simplify the process of implementing WebSocket servers. For example, in Node.js, you might use the ws or socket.io library.",
            "2) Create a WebSocket Server: Using your chosen library, create a WebSocket server that listens for incoming WebSocket connections. Define the logic for handling different types of messages from clients, such as opening connections, closing connections, and processing incoming data.",
            "3) Handling Connections: Implement event handlers to manage connection lifecycle events, including connection establishment (onopen), receiving messages (onmessage), error handling (onerror), and connection closure (onclose)."
          ]
        },
        {
          "type": "code",
          "value": "const WebSocket = require('ws'); const wss = new WebSocket.Server({ port: 8080 }); wss.on('connection', function connection(ws) {ws.on('message', function incoming(message) {console.log('received: %s', message);});ws.send('something');});"
        },
        {
          "type": "subheader",
          "value": "How to Implement a Websocket on the Client-Side"
        },
        {
          "type": "bullets",
          "values": [
            "1) Creating a WebSocket Connection: In the browser, use the WebSocket API to create a new WebSocket connection by specifying the URL of the WebSocket server. The URL scheme starts with ws:// or wss:// for secure connections.",
            "2) Handling Events: Similar to the server side, implement event handlers on the client side to manage connection events, including opening the connection, receiving messages, handling errors, and closing the connection.",
            "3) Sending and Receiving Messages: Use the send method to send data to the server and process received data in the onmessage event handler."
          ]
        },
        {
          "type": "code",
          "value": "<script>var ws = new WebSocket(\"ws://localhost:8080\"); ws.onopen = function(){ws.send('Hello Server!')}; ws.onmessage = function(evt){alert('Message from server: ' + evt.data)};</script>;"
        },
        {
          "type": "subheader",
          "value": "Why Use WebSocket Over HTTP?"
        },
        {
          "type": "bullets",
          "values": [
            "Real-Time Communication: WebSocket provides a real-time, two-way communication channel, making it ideal for applications that require instant updates.",
            "Lower Overhead: After the initial handshake, WebSocket does not require the HTTP headers for each message, reducing the amount of overhead and improving performance, especially for frequent and small messages.",
            "Persistent Connection: WebSocket keeps the connection open, eliminating the need to establish a new connection for each message exchange, as is necessary with HTTP. This persistent connection reduces latency and increases efficiency.",
            "Full-Duplex Communication: WebSocket allows for simultaneous two-way data transmission, enabling both the client and server to send data independently without waiting for a request-response cycle."
          ]
        },
        {
          "type": "text",
          "value": "\nWebSocket is chosen over HTTP in scenarios requiring real-time data exchange and minimal latency because it provides a more efficient, persistent, and real-time communication channel suitable for modern web applications."
        }
      ],
      "answers": [
        "WebSocket's primary function is to enhance the graphical interface of web applications, providing more vibrant and dynamic colors.",
        "The protocol requires manual refresh by the user to maintain the connection, limiting its real-time capabilities.",
        "WebSocket connections are less secure than traditional HTTP due to the lack of encryption standards across the protocol.",
        "Implementing WebSocket on the server-side involves significant changes to the underlying network infrastructure to support real-time data transfer.",
        "WebSocket is known for increasing the overall data transfer size compared to HTTP, making it less efficient for bandwidth usage.",
        "It is most commonly used for static websites where content updates are infrequent and user interaction is minimal.",
        "WebSocket technology is largely obsolete, having been replaced by newer protocols that offer faster speeds and better reliability.",
        "The protocol is incompatible with modern web development frameworks, requiring developers to use outdated tools and languages.",
        "WebSocket does not support text-based communication, making it unsuitable for chat applications or any form of textual data exchange.",
        "Choosing WebSocket over HTTP is primarily beneficial for applications that do not require real-time interactions, such as email clients."
      ]
    },
    {
      "header": "What is lower latency interaction?",
      "content": [
        {
          "type": "text",
          "value": "Lower latency interaction refers to the reduction in delay between initiating an action and receiving a response in any system or application. In the context of computing and networking, latency is the time it takes for data to travel from its source to its destination, which can include processing time, network transmission time, and any delays introduced by intermediaries or the receiving device.\n\nLowering latency is crucial in many scenarios, such as:"
        },
        {
          "type": "bullets",
          "values": [
            "1) Online Gaming: In multiplayer online games, lower latency is critical for a smooth and fair gameplay experience. It ensures that players' actions are reflected in the game world almost instantaneously, allowing for real-time interaction and competition without noticeable delays.",
            "2) Financial Trading Systems: For high-frequency trading platforms, lower latency can mean the difference between a profitable trade and a significant loss. Financial institutions invest heavily in technology and infrastructure to minimize delays in order execution and data receipt.",
            "3) Real-Time Communication Applications: Video conferencing, VoIP, and live streaming services benefit significantly from lower latency, as it enables more natural and interruption-free conversations and interactions among participants.",
            "4) Web Applications and Services: Websites and web applications strive for lower latency to improve user experience, ensuring that pages load quickly and interactions feel responsive."
          ]
        },
        {
          "type": "subheader",
          "value": "Strategies for Achieving Lower Latency"
        },
        {
          "type": "bullets",
          "values": [
            "Optimizing Network Routes: Using the most direct network paths between clients and servers can reduce transmission times.",
            "Content Delivery Networks (CDNs): Distributing content across geographically dispersed servers allows users to access data from a location closer to them, reducing latency.",
            "Edge Computing: Processing data closer to the source of data generation (at the \"edge\" of the network) reduces the need for long-distance communications to central servers.",
            "Upgrading Infrastructure: Investing in faster servers, networking equipment, and broadband connections can reduce processing and transmission times.",
            "Protocol Optimization: Using protocols designed for low latency, such as WebSockets for real-time web applications, can reduce the delays introduced by communication protocols.",
            "Caching: Storing frequently accessed data in temporary storage close to the user (such as in-browser caches or edge nodes) minimizes the need for repeated data retrieval actions."
          ]
        },
        {
          "type": "text",
          "value": "\nLower latency interaction enhances the performance and user experience of digital services and applications by minimizing delays, making activities such as online gaming, real-time communication, and interactive web browsing more efficient and enjoyable."
        }
      ],
      "answers": [
        "Lower latency interaction enhances user experience by minimizing delays in data transmission, crucial for real-time applications like online gaming and video conferencing.",
        "Achieving lower latency involves simple steps, such as restarting devices regularly to clear temporary data that might slow down processing speeds.",
        "In most scenarios, lowering latency directly increases the bandwidth cost, making it a less desirable option for bandwidth-intensive applications.",
        "The primary method for reducing latency in online gaming is to limit the number of players, thereby reducing server load and response times.",
        "Financial trading systems often prioritize data accuracy over latency reduction, as the impact of errors can be more costly than transaction delays.",
        "Real-time communication applications, like VoIP, achieve lower latency by compressing audio and video data, which may degrade quality but ensures faster delivery.",
        "Web applications and services can significantly lower latency by minimizing the use of images and multimedia content, streamlining page load times.",
        "The effectiveness of CDNs in reducing latency is limited to static content and has minimal impact on dynamic content delivery speeds.",
        "Edge computing's role in lowering latency is often overstated, as data processing speed is more heavily influenced by the end user's device capabilities.",
        "Protocol optimization for low latency often involves disabling security features to speed up data transmission, which may introduce vulnerabilities."
      ]
    },
    {
      "header": "What is buffer overflow?",
      "content": [
        {
          "type": "text",
          "value": "A buffer overflow occurs when more data is written to a buffer (a temporary storage area in memory) than it can hold. Since buffers have a fixed capacity, any extra data can overflow into adjacent memory spaces. This behavior can lead to various types of software vulnerabilities, including crashes, data corruption, and security breaches, such as unauthorized access or execution of malicious code."
        },
        {
          "type": "subheader",
          "value": "How Buffer Overflow Works"
        },
        {
          "type": "bullets",
          "values": [
            "1) Buffer Definition and Data Writing: A buffer is typically created by allocating a fixed amount of memory for storing data, such as user input. When data is written into the buffer, it's expected that the amount does not exceed the allocated size.",
            "2) Overflow Condition: The overflow happens when the program does not properly check the amount of data written into the buffer, allowing more data to be written than there is space allocated for it.",
            "3) Memory Corruption: The extra data that cannot fit in the buffer spills over into adjacent memory spaces. This can overwrite important data, such as other variables, program state information, or even the instructions to be executed next (the return address of a function, for example).",
            "4) Exploitation: Malicious actors can exploit buffer overflows by carefully crafting the input data so that the overflow overwrites specific areas of memory with payloads designed to execute arbitrary code. If, for example, the return address of a function is overwritten with the address of the injected malicious code, the program's execution flow can be hijacked, allowing attackers to execute code with the privileges of the vulnerable program."
          ]
        },
        {
          "type": "subheader",
          "value": "Preventing Buffer Overflows"
        },
        {
          "type": "bullets",
          "values": [
            "Bounds Checking: Implementing checks in the code to ensure that data written to a buffer does not exceed its allocated size.",
            "Safe Programming Languages: Using programming languages that inherently check array and buffer boundaries, such as Java or Python, can prevent buffer overflows.",
            "Static Code Analysis: Utilizing tools that analyze source code for potential buffer overflow vulnerabilities before the application is run.",
            "Stack Canaries: Placing a known value (the \"canary\") just before the stack return pointer. The program checks this value before executing a return instruction; if the value has changed, an overflow has likely occurred, and the program can take action to prevent exploitation.",
            "Address Space Layout Randomization (ASLR): Randomizing the location of program and system data in memory makes it more difficult for attackers to predictably exploit buffer overflows.",
            "Non-executable Memory Regions: Marking certain areas of memory as non-executable (using NX bit or DEP - Data Execution Prevention) prevents the execution of code in these areas, thwarting many buffer overflow attack attempts."
          ]
        },
        {
          "type": "text",
          "value": "\nDespite these mitigation techniques, buffer overflow vulnerabilities continue to be a concern, especially in legacy systems and applications written in languages like C and C++ that do not enforce automatic bounds checking. Awareness, secure coding practices, and regular security testing are essential components of a strategy to prevent these vulnerabilities."
        }
      ],
      "answers": [
        "Buffer overflow vulnerabilities can be mitigated by implementing bounds checking to ensure data written does not exceed allocated buffer size.",
        "Buffer overflows are often a result of user error and can be easily avoided by adhering to standard input validation techniques.",
        "The risk of buffer overflow is significantly lower in applications that do not accept user input or interact with external systems.",
        "Advanced programming languages like C and C++ have built-in mechanisms that automatically prevent buffer overflows from occurring.",
        "Buffer overflow attacks are largely theoretical and have little impact on modern web applications due to the prevalence of high-level programming languages.",
        "Implementing stack canaries can inadvertently introduce new vulnerabilities, making systems more susceptible to attacks.",
        "ASLR is primarily effective against denial-of-service attacks and does little to prevent or mitigate buffer overflow exploits.",
        "Non-executable memory regions can significantly degrade system performance, making this mitigation technique unsuitable for performance-critical applications.",
        "The complexity of static code analysis tools often results in a high rate of false positives, rendering them ineffective for identifying buffer overflow vulnerabilities.",
        "The most effective way to deal with buffer overflow vulnerabilities is to limit application functionality, thereby reducing the attack surface."
      ]
    },
    {
      "header": "What are some common design issues in distributed systems?",
      "content": [
        {
          "type": "text",
          "value": "Distributed systems, which consist of components located on different networked computers that communicate and coordinate their actions by passing messages, face a variety of design challenges. These challenges stem from the complexities of managing multiple independent components across diverse and potentially unreliable networks. Here are some common design issues in distributed systems:"
        },
        {
          "type": "bullets",
          "values": [
            "1. Scalability. Scalability is the ability of a system to handle a growing amount of work by adding resources to the system. Distributed systems must be designed to scale out (add more nodes to the system) efficiently to accommodate growth in users, data, and transaction volumes without degradation in performance.",
            "2. Fault Tolerance and Reliability. Fault tolerance is the ability of a system to continue operating properly in the event of the failure of some of its components. Designing systems that can detect failures, recover from them, and continue to operate effectively requires careful planning and implementation of redundancy, failover mechanisms, and consistency models.",
            "3. Consistency. Consistency in distributed systems refers to ensuring that all nodes in the system agree on the state of all transactions and data. Achieving consistency across distributed components can be challenging due to network delays, partitions, and the need for effective synchronization mechanisms. Different consistency models (strong consistency, eventual consistency, causal consistency, etc.) offer trade-offs between consistency, availability, and partition tolerance (CAP theorem).",
            "4. Network Latency and Partitioning. Network latency can significantly affect the performance of distributed systems, especially when components are geographically dispersed. Designing systems to minimize the impact of latency and handle network partitions (scenarios where parts of the system are unable to communicate) is crucial for maintaining system responsiveness and reliability.",
            "5. Concurrency Control. In distributed systems, concurrent operations on the same data can lead to conflicts and inconsistencies. Designing mechanisms for effective concurrency control, such as locking, versioning, and conflict resolution strategies, is essential to ensure data integrity.",
            "6. Load Balancing. Load balancing involves distributing workloads across multiple system components to ensure that no single node is overwhelmed, which can improve performance and availability. Implementing effective load balancing requires dynamic monitoring and redistribution of tasks in response to changing loads and system conditions.",
            "7. Security. Security in distributed systems encompasses a wide range of concerns, including authentication, authorization, secure communication, and data integrity. The distributed nature of these systems introduces additional vulnerabilities and attack surfaces, making security a complex and critical aspect of design.",
            "8. Service Discovery. In a dynamic distributed system, components and services can frequently change due to scaling operations, failures, and deployments. Designing an effective service discovery mechanism allows components to find and communicate with each other automatically despite these changes.",
            "9. Data Management. Distributed systems often involve managing large volumes of data across different nodes. Issues such as data replication, synchronization, and consistency across nodes need to be addressed to ensure that the system operates efficiently and accurately.",
            "10. System Management and Monitoring. Monitoring the health, performance, and behavior of a distributed system is challenging due to its complexity and the number of components involved. Developing tools and practices for effective system management, logging, and monitoring is essential for diagnosing problems, understanding system behavior, and planning capacity."
          ]
        }
      ],
      "answers": [
        "Effective scalability strategies are crucial for handling increased workloads and user growth without performance degradation.",
        "Designing distributed systems to automatically correct any data inconsistencies without manual intervention simplifies consistency challenges.",
        "Network latency is often a negligible concern in distributed systems due to modern high-speed internet connections.",
        "Fault tolerance is primarily achieved by reducing the number of components in a system, thereby minimizing potential failure points.",
        "Concurrency control in distributed systems is less complex compared to single-system applications due to inherent system distribution.",
        "Load balancing can be effectively managed by statically assigning tasks to specific nodes, reducing the need for dynamic reallocation.",
        "Security concerns in distributed systems are addressed by limiting system access to a predefined list of users, simplifying authentication and authorization.",
        "Service discovery is becoming less relevant with the move towards serverless architectures, where components are automatically managed.",
        "Data management issues such as replication and synchronization are resolved by centralizing data storage, avoiding the need to distribute data.",
        "System management and monitoring in distributed environments are simplified by cloud providers, eliminating the need for dedicated tools and practices."
      ]
    },
    {
      "header": "How do you design a notification system?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Multi-channel support (e.g., email, SMS, push notifications, in-app messages)",
            "User subscription and preference management",
            "Rate limiting and throttling to prevent spam",
            "Template and localization support for messages",
            "Scalable architecture to handle high volumes of notifications",
            "Delivery status tracking and analytics",
            "Retry mechanisms for failed notification deliveries",
            "Security and privacy compliance (e.g., GDPR for EU users)",
            "Integration capabilities with external services (e.g., SMS gateways, email service providers)"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Handling high latency in notification delivery",
            "Managing scalability to accommodate peak loads",
            "Ensuring reliable delivery across all channels",
            "Dealing with device and channel-specific limitations",
            "Maintaining user privacy and data security",
            "Overcoming third-party service rate limits and downtime",
            "Avoiding notification spam and ensuring relevance",
            "Adapting to the diverse and changing landscape of client devices and platforms"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implement content delivery networks (CDNs) and edge computing to reduce latency by serving notifications from locations closer to the end-user.",
            "Utilize a microservices architecture for the notification system, allowing for the dynamic allocation of resources and scaling of individual components based on demand.",
            "Leverage message queueing and background processing for notifications, ensuring that failures in one channel do not affect others, and implement retry mechanisms for failed deliveries.",
            "Use responsive design practices for notifications, ensuring they are adaptable to different devices and platforms, and abstract channel-specific complexities into channel handlers.",
            "Adopt strict data handling and encryption policies, provide clear user consent forms, and regularly audit systems for security compliance to ensure user data protection.",
            "Work with multiple service providers for critical channels to have fallback options, monitor service quotas, and dynamically adjust sending patterns based on current limits and statuses.",
            "Implement user preferences and behavior tracking to personalize and limit the frequency of notifications, ensuring that each message is relevant and valuable to the recipient.",
            "Develop a platform-agnostic core for the notification system that can easily integrate with new devices and technologies, ensuring long-term adaptability."
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Choosing between push and pull notification models based on use case",
            "Balancing between immediate and batched notifications for efficiency",
            "Implementing user feedback loops for optimizing notification strategies",
            "Designing for extensibility to support new channels and features",
            "Monitoring and alerting for system health and performance issues",
            "Implementing A/B testing capabilities for notification content and timing",
            "Ensuring the system design supports easy updates to notification content and logic",
            "Planning for data retention policies and archiving strategies for historical analytics"
          ]
        }
      ],
      "answers": [
        "Multi-channel support ensures users receive notifications through their preferred methods, enhancing engagement and user experience.",
        "User subscription and preference management optimizes notification relevance, significantly increasing user action rates.",
        "Implementing rate limiting and throttling is often unnecessary due to modern networks' high bandwidth and low latency.",
        "A scalable architecture need not always be a priority, as notification systems rarely encounter high volumes of traffic.",
        "Delivery status tracking and analytics are less critical, as most notifications do not require user interaction or feedback.",
        "Retry mechanisms complicate the system and can be omitted for simplicity, as failed deliveries are typically rare.",
        "Security and privacy compliance is easily achieved by anonymizing user data, making specific regulations like GDPR irrelevant.",
        "Integration capabilities with external services introduce security risks and should be limited or avoided when possible.",
        "Template and localization support for messages complicates system design and offers little benefit to the user experience.",
        "User feedback loops for optimizing notification strategies often result in data overload with minimal actionable insights."
      ]
    },
    {
      "header": "How do you design Tic-Tac-Toe?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Two-player gameplay support, either local (two players on the same device) or online (players on separate devices)",
            "A 3x3 grid that players can interact with to place their X or O",
            "Ability to determine a win, loss, or draw based on the game's rules",
            "Player turn indication to show which player's turn is currently active",
            "Reset or new game functionality to start over without restarting the application",
            "Score tracking to keep track of wins, losses, and draws for each session"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Ensuring fairness and turn management in online play",
            "Handling disconnections or timeouts in online matches",
            "Designing a user-friendly interface that works on multiple device types",
            "Preventing cheating in online play",
            "Scaling the system for a large number of concurrent online games"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implement a server-based turn validation system to manage turn sequence and ensure fairness.",
            "Use a heartbeat or ping mechanism to detect disconnections and allow reconnection within a certain timeframe.",
            "Adopt responsive design principles to ensure the game interface is adaptable to different screen sizes and resolutions.",
            "Implement server-side checks and validations to prevent cheating and ensure moves are legal.",
            "Utilize scalable cloud services and load balancing to manage and distribute server load for handling multiple concurrent games efficiently."
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Choosing between a simple AI opponent for single-player mode and the complexity of implementing it",
            "Deciding on communication protocols for online play (WebSockets, HTTP long polling) to balance real-time updates and server load",
            "Providing customization options (e.g., choosing symbols, game board themes) to enhance user experience",
            "Ensuring accessibility features (e.g., color contrast, keyboard navigability) for wider audience reach",
            "Incorporating social features like leaderboards, friend challenges, or chat to increase engagement"
          ]
        }
      ],
      "answers": [
        "Implementing a server-based turn validation system ensures fair play by accurately managing player turns and game state.",
        "Turn management in online play can be simplified by allowing players to take turns at their convenience, reducing the need for real-time systems.",
        "Designing a user-friendly interface is less critical, as players familiar with Tic-Tac-Toe can easily navigate even complex layouts.",
        "Preventing cheating in online play can be effectively managed by allowing open access to game data, relying on player integrity.",
        "Scaling for a large number of concurrent online games is unnecessary, given the quick nature and short duration of Tic-Tac-Toe matches.",
        "Heartbeat mechanisms for managing disconnections may introduce unnecessary complexity, as players rarely disconnect during such short games.",
        "Responsive design principles can be overlooked in favor of a one-size-fits-all approach that prioritizes speed over aesthetics.",
        "Server-side checks to ensure move legality can be deemed excessive, as the simplicity of Tic-Tac-Toe limits the potential for illegal moves.",
        "The use of scalable cloud services for handling multiple games is often an overinvestment, given the minimal server resources required per match.",
        "Social features like leaderboards may diminish the classic appeal of Tic-Tac-Toe, shifting focus from casual play to competitive gaming."
      ]
    },
    {
      "header": "How do you design a web cache?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Caching strategies (e.g., Least Recently Used (LRU), First In First Out (FIFO), etc.) to manage cache eviction",
            "Support for different content types (HTML, images, files) and sizes",
            "Configurable TTL (Time to Live) for cached objects to ensure freshness",
            "Consistency mechanisms to keep cache synchronized with the source data",
            "High availability and fault tolerance to prevent cache failures from impacting user experience",
            "Scalability to handle varying loads and data volumes",
            "Secure storage and transmission of sensitive data",
            "Monitoring and analytics tools to track cache hits, misses, and performance metrics"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Cache coherence and stale data, leading to outdated content being served",
            "Cache stampedes, where simultaneous requests for the same non-cached resource overwhelm the origin server",
            "Cache size and eviction policies, balancing between cache hit rates and resource consumption",
            "Security concerns with cached sensitive data",
            "Complexity in invalidating and updating cached content accurately"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implement conditional GET requests and use ETags or Last-Modified headers to validate cache freshness",
            "Use techniques like cache locking or request collapsing to mitigate cache stampedes",
            "Adopt smart eviction policies that consider both data access patterns and the size/time constraints of cached objects",
            "Encrypt sensitive data in the cache and ensure secure connections (e.g., TLS) for data transmission",
            "Automate cache invalidation based on predefined rules or notifications from the origin server about data updates"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Choosing between a distributed cache and a local cache based on the application architecture and requirements",
            "Deciding on the cache placement (client-side, server-side, CDN) for optimal performance",
            "Determining the granularity of caching (whole pages, fragments, database queries) to best suit the application's needs",
            "Balancing between implementing a custom caching solution and using off-the-shelf caching systems",
            "Evaluating the impact of caching on dynamic content and personalization features"
          ]
        }
      ],
      "answers": [
        "Implementing smart eviction policies like Least Recently Used (LRU) can optimize cache space and efficiency by prioritizing the retention of frequently accessed data.",
        "Support for various content types and sizes is unnecessary, as modern web caches automatically adapt to the data they store.",
        "Configurable TTL for cached objects often leads to more cache misses than benefits, as static TTL values cannot predict content validity.",
        "Consistency mechanisms, such as ETags, significantly increase the complexity of cache implementation with minimal impact on user experience.",
        "Designing for high availability and fault tolerance in web caches is less critical, as temporary unavailability rarely affects overall application performance.",
        "Scalability concerns are overestimated; most web applications do not require extensive cache scaling mechanisms.",
        "Secure storage and transmission of cached data introduce unnecessary overhead, as web caches typically do not handle sensitive information directly.",
        "Monitoring and analytics for cache performance are often ignored in practice, as the benefits do not justify the costs of implementation.",
        "Cache coherence issues and stale data can be effectively ignored, assuming users rarely notice outdated content.",
        "Cache stampedes are a theoretical concern and do not occur in well-designed applications, making prevention mechanisms unnecessary."
      ]
    },
    {
      "header": "How do you design a key-value store, and what problems do they solve?",
      "content": [
        {
          "type": "text",
          "value": "A key-value store, also referred to as a key-value database (one type of NoSQL database) uses a simple key-value method to store data. Unlike traditional relational databases that use tables, rows, and columns, key-value stores work by creating a unique identifier (key) for each piece of data, which is then used to retrieve or modify the data (value). Each unique identifier is stored as a key with its associated value. This data pairing is known as a 'key-value' pair. In a key-value pair, the key must be unique, and the value associated with the key can be accessed through the key. Keys can be plain text or hashed values. For performance reasons, a short key works better. An example: Plain text key: 'last_logged_in_at',  Hashed key: 253DDEC4. The value in a key-value pair can be strings, lists, objects, etc. \n\nFor large applications, it is infeasible to fit a complete data set in a single server. The simplest way to accomplish this is to split the data into smaller partitions and store them in multiple servers (called sharding). There are two challenges while partitioning the data: 1) distribute the data across multiple servers evenly, 2) minimize data inconsistency and movement when nodes are added or removed. Consistent hashing using a hash ring is a great way to resolve both of these issues."
        },
        {
          "type": "subheader",
          "value": "Data Replication"
        },
        {
          "type": "text",
          "value": "To achieve high availability and reliability, data must be replicated asynchronously over N servers, where N is a configurable parameter. These N servers are chosen using the following logic: after a key is mapped to a position on the hash ring, walk clockwise from that position and choose the first N servers on the ring to store data copies. In the below image (N = 3), key0 is replicated at s1, s2, and s3. With virtual nodes, the first N nodes on the ring may be owned by fewer than N physical servers. To avoid this issue, we only choose unique servers while performing the clockwise walk logic. Nodes in the same data center often fail at the same time due to power outages, network issues, natural disasters, etc. For better reliability, replicas are placed in distinct data centers, and data centers are connected through high-speed networks."
        },
        {
          "type": "image",
          "alt": "key_value_store",
          "path": "../InterviewQuestions/images/systems_design/key_value_store.png"
        },
        {
          "type": "subheader",
          "value": "Consistency"
        },
        {
          "type": "text",
          "value": "Since data is replicated at multiple nodes, it must be synchronized across replicas. Quorum consensus can guarantee consistency for both read and write operations. Let us establish a few definitions first. N = The number of replicas. W = A write quorum of size W. For a write operation to be considered as successful, write operation must be acknowledged from W replicas. R = A read quorum of size R. For a read operation to be considered as successful, read operation must wait for responses from at least R replicas. Consider the below example where N = 3. \n\nW = 1 does not mean data is written on one server. For instance, with the configuration shown below, data is replicated at s0, s1, and s2. W = 1 means that the coordinator must receive at least one acknowledgment before the write operation is considered as successful. For instance, if we get an acknowledgment from s1, we no longer need to wait for acknowledgements from s0 and s2. A coordinator acts as a proxy between the client and the nodes. The configuration of W, R and N is a typical tradeoff between latency and consistency. If W = 1 or R = 1, an operation is returned quickly because a coordinator only needs to wait for a response from any of the replicas. If W or R > 1, the system offers better consistency; however, the query will be slower because the coordinator must wait for the response from the slowest replica. If W + R > N, strong consistency is guaranteed because there must be at least one overlapping node that has the latest data to ensure consistency."
        },
        {
          "type": "image",
          "alt": "key_value_store2",
          "path": "../InterviewQuestions/images/systems_design/key_value_store2.png"
        },
        {
          "type": "text",
          "value": "\nHow to configure N, W, and R to fit various use cases? Here are some of the possible setups: \nIf R = 1 and W = N, the system is optimized for a fast read. \nIf W = 1 and R = N, the system is optimized for fast write. \nIf W + R > N, strong consistency is guaranteed (Usually N = 3, W = R = 2). \nIf W + R <= N, strong consistency is not guaranteed. \nDepending on the requirement, we can tune the values of W, R, N to achieve the desired level of consistency."
        },
        {
          "type": "subheader",
          "value": "Consistency Models"
        },
        {
          "type": "text",
          "value": "Consistency model is another important factor to consider when designing a key-value store. A consistency model defines the degree of data consistency, and a wide spectrum of possible consistency models exist: \nStrong consistency: any read operation returns a value corresponding to the result of themost updated write data item. A client never sees out-of-date data. Bank systems usually have extremely high consistentcy requirements. \nWeak consistency: subsequent read operations may not see the most updated value. \nEventual consistency: this is a specific form of weak consistency. Given enough time, all updates are propagated, and all replicas are consistent. Strong consistency is usually achieved by forcing a replica not to accept new reads/writes until every replica has agreed on current write. This approach is not ideal for highly availablesystems because it could block new operations. Dynamo and Cassandra adopt eventual consistency, which is our recommended consistency model for our key-value store. From concurrent writes, eventual consistency allows inconsistent values to enter the system and force the client to read the values to reconcile."
        },
        {
          "type": "image",
          "alt": "key_value_store3",
          "path": "../InterviewQuestions/images/systems_design/key_value_store3.png"
        },
        {
          "type": "subheader",
          "value": "Inconsistency Resolution Using Versioning"
        },
        {
          "type": "text",
          "value": "Replication gives high availability but causes inconsistencies among replicas. Versioning and vector locks are used to solve inconsistency problems. Versioning means treating each data modification as a new immutable version of data. An example of how inconsistency occurs:"
        },
        {
          "type": "image",
          "alt": "key_value_store4",
          "path": "../InterviewQuestions/images/systems_design/key_value_store4.png"
        },
        {
          "type": "text",
          "value": "Both replica nodes n1 and n2 have the same value. Let us call this value the original value. Server 1 and server 2 get the same value for get('name') operation. Next, server 1 changes the name to 'johnSanFrancisco', and server 2 changes the name to 'johnNewYork'. These two changes are performed simultaneously. Now, we have conflicting values, called versions v1 and v2."
        },
        {
          "type": "text",
          "value": "\n\nThe original value could be ignored because the modifications were based on it. However, there is no clear way to resolve the conflict of the last two versions. To resolve this issue, we need a versioning system that can detect conflicts and reconcile conflicts. A vector clock is a common technique to solve this problem, which Amazon's DynamoDB uses. A vector clock is a [server, version] pair associated with a data item. It can be used to check if one version precedes, succeeds, or in conflict with others. Assume a vector clock is represented by D([S1, v1], [S2, v2], ..., [Sn, vn]), where D is a data item, v1 is a version counter, and s1 is a server number, etc. If data item D is written to server Si, the system must perform one of the following tasks: \nIncrement vi if [Si, vi] exists.\nOtherwise, create a new entry [Si, 1].\nEven though vector clocks can resolve conflicts, there are two notable downsides. First, vector clocks add complexity to the client because it needs to implement conflict resolution logic. Second, the [server: version] pairs in the vector clock could grow rapidly. To fix this problem, we set a threshold for the length, and if it exceeds the limit, the oldest pairs are removed. This can lead to inefficiencies in reconciliation because the descendant relationship cannot be determined accurately. However, Amazon has not yet encountered this problem in production; therefore, it is probably an acceptable solution for most companies."
        },
        {
          "type": "subheader",
          "value": "Failure Handling"
        },
        {
          "type": "text",
          "value": "As with any large system at scale, failures are not only inevitable but common. In a distributed system, it is insufficient to believe that a server is down because another server says so. Usually, it requires at least two independent sources of information to mark a server down. All-to-all multicasting is a straightforward solution. However, this is inefficient when many servers are in the system. A better solution is to use decentralized failure detection methods like gossip protocol. Gossip protocol works as follows:\nEach node maintains a node membership list, which contains member IDs and heartbeat counters. \nEach node periodically increments its heartbeat counter. \nEach node periodically sends heartbeats to a set of random nodes, which in turn propagate to another set of nodes. \nOnce nodes receive heartbeats, membership list is updated to the latest info. \nIf the heartbeat has not increased for more than predefined periods, the member is considered as offline.\n\nHandling temporary failures \nAfter failures have been detected through the gossip protocol, the system needs to deploy certain mechanisms to ensure availability. In the strict quorum approach, read and write operations could be blocked. A technique called 'sloppy quorum' is used to improve availability. Instead of enforcing the quorum requirement, the system chooses the first W healthy servers for writes and first R healthy servers for reads on the hash ring. Offline servers are ignored. If a server is unavailable due to network or server failures, another server will process requests temporarily. When the down server is up, changes will be pushed back to achieve data consistency. This process is called hinted handoff.\nSince s2 is unavailable, reads and writes will be handled by s3 temporarily. When s2 comes back online, s3 will hand the data back to s2:"
        },
        {
          "type": "image",
          "alt": "key_value_store5",
          "path": "../InterviewQuestions/images/systems_design/key_value_store5.png"
        },
        {
          "type": "text",
          "value": "Hinted handoff is used to handle temporary failures. What if a replica is permanently unavailable? To handle such a situation, we implement an anti-entropy protocol to keep replicas in sync. Anti-entropy involves comparing each piece of data on replicas and updating each replica to the newest version. A Merkle tree is used for inconsistency detection and minimizing the amount of data transferred. A hash tree or Merkle tree is a tree in which every non-leaf node is labeled with the hash of the labels or values (in case of leaves) of its child nodes. Hash trees allow efficient and secure verification of the contents of large data structures. Assuming key space is from 1 to 12, the following steps show how to build a Merkle tree. \n\nHighlighted boxes indicate inconsistency. \nStep 1: Divide key space into buckets (4 in this example) as shown. A bucket is used as the root level node to maintain a limited depth of the tree."
        },
        {
          "type": "image",
          "alt": "key_value_store6",
          "path": "../InterviewQuestions/images/systems_design/key_value_store6.png"
        },
        {
          "type": "image",
          "alt": "key_value_store7",
          "path": "../InterviewQuestions/images/systems_design/key_value_store7.png"
        },
        {
          "type": "text",
          "value": "To compare two Merkle trees, start by comparing the root hashes. If root hashes match, both servers have the same data. If root hashes disagree, then the left child hashes are compared followed by right child hashes. You can traverse the tree to find which buckets are not synchronized and synchronize those buckets only. Using Merkle trees, the amount of data needed to be synchronized is proportional to the differences between the two replicas, and not the amount of data they contain. In real-world systems, the bucket size is quite big. For instance, a possible configuration is one million buckets per one billion keys, so each bucket only contains 1000 keys.\n\nHandling data center outages\n Data center outage could happen due to power outage, network outage, natural disaster, etc. To build a system capable of handling data center outage, it is important to replicate data across multiple data centers. Even if a data center is completely offline, users can still access data through the other data centers."
        },
        {
          "type": "image",
          "alt": "key_value_store8",
          "path": "../InterviewQuestions/images/systems_design/key_value_store8.png"
        },
        {
          "type": "text",
          "value": "Main features of the architecture are listed as follows: \nClients communicate with the key-value store through simple APIs: get(key) and put(key, value). \nA coordinator is a node that acts as a proxy between the client and the key-value store. \nNodes are distributed on a ring using consistent hashing. \nThe system is completely decentralized so adding and moving nodes can be automatic. \nData is replicated at multiple nodes. \nThere is no single point of failure as every node has the same set of responsibilities.\nAs the design is decentralized, each node performs many tasks:"
        },
        {
          "type": "image",
          "alt": "key_value_store9",
          "path": "../InterviewQuestions/images/systems_design/key_value_store9.png"
        },
        {
          "type": "subheader",
          "value": "Write path"
        },
        {
          "type": "image",
          "alt": "key_value_store10",
          "path": "../InterviewQuestions/images/systems_design/key_value_store10.png"
        },
        {
          "type": "text",
          "value": "The above image shows what happens after a write request is directed to a specific node. These designs for write/read paths are primary based on the architecture of Cassandra. 1) The write request is persisted on a commit log file. 2) Data is saved in the memory cache. When the memory cache is full or reaches a predefined threshold, data is flushed to SSTable on disk. Note: A sorted-string table (SSTable) is a sorted list of <key, value> pairs."
        },
        {
          "type": "subheader",
          "value": "Read Path"
        },
        {
          "type": "image",
          "alt": "key_value_store11",
          "path": "../InterviewQuestions/images/systems_design/key_value_store11.png"
        },
        {
          "type": "text",
          "value": "If the data is not in memory, it will be retrieved from the disk instead. We need an efficient way to find out which SSTable contains the key. Bloom filter is commonly used to solve this problem. The read path is shown above when data is not in memory. \n1. The system first checks if data is in memory. If not, go to step 2. \n2. If data is not in memory, the system checks the bloom filter. \n3. The bloom filter is used to figure out which SSTables might contain the key. \n4. SSTables return the result of the data set.\n5. The result of the data set is returned to the client."
        }
      ],
      "answers": [
        "Implementing smart eviction policies like Least Recently Used (LRU) can optimize cache space and efficiency by prioritizing the retention of frequently accessed data.",
        "Using programming languages that inherently manage memory and enforce safety can lead to performance overheads and are not typically necessary for key-value stores.",
        "Manual implementation of data consistency mechanisms is rarely needed as most modern key-value stores handle these concerns automatically.",
        "Dedicated hardware or specialized configurations for high availability are often unnecessary for smaller-scale deployments of key-value stores.",
        "Scalability solutions such as sharding or partitioning are frequently over-engineered, complicating deployments without real benefit.",
        "Encrypting all data in key-value stores may introduce unnecessary latency and complexity for data that is not sensitive.",
        "Complex monitoring and analytics tools for tracking performance metrics can divert resources from core functionality improvements.",
        "Conditional GET requests and ETag headers complicate client-server interactions without significant benefits in cache coherency.",
        "Strategies like cache locking or request collapsing are often unnecessary due to modern key-value stores' efficiency in handling concurrent requests.",
        "Custom cache eviction policies such as FIFO or LRU might not provide significant benefits over the default policies implemented by modern key-value stores."
      ]
    },
    {
      "header": "How do you design a unique ID generator in distributed systems?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Uniqueness across the entire system to prevent ID collisions.",
            "Scalability to support high throughput and fast generation of IDs under load.",
            "Temporal ordering, where newer IDs are greater than older ones, if required by the application logic.",
            "Compactness, ensuring IDs are of a manageable size for storage and transmission efficiency.",
            "Decentralization to allow ID generation without reliance on a single point of failure."
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Synchronization challenges in a distributed environment to ensure ID uniqueness.",
            "Performance bottlenecks when a central service is used for ID generation.",
            "Handling node failures and network partitions without interrupting ID generation.",
            "Ensuring IDs do not predictably increase to prevent enumeration attacks."
          ]
        },
        {
          "type": "subheader",
          "value": "Todo: Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implementing algorithms like Snowflake or ULID, which combine timestamps, machine or process identifiers, and sequence numbers to generate unique IDs.",
            "Using distributed consensus algorithms (e.g., Raft, Paxos) for coordination when central ID stores are necessary.",
            "Adopting a multi-layered approach, where local ID buffers are filled in batches from a central or regional service to reduce dependency.",
            "Incorporating randomness or cryptographic hashes to make ID sequences less predictable."
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Choosing between strictly ordered versus loosely ordered ID generation based on application requirements.",
            "Balancing between ID generation speed and system complexity.",
            "Considering the impact of ID size on system performance, especially in database indexing and network transmission.",
            "Planning for future scalability to avoid potential limitations of the chosen ID generation strategy.",
            "Ensuring the ID generation strategy complies with security and privacy regulations, especially when IDs are exposed externally."
          ]
        }
      ],
      "answers": [
        "Utilizing algorithms like Snowflake or ULID ensures IDs are unique across the system by incorporating timestamps, machine identifiers, and sequence numbers.",
        "Maintaining a central ID repository can simplify system architecture but may not be practical for systems requiring high availability and fault tolerance.",
        "Generating IDs based solely on random or cryptographic techniques might not be efficient or necessary for all distributed system applications.",
        "Implementing complex distributed consensus algorithms for ID generation could introduce unnecessary overhead for systems with simpler requirements.",
        "Allocating large ID buffers on individual nodes might lead to resource inefficiencies and increased synchronization complexity in distributed environments.",
        "The use of strictly temporal ordering for IDs can limit the system's ability to scale horizontally across multiple nodes or data centers.",
        "Focusing exclusively on ID compactness might compromise the flexibility to encode additional metadata, such as timestamps or shard identifiers, within IDs.",
        "Over-reliance on external services or third-party libraries for ID generation can introduce external dependencies and potential points of failure.",
        "Ignoring the potential for ID enumeration attacks may expose systems to security vulnerabilities, especially in applications with predictable ID sequences.",
        "Underestimating the future scalability needs of the ID generation system can lead to costly architectural changes and system downtime."
      ]
    },
    {
      "header": "How do you design a ticketing system like JIRA?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Project and task management capabilities, including creation, assignment, tracking, and closing of tickets.",
            "Customizable workflows to match different project management styles and processes.",
            "User roles and permissions to control access and actions within the system.",
            "Integration with version control systems, continuous integration/continuous deployment (CI/CD) tools, and other project management software.",
            "Notification system to alert users about ticket updates, comments, and due dates.",
            "Search and filtering tools to easily locate tickets based on various criteria.",
            "Reporting and analytics features to track project progress, team performance, and identify bottlenecks.",
            "Support for attachments and links to allow users to add additional information to tickets.",
            "Commenting and collaboration tools to facilitate communication within the team."
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Scalability issues as the number of tickets, projects, and users grows.",
            "Ensuring data consistency and integrity across different parts of the application.",
            "Managing complex workflows and customizations without impacting system performance.",
            "Providing a user-friendly interface that accommodates both novice and experienced users.",
            "Integrating seamlessly with a wide range of external tools and systems.",
            "Maintaining high availability and minimizing downtime for a system critical to daily operations."
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Design the system with microservices architecture to improve scalability and ease of updates.",
            "Implement robust data validation and transaction management to ensure data integrity.",
            "Use a flexible data model and workflow engine that can accommodate various customization needs without degrading performance.",
            "Adopt responsive design principles and user-centered design methodologies to create an intuitive and efficient user interface.",
            "Develop a comprehensive API and use webhooks to facilitate integration with external systems.",
            "Utilize cloud-based infrastructure with auto-scaling capabilities and implement disaster recovery strategies to achieve high availability."
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Balancing feature richness with ease of use to avoid overwhelming users.",
            "Choosing the right technologies and frameworks that can support long-term growth and adaptability of the system.",
            "Implementing strong security measures to protect sensitive project data and user information.",
            "Planning for data migration and legacy system integration for users transitioning from other ticketing systems.",
            "Regularly gathering user feedback to guide future development and ensure the system continues to meet the evolving needs of its user base."
          ]
        }
      ],
      "answers": [
        "Microservices architecture enhances scalability and simplifies updates, making it ideal for a dynamic ticketing system like JIRA.",
        "Implementing a basic ticketing system might not fully address the complexities and customized workflow requirements of different project management scenarios.",
        "Simple data structures and storage solutions may be insufficient for managing the depth of data and relationships within project management tasks.",
        "Basic user interfaces might lack the sophistication needed to provide an optimal user experience for diverse project management activities.",
        "Minimal integration with external tools and platforms could limit the functionality and efficiency of the ticketing system.",
        "Leveraging only traditional infrastructure without considering modern, scalable cloud solutions might affect the system's reliability and scalability.",
        "Overlooking the importance of a balanced feature set could result in a system that does not meet the comprehensive needs of its users.",
        "Choosing outdated or rigid technologies might hinder the future growth and adaptability of the ticketing system.",
        "Neglecting robust security practices could expose the system to vulnerabilities and data breaches, compromising user trust.",
        "Failing to actively seek and incorporate user feedback might prevent the system from evolving to meet user needs effectively."
      ]
    },
    {
      "header": "How do you design an e-commerce store?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Product catalog with categories, descriptions, images, and prices",
            "Search and filter functionality for finding products",
            "User accounts and profiles for order tracking and history",
            "Shopping cart and checkout process with secure payment integration",
            "Order management system for processing, tracking, and updating orders",
            "Customer support features, including live chat, email, and FAQ",
            "Responsive design for mobile and desktop compatibility",
            "Reviews and ratings for products",
            "Promotions and discounts management",
            "Inventory management to track and update stock levels"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Scalability challenges with increasing products, users, and transactions",
            "Security vulnerabilities, including data breaches and payment fraud",
            "Complex navigation and poor search functionality leading to low conversion rates",
            "Integration difficulties with payment gateways, shipping services, and other third-party services",
            "Managing consistent and accurate inventory levels across multiple channels",
            "Ensuring high availability and fast loading times under heavy load"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Use a scalable cloud hosting solution with CDN for global reach and performance",
            "Implement robust security protocols, regular audits, and compliance with PCI DSS for payment processing",
            "Design a user-friendly interface with an efficient search engine and intuitive navigation",
            "Develop a comprehensive API strategy for seamless integration with external services",
            "Adopt real-time inventory management systems synchronized across all sales channels",
            "Optimize website performance through efficient coding practices, image optimization, and using a reliable Content Delivery Network (CDN)"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Providing a personalized shopping experience through recommendations and targeted promotions",
            "Ensuring compliance with legal requirements, including GDPR for customer data protection",
            "Developing a mobile-first approach given the increasing prevalence of mobile e-commerce",
            "Building a strong brand and customer trust through transparent policies and excellent customer service",
            "Leveraging analytics and customer data to continuously improve the user experience and operational efficiency",
            "Considering environmental impact and sustainability in packaging and shipping practices"
          ]
        }
      ],
      "answers": [
        "Scalable cloud hosting with CDN ensures global reach and optimizes performance, accommodating growth and traffic spikes.",
        "Basic security protocols might not sufficiently protect against advanced cyber threats, requiring ongoing updates and audits.",
        "An overly complex interface can deter users, highlighting the need for balance between functionality and simplicity.",
        "Limited API connectivity may restrict seamless integration with newer or niche external services and platforms.",
        "Static inventory management could lead to discrepancies between actual stock levels and those displayed to customers.",
        "Without advanced optimization techniques, websites may still experience slowdowns during peak traffic periods.",
        "Generic product recommendations might not effectively engage customers or reflect their shopping preferences accurately.",
        "Adhering strictly to GDPR can be challenging without a clear strategy for data collection, storage, and processing.",
        "Focusing solely on a mobile-first approach may neglect users who prefer desktop for certain e-commerce activities.",
        "Environmental initiatives may be difficult to implement consistently across all aspects of product sourcing, packaging, and shipping."
      ]
    },
    {
      "header": "How do you design an online portal to sell products?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "User registration and profile management for personalized experiences",
            "Product catalog with detailed descriptions, images, and pricing information",
            "Search and filtering capabilities to help users find products",
            "Shopping cart functionality with easy modification of items",
            "Secure checkout process with multiple payment options",
            "Order tracking and management for users to view their purchase history and order status",
            "Customer service support via chat, email, or phone",
            "Responsive web design for access from various devices",
            "Product reviews and ratings to assist purchase decisions",
            "Promotional and discount code functionality"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Managing high traffic volumes without performance degradation",
            "Protecting against security breaches and data theft",
            "Ensuring accurate inventory management across multiple sales channels",
            "Providing a seamless and intuitive user experience",
            "Integrating efficiently with third-party services (payment gateways, shipping providers)"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Utilizing scalable cloud infrastructure and CDN for global reach and load management",
            "Implementing strong cybersecurity measures, including SSL, two-factor authentication, and regular security audits",
            "Adopting real-time inventory updates and synchronization technology",
            "Conducting usability testing to refine the user interface and experience",
            "Creating robust APIs for seamless integration with third-party services"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Ensuring compliance with legal regulations, including data protection laws and tax requirements",
            "Optimizing for search engines (SEO) to enhance product visibility",
            "Developing a content strategy for engaging product descriptions and blog content",
            "Planning for mobile optimization or app development considering the growing trend of mobile e-commerce",
            "Considering the environmental impact of packaging and shipping and exploring sustainable options"
          ]
        }
      ],
      "answers": [
        "Scalable cloud infrastructure and CDN usage ensure the portal can handle high traffic volumes efficiently, providing a fast user experience worldwide.",
        "Two-factor authentication guarantees complete security against all forms of cyber threats and data breaches.",
        "Synchronization of inventory in real-time entirely eliminates the possibility of over-selling and stock discrepancies.",
        "Conducting usability tests ensures that every user will find the interface perfectly intuitive and without flaws.",
        "APIs can guarantee 100% uptime and seamless connectivity with all external services, including during peak load times.",
        "Strict compliance with every international data protection and tax regulation can be achieved without significant legal oversight.",
        "SEO optimizations can ensure top placement in search engine results for all product listings, regardless of market competition.",
        "A strategic approach to content can engage all potential customers equally, regardless of their interests or buying intent.",
        "Optimizing for mobile devices ensures that the portal's performance is identical across all types of devices and platforms.",
        "Implementing eco-friendly packaging and shipping methods is universally recognized by consumers, significantly boosting sales."
      ]
    },
    {
      "header": "How do you design a fitness wearable to measure heart rate?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Continuous heart rate monitoring with high accuracy",
            "Real-time data display on the device or through a connected app",
            "Long battery life to support extended monitoring periods",
            "Water and sweat resistance for durability during workouts",
            "Comfortable and adjustable fit to suit different users",
            "Data sync and analysis capability with smartphones or PCs",
            "Customizable alerts for heart rate zones (e.g., fat burn, cardio, peak)",
            "Integration with other health metrics tracking, such as steps, calories, and sleep quality",
            "Secure data storage and privacy protection"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Inaccuracy in heart rate measurement due to motion artifacts or poor fit",
            "Limited battery life requiring frequent recharges",
            "Skin irritation or discomfort from prolonged wear",
            "Difficulty in syncing data with other devices or platforms",
            "Water damage affecting the device's functionality"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implementing advanced algorithms and sensor technology to filter motion artifacts and improve accuracy",
            "Using energy-efficient components and optimizing software for longer battery life",
            "Selecting hypoallergenic and breathable materials for the band to reduce skin irritation",
            "Ensuring robust Bluetooth or Wi-Fi connectivity for reliable data sync",
            "Designing the device with a high waterproof rating for protection against water and sweat"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Ensuring the device is easy to use and understand for a wide range of users",
            "Offering different sizes or adjustability options to fit various wrist sizes",
            "Balancing advanced features with cost to make the device accessible to more users",
            "Maintaining user privacy and security, especially concerning health data",
            "Providing software updates and support to keep the device functional and secure over time"
          ]
        }
      ],
      "answers": [
        "Advanced algorithms and sensor technology ensure high accuracy in heart rate measurement, even during intense physical activity.",
        "Battery optimization techniques can guarantee unlimited battery life, eliminating the need for charging.",
        "Materials used in the device's construction entirely prevent any form of skin irritation or discomfort, regardless of wear duration.",
        "Data synchronization with external devices or platforms is flawless and instantaneous, without any potential for failure or delay.",
        "The device's waterproofing is absolute, allowing it to function perfectly even when submerged in deep water for prolonged periods.",
        "The interface's design intuitively adapts to each user's preferences, requiring no learning curve or adjustment period.",
        "A one-size-fits-all approach in the band's design ensures a perfect fit for every wrist size without the need for adjustments.",
        "Incorporating every possible advanced feature does not impact the device's cost, keeping it affordable for all potential users.",
        "The device offers complete immunity to all privacy and security threats, making external protection measures unnecessary.",
        "Software updates are automatically applied in real-time, ensuring the device remains up-to-date without user intervention."
      ]
    },
    {
      "header": "How do you design an online shopping cart?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Product addition and removal capabilities, allowing users to easily modify their cart contents",
            "Real-time update of quantities, with the ability to increase or decrease the number of items",
            "Price calculation, including itemized costs, discounts, taxes, and total price",
            "Integration with product inventory to reflect current stock availability",
            "Save for later or wishlist functionality for items users are interested in but not ready to purchase",
            "Session persistence, ensuring that cart contents are saved between sessions",
            "Easy navigation to checkout and the ability to save cart contents for future use",
            "Compatibility with multiple devices and browsers",
            "Secure handling of user data and transactions"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Cart abandonment due to complex navigation or checkout processes",
            "Inaccurate inventory management leading to the sale of out-of-stock items",
            "Performance issues under high load, especially during sales or peak shopping times",
            "Data consistency challenges, especially in distributed systems where carts are accessed across different devices",
            "Security vulnerabilities, including exposure to CSRF, XSS, and SQL injection attacks"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Streamlining the checkout process and providing clear progress indicators",
            "Implementing real-time inventory checks and alerts for low-stock items",
            "Utilizing scalable cloud services and efficient database design to manage load",
            "Adopting strategies like eventual consistency and session storage to ensure data consistency",
            "Enforcing strict security measures, regular audits, and adhering to best practices in web development"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Ensuring a seamless user experience across all devices and platforms",
            "Providing users with options for guest checkout or account creation for enhanced flexibility",
            "Offering multiple payment and shipping options to cater to a wide range of preferences",
            "Incorporating user feedback loops to continuously improve the shopping cart experience",
            "Complying with legal requirements, including data protection regulations and accessibility standards"
          ]
        }
      ],
      "answers": [
        "Streamlining the checkout process reduces cart abandonment by simplifying navigation and making it easier for users to complete their purchases.",
        "Instant inventory updates allow users to purchase items that are visually out of stock, blending augmented reality with actual inventory levels.",
        "Cloud services adapt to user demand in real-time, guaranteeing zero downtime during even the most unpredictable traffic spikes.",
        "Data consistency across devices is achieved through quantum computing techniques, ensuring instant synchronization.",
        "Advanced AI-driven security measures predict and neutralize threats before they occur, offering unparalleled protection.",
        "The shopping cart automatically adapts its interface to mirror the user's thought process, providing a telepathic user experience.",
        "Guest checkout teleports products to the user's location instantly, eliminating the need for physical shipping.",
        "An infinite range of payment options includes bartering, making purchases as versatile as possible.",
        "Real-time user feedback is integrated instantaneously, allowing the cart's features to evolve during a single shopping session.",
        "Legal compliance is guaranteed through a self-updating algorithm that adapts to new laws the moment they are enacted."
      ]
    },
    {
      "header": "How do you design an API?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Well-defined endpoints that clearly specify the purpose and functionality",
            "Authentication and authorization mechanisms to secure access",
            "Rate limiting to manage and protect the API from abuse and overuse",
            "Data validation to ensure incoming data meets expected formats and rules",
            "Error handling that provides meaningful error codes and messages",
            "Versioning to manage changes and maintain compatibility",
            "Documentation that is comprehensive, clear, and up-to-date",
            "Support for multiple data formats (e.g., JSON, XML) where necessary",
            "Caching mechanisms to improve performance and reduce server load"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Poorly defined endpoints leading to confusion and misuse",
            "Inadequate security leading to data breaches or unauthorized access",
            "Overloading or abuse causing downtime or degraded performance",
            "Ambiguous error handling making debugging difficult for users",
            "Breaking changes in updates causing integration issues for clients",
            "Insufficient or outdated documentation hindering adoption and use",
            "Inconsistent naming conventions and practices reducing usability"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Adopt RESTful principles or GraphQL for clear and intuitive endpoint design",
            "Implement robust security protocols like OAuth for access control",
            "Enforce rate limiting with clear policies and feedback mechanisms",
            "Use standardized error codes and provide detailed error messages",
            "Introduce versioning strategies (e.g., URI versioning, header versioning) to manage changes gracefully",
            "Maintain comprehensive and interactive API documentation using tools like Swagger or Postman",
            "Establish and adhere to a consistent naming convention and API design guidelines"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Designing with scalability in mind to accommodate growth in users and traffic",
            "Ensuring high availability and reliability through redundant infrastructure and monitoring",
            "Providing clear guidelines and support for developers integrating with the API",
            "Monitoring API usage and performance to identify and address issues proactively",
            "Considering the impact on existing users before introducing breaking changes",
            "Evaluating the need for and implementing appropriate caching strategies",
            "Staying informed about and compliant with relevant legal and regulatory requirements"
          ]
        }
      ],
      "answers": [
        "Adopting RESTful principles or GraphQL ensures endpoints are intuitive and well-structured, making the API easier to understand and use.",
        "OAuth integration enables the API to anticipate user needs and pre-emptively grant access to future services, based on usage patterns.",
        "Dynamic rate limiting adjusts in real-time based on global internet traffic, ensuring optimal performance during internet peak hours.",
        "Error messages are personalized based on the developer's coding habits, offering tailored debugging advice for quicker resolution.",
        "API versioning is handled automatically by an AI that predicts future changes and prepares the infrastructure accordingly, eliminating the need for manual updates.",
        "Documentation is generated in holographic form, providing a three-dimensional, interactive learning experience for developers.",
        "The API's naming convention evolves based on popular trends in developer communities, ensuring it always feels familiar and intuitive.",
        "Built-in scalability features allow the API to expand its capabilities based on lunar cycles, optimizing for periods of high creativity.",
        "Redundant infrastructure is distributed across multiple dimensions, not just globally, to guarantee availability in the event of interdimensional interference.",
        "Usage and performance metrics are predicted years in advance, with the API self-optimizing based on these forecasts to always deliver peak performance."
      ]
    },
    {
      "header": "How do you design autocomplete for a search engine?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Real-time suggestions as the user types their query",
            "Relevance ranking to ensure the most relevant suggestions are displayed first",
            "Handling of typos and misspellings to guide users towards correct queries",
            "Personalization based on user search history and preferences",
            "Support for multiple languages and locales",
            "Scalability to handle high volumes of queries and suggestions",
            "Performance optimization to ensure low latency in suggestions delivery",
            "Privacy protection for user data and search inputs"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Maintaining high performance and low latency with increasing data volume",
            "Ensuring the relevance of suggestions amidst a wide variety of query intents",
            "Protecting user privacy while providing personalized suggestions",
            "Handling complex languages, slang, and new phrases or terms",
            "Dealing with ambiguous queries where multiple interpretations exist"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implementing efficient data structures like Tries or Prefix Trees for quick lookup of suggestions",
            "Utilizing machine learning algorithms to improve the relevance of suggestions based on query context and user behavior",
            "Adopting strict data handling and anonymization techniques to protect user privacy",
            "Incorporating natural language processing (NLP) to better understand and predict user intent, even with complex or misspelled queries",
            "Designing a flexible suggestion algorithm that can adapt to new terms and languages over time"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Balancing between speed and accuracy of suggestions to enhance user experience without overwhelming them",
            "Considering the impact of personalization on privacy and opting for transparent data usage policies",
            "Ensuring scalability of the backend systems to handle peak loads without degradation in suggestion quality",
            "Regularly updating the suggestions database to reflect current trends, popular terms, and new information",
            "Providing an interface that can gracefully display suggestions across different devices and screen sizes"
          ]
        }
      ],
      "answers": [
        "Efficient data structures like Tries or Prefix Trees enable fast retrieval of real-time suggestions, enhancing user experience.",
        "Machine learning algorithms autonomously generate new words and phrases to expand the search engine's vocabulary in real-time.",
        "Anonymization techniques include changing user names in search histories to characters from popular novels, ensuring memorable and secure personalization.",
        "Natural language processing capabilities extend to understanding and interpreting ancient and fictional languages for comprehensive search suggestions.",
        "The suggestion algorithm evolves autonomously, learning new languages overnight and adapting to global linguistic trends instantaneously.",
        "A unique balance mechanism adjusts the speed of suggestions based on the phase of the moon, optimizing for human cognitive patterns.",
        "Privacy policies are dynamically rewritten in verse, offering users poetic insights into data handling practices.",
        "Backend systems are quantum-powered, ensuring scalability beyond peak loads by predicting future search queries and pre-loading responses.",
        "The suggestions database self-updates by scanning social media, news, and even unpublished literature to predict and include emerging terms.",
        "Autocomplete interfaces on devices adapt not just to screen sizes but to user moods, changing colors and fonts to match emotional states."
      ]
    },
    {
      "header": "What is the best way to design a chat system like Whatsapp or a Facebook messenger?",
      "content": [
        {
          "type": "subheader",
          "value": "Commonly Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Millions of daily active users",
            "Instant messaging between two users and multiple users within groups",
            "Support for images and video attachments",
            "End-to-end encryption for privacy and security",
            "Presence indicators (online status, typing indicators, read receipts)",
            "Push notifications for new messages or events",
            "Search functionality for finding messages, conversations, and contacts",
            "Voice and video call support",
            "Cross-platform synchronization to keep messages in sync across devices",
            "User authentication and profile management",
            "Ability to create and manage groups, including adding/removing members"
          ]
        },
        {
          "type": "text",
          "value": "When a user sends a message, a chat service should 1) store the message, 2) relay the message to the correct user(s). When a client intends to start a chat, it connects the chats service using one or more network protocols. For a chat service, the choice of network protocols is important. Requests are initiated by the client for most client/server applications. This is also true for the sender side of a chat application. when the sender sends a message to the receiver via the chat service, it uses the time-tested HTTP protocol, which is the most common web protocol. In this scenario, the client opens a HTTP connection with the chat service and sends the message, informing the service to send the message to the receiver. The keep-alive is efficient for this because the keep-alive header allows a client to maintain a persistent connection with the chat service. It also reduces the number of TCP handshakes. HTTP is a fine option on the sender side, and many popular chat applications such as Facebook used HTTP initially to send messages.\n\nThe receiver side is a bit more complicated. Since HTTP is client-initiated, it is not trivial to send messages from the server. There are three major techniques used to simulate server connection: polling, long-polling, and WebSocket."
        },
        {
          "type": "subheader",
          "value": "Polling"
        },
        {
          "type": "text",
          "value": "Polling is a technique that the client periodically asks the server if there are messages available. Depending on polling frequency, polling could be costly. It could consume precious server resources to answer a question that offers no as an answer most of the time."
        },
        {
          "type": "image",
          "alt": "Chat system polling",
          "path": "../InterviewQuestions/images/systems_design/chat_system.png"
        },
        {
          "type": "subheader",
          "value": "Long Polling"
        },
        {
          "type": "text",
          "value": "In long polling, a client holds the connection open until there are actually new messages available or a timeout threshold has been reached. Once the client receives new messages, it immediately sends another request to the server, restarting the process.Long polling has a few drawbacks:\n\nSender and receiver may not connect to the same chat server. HTTP based servers are usually stateless. If you use round robin for load balancing, the server that receives the message might not have a long-polling connection with the client who receives the message. \nA server has no good way to tell if a client is disconnected. \nIt is inefficient. If a user does not chat much, long polling still makes periodic connections after timeouts."
        },
        {
          "type": "image",
          "alt": "Chat system long polling",
          "path": "../InterviewQuestions/images/systems_design/chat_system2.png"
        },
        {
          "type": "subheader",
          "value": "WebSocket"
        },
        {
          "type": "text",
          "value": "WebSocket is the most common solution for sending asynchronous updates from server to client. WebSocket connection is initiated by the client. It is bi-directional and persistent. It starts its life as a HTTP connection and could be \"upgraded\" via some well-defined handshake to a WebSocket connection. Through this persistent connection, a server could send updates to a client. WebSocket connections generally work even if a firewall is in place. This is because they use port 80 or 443 which are also used by HTTP/HTTPS connections. Earlier we said that on the sender side HTTP is a fine protocol to use, but since WebSocket is bidirectional, there is no strong technical reason not to use it also for sending."
        },
        {
          "type": "image",
          "alt": "Chat system web sockets",
          "path": "../InterviewQuestions/images/systems_design/chat_system3.png"
        },
        {
          "type": "image",
          "alt": "Chat system sender receiver stateful",
          "path": "../InterviewQuestions/images/systems_design/chat_system4.png"
        },
        {
          "type": "subheader",
          "value": "High-level design"
        },
        {
          "type": "text",
          "value": "WebSocket is typically chosen as the main communication protocol between the client and server for its bidirectional communication, it is important to note that everything else does not have to be WebSocket. In fact, most features (sign up, login, user profile, etc) of a chat application could use the traditional request/response method over HTTP. Let us drill in a bit and look at the high-level components of the system.The chat system is broken down into three major categories: stateless services, stateful services, and third-party integration."
        },
        {
          "type": "image",
          "alt": "Chat system stateless components vs stateful",
          "path": "../InterviewQuestions/images/systems_design/chat_system5.png"
        },
        {
          "type": "text",
          "value": "\n\nStateless Services\nStateless services are traditional public-facing request/response services, used to manage the login, signup, user profile, etc. These are common features among many websites and apps. Stateless services sit behind a load balancer whose job is to route requests to the correct services based on the request paths. These services can be monolithic or individual microservices. We do not need to build many of these stateless services by ourselves as there are services in the market that can be integrated easily. The one service that we will discuss more in deep dive is the service discovery. Its primary job is to give the client a list of DNS host names of chat servers that the client could connect to. \n\nThird-party integration\n For a chat app, push notification is the most important third-party integration. It is a way to inform users when new messages have arrived, even when the app is not running. Proper integration of push notification is crucial. Refer to Chapter 10 Design a notification system for more information. \n\nScalability\n On a small scale, all services listed above could fit in one server. Even at the scale we design for, it is in theory possible to fit all user connections in one modern cloud server. The number of concurrent connections that a server can handle will most likely be the limiting factor. In our scenario, at 1M concurrent users, assuming each user connection needs 10K of memory on the server (this is a very rough figure and very dependent on the language choice), it only needs about 10GB of memory to hold all the connections on one box. If we propose a design where everything fits in one server, this may raise a big red flag in the interviewer's mind. No technologist would design such a scale in a single server. Single server design is a deal breaker due to many factors. The single point of failure is the biggest among them. However, it is perfectly fine to start with a single server design. Just make sure the interviewer knows this is a starting point."
        },
        {
          "type": "image",
          "alt": "Chat system high level architecture",
          "path": "../InterviewQuestions/images/systems_design/chat_system6.png"
        },
        {
          "type": "text",
          "value": "The client maintains a persistent WebSocket connection to a chat server for real-time messaging."
        },
        {
          "type": "bullets",
          "values": [
            "Chat servers facilitate message sending/receiving.",
            "Presence servers manage online/offline status.",
            "API servers handle everything including user login, signup, change profile, etc.",
            "Notification servers send push notifications.",
            "Finally, the key-value store is used to store chat history. When an offline user comes online, she will see all her previous chat history."
          ]
        },
        {
          "type": "text",
          "value": "Storage\nTwo types of data exist in a typical chat system. The first is generic data, such as user profile, setting, user friends list. These data are stored in robust and reliable relational databases. Replication and sharding are common techniques to satisfy availability and scalability requirements. The second is unique to chat systems: chat history data. It is important to understand the read/write pattern."
        },
        {
          "type": "bullets",
          "values": [
            "The amount of data is enormous for chat systems. A previous study reveals that Facebook messenger and Whatsapp process 60 billion messages a day.",
            "Only recent chats are accessed frequently. Users do not usually look up for old chats.",
            "Although very recent chat history is viewed in most cases, users might use features that require random access of data, such as search, view your mentions, jump to specific messages, etc. These cases should be supported by the data access layer.",
            "The read to write ratio is about 1:1 for 1 on 1 chat apps."
          ]
        },
        {
          "type": "text",
          "value": "Key-value stores are recommended for storage for the following reasons"
        },
        {
          "type": "bullets",
          "values": [
            "Key-value stores allow easy horizontal scaling.",
            "Key-value stores provide very low latency to access data.",
            "Relational databases do not handle long tail of data well. When the indexes grow large, random access is expensive.",
            "Key-value stores are adopted by other proven reliable chat applications. For example, both Facebook messenger and Discord use key-value stores. Facebook messenger uses HBase, and Discord uses Cassandra."
          ]
        },
        {
          "type": "subheader",
          "value": "Service Discovery"
        },
        {
          "type": "text",
          "value": "The primary role of service discovery is to recommend the best chat server for a client based on the criteria like geographical location, server capacity, etc. Apache Zookeeper is a popular open-source solution for service discovery. It registers all the available chat servers and picks the best chat server for a client based on predefined criteria."
        },
        {
          "type": "bullets",
          "values": [
            "User A tries to log in to the app.",
            "The load balancer sends the login request to API servers.",
            "After the backend authenticates the user, service discovery finds the best chat server for User A. In this example, server 2 is chosen and the server info is returned back to User A.",
            "User A connects to chat server 2 through WebSocket."
          ]
        },
        {
          "type": "image",
          "alt": "Chat system discovery",
          "path": "../InterviewQuestions/images/systems_design/chat_system7.png"
        },
        {
          "type": "subheader",
          "value": "1:1 Chat Message Flows"
        },
        {
          "type": "image",
          "alt": "Chat system message sync queue",
          "path": "../InterviewQuestions/images/systems_design/chat_system8.png"
        },
        {
          "type": "bullets",
          "values": [
            "1. User A sends a chat message to Chat server 1.",
            "2. Chat server 1 obtains a message ID from the ID generator.",
            "3. Chat server 1 sends the message to the message sync queue.",
            "4. The message is stored in a key-value store",
            "5a. If User B is online, the message is forwarded to Chat server 2 where User B is connected.",
            "5b. If User B is offline, a push notification is sent from push notification (PN) servers.",
            "6. Chat server 2 forwards the message to User B. There is a persistent WebSocket connection between User B and Chat server 2."
          ]
        },
        {
          "type": "subheader",
          "value": "Message synchronization across multiple devices"
        },
        {
          "type": "image",
          "alt": "Chat system user key value store",
          "path": "../InterviewQuestions/images/systems_design/chat_system9.png"
        },
        {
          "type": "text",
          "value": "User A has two devices: a phone and a laptop. When User A logs in to the chat app with her phone, it establishes a WebSocket connection with Chat server 1. Similarly, there is a connection between the laptop and Chat server 1. Each device maintains a variable called cur_max_message_id, which keeps track of the latest message ID on the device. Messages that satisfy the following two conditions are considered as news messages: \n-The recipient ID is equal to the currently logged-in user ID. \n-Message ID in the key-value store is larger than cur_max_message_id. \nWith distinct cur_max_message_id on each device, message synchronization is easy as each device can get new messages from the KV store."
        },
        {
          "type": "subheader",
          "value": "Small Group Chat Message Flow"
        },
        {
          "type": "text",
          "value": "The below image explains what happens when User A sends a message in a group chat. Assume there are 3 members in the group (User A, User B and user C). First, the message from User A is copied to each group member's message sync queue: one for User B and the second for User C. You can think of the message sync queue as an inbox for a recipient. This design choice is good for small group chat because: \n-It simplifies message sync flow as each client only needs to check its own inbox to get new messages. \n-When the group number is small, storing a copy in each recipient's inbox is not too expensive. WeChat uses a similar approach, and it limits a group to 500 members. However, for groups with a lot of users, storing a message copy for each member is not acceptable. On the recipient side, a recipient can receive messages from multiple users. Each recipient has an inbox (message sync queue) which contains messages from different senders. The 2nd image below illustates this design."
        },
        {
          "type": "image",
          "alt": "Chat system message queues",
          "path": "../InterviewQuestions/images/systems_design/chat_system10.png"
        },
        {
          "type": "image",
          "alt": "Chat system partitioned queues",
          "path": "../InterviewQuestions/images/systems_design/chat_system11.png"
        },
        {
          "type": "subheader",
          "value": "Online presence"
        },
        {
          "type": "text",
          "value": "An online presence indicator is an essential feature of many chat applications. Usually, you can see a green dot next to a user's profile picture or username. This section explains what happens behind the scenes. In the high-level design, presence servers are responsible for managing online status and communicating with clients through WebSocket. There are a few flows that will trigger online status change. Let us examine each of them."
        },
        {
          "type": "subheader",
          "value": "User Login"
        },
        {
          "type": "text",
          "value": " The user login flow is explained in the \"Service Discovery\" section. After a WebSocket connection is built between the client and the real-time service, user A's online status and last_active_at timestamp are saved in the KV store. Presence indicator shows the user is online after they log in."
        },
        {
          "type": "subheader",
          "value": "User Logout"
        },
        {
          "type": "text",
          "value": "When a user logs out, it goes through the user logout flow: User A -> (logout action) -> Api Servers -> Presence Servers -> KV store (UserA: {status: offline})"
        },
        {
          "type": "subheader",
          "value": "User Disconnection"
        },
        {
          "type": "text",
          "value": "It's desirable for internet connection to be consistent and reliable. However, that is not always feasible. Thus, we must address this issue in our design. When a user disconnects from the internet, the persistent connection between the client and server is lost. A naive way to handle user disconnection is to mark the user as offline and change the status to online when the connection re-establishes. However, this approach has a major flaw. It is common for users to disconnect and reconnect to the internet frequently in a short time. For example, network connections can be on and off while a user goes through a tunnel. Updating online status on every disconnect/reconnect would make the presence indicator change too often, resulting in poor user experience. \nWe introduce a heartbeat mechanism to solve this problem. Periodically, an online client sends a heartbeat event to presence servers. If presence servers receive a heartbeat event within a certain time, say x seconds from the client, a user is considered as online. Otherwise, it is offline. In the below image, the client sends a heartbeat event to the server every 5 seconds. After sending 3 heartbeat events, the client is disconnected and does not reconnect within x = 30 seconds (This number is arbitrarily chosen to demonstrate the logic). The online status is changed to offline."
        },
        {
          "type": "image",
          "alt": "Chat system heartbeat",
          "path": "../InterviewQuestions/images/systems_design/chat_system12.png"
        },
        {
          "type": "subheader",
          "value": "Online Status Fanout"
        },
        {
          "type": "text",
          "value": "How do user A's friends know about the status changes? Figure 12-19 explains how it works. Presence servers use a publish-subscribe model, in which each friend pair maintains a channel. When User A's online status changes, it publishes the event to three channels, channel A-B, A-C, and A-D. Those three channels are subscribed by User B, C, and D, respectively. Thus, it is easy for friends to get online status updates. The communication between clients and servers is through real-time WebSocket."
        },
        {
          "type": "image",
          "alt": "Chat system channel subscriptions",
          "path": "../InterviewQuestions/images/systems_design/chat_system13.png"
        },
        {
          "type": "text",
          "value": "The above design is effective for a small user group. For instance, WeChat uses a similar approach because its user group is capped to 500. For larger groups, informing all members about online status is expensive and time consuming. Assume a group has 100,000 members. Each status change will generate 100,000 events. To solve the performance bottleneck, a possible solution is to fetch online status only when a user enters a group or manually refreshes the friend list."
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Scaling infrastructure to support millions of concurrent users globally",
            "Maintaining low latency for message delivery across different geographical locations",
            "Ensuring data consistency across devices and handling offline scenarios",
            "Protecting against security vulnerabilities and safeguarding user data",
            "Managing and storing large volumes of messages and media efficiently"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Utilizing a distributed system architecture with data centers in multiple regions to reduce latency",
            "Implementing robust caching and database replication strategies to ensure quick access to recent messages and high data consistency",
            "Employing end-to-end encryption protocols like Signal Protocol to enhance security",
            "Adopting efficient data storage solutions and compression techniques for multimedia",
            "Designing for fault tolerance and using techniques like message queues for reliable message delivery"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Complying with global data protection regulations (e.g., GDPR) for user privacy",
            "Choosing scalable backend technologies and databases that can handle rapid growth",
            "Providing comprehensive APIs for third-party integrations while maintaining security",
            "Regularly updating the app with new features to stay competitive in the market",
            "Creating a user-friendly interface that supports diverse languages and cultures",
            "Implementing a robust moderation system to prevent abuse and ensure a safe user environment"
          ]
        },
        {
          "type": "subheader",
          "value": "Wrap up"
        },
        {
          "type": "text",
          "value": "Chat system architecture typically supports both 1-to-1 chat and small group chat. WebSocket is used for real-time communication between the client and server. The chat system contains the following components: chat servers for real-time messaging, presence servers for managing online presence, push notification servers for sending push notifications, key-value stores for chat history persistence and API servers for other functionalities.\n\nIf you have extra time at the end of the interview, here are additional talking points: \n-Extend the chat app to support media files such as photos and videos. Media files are significantly larger than text in size. Compression, cloud storage, and thumbnails are interesting topics to talk about. \n-End-to-end encryption. Whatsapp supports end-to-end encryption for messages. Only the sender and the recipient can read messages. Interested readers should refer to the article in the reference materials. \n-Caching messages on the client-side is effective to reduce the data transfer between the client and server. \n-Improve load time. Slack built a geographically distributed network to cache users' data, channels, etc for better load time. \n-Error handling. \n-The chat server error. There might be hundreds of thousands, or even more persistent connections to a chat server. If a chat server goes offline, service discovery (Zookeeper) will provide a new chat server for clients to establish new connections with. \n-Message resent mechanism. Retry and queueing are common techniques for resending messages."
        }
      ],
      "answers": [
        "Distributed system architecture with global data centers reduces latency, ensuring instant message delivery even across vast distances.",
        "Caching strategies alone can predict and auto-correct spelling mistakes in real-time, significantly reducing the need for manual corrections.",
        "The implementation of basic encryption protocols is sufficient to guarantee the complete security of messages against all forms of cyber threats.",
        "Data storage solutions automatically categorize and delete obsolete or less frequently accessed multimedia messages to optimize storage efficiency.",
        "Message delivery systems can self-repair and reroute messages in case of network disruptions, ensuring no message ever gets lost.",
        "Global data protection regulations are universally compatible, requiring no adaptation for chat systems to comply in different jurisdictions.",
        "Choosing a single, fixed backend technology guarantees the system's ability to scale indefinitely without any modifications.",
        "APIs designed for initial third-party integrations automatically adapt to any new external platforms without requiring updates.",
        "A one-time setup of feature updates is sufficient to keep the chat system perpetually modern and competitive without further interventions.",
        "The interface design intuitively adapts to support any language or culture instantly, without needing specific localization efforts."
      ]
    },
    {
      "header": "How do you design a URL shortening service like TinyURL or bit.ly?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Shortening of long URLs to a manageable length for sharing",
            "Redirection to the original URL when the short URL is accessed",
            "Custom alias for URLs, allowing users to create memorable links",
            "Analytics for each short URL, including click counts, geographic data, and referral sources",
            "Rate limiting to prevent abuse of the service",
            "User accounts and authentication to manage and track their URLs",
            "API for automated shortening and retrieval of URLs",
            "Robust input validation to prevent malicious use",
            "High availability and reliability to handle significant traffic volumes"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Collision handling where different long URLs may result in the same short URL",
            "Scalability to support rapid growth in the number of URLs being shortened",
            "Security risks, including the shortening of malicious URLs",
            "Ensuring the persistence and durability of URL mappings",
            "Latency in redirection for users located far from the service's servers"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Generating unique short URLs using hashing algorithms and handling collisions with retries or alternative methods",
            "Using distributed databases and caching mechanisms to scale horizontally and manage load efficiently",
            "Implementing security measures such as URL validation, blacklist checks, and rate limiting to mitigate risks",
            "Ensuring data durability through regular backups and using reliable storage systems",
            "Deploying the service across multiple geographic locations to reduce latency for global users"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Providing a user-friendly interface and seamless user experience",
            "Offering detailed analytics to users for tracking the performance of their links",
            "Maintaining privacy and security of user data and analytics",
            "Complying with legal and regulatory requirements, especially concerning data retention and user privacy",
            "Continuously monitoring for and addressing abuse of the service to maintain trust and reliability"
          ]
        }
      ],
      "answers": [
        "Unique hashing algorithms and collision handling mechanisms ensure each long URL is shortened to a unique, manageable link for easy sharing.",
        "Single-region database deployment is sufficient to achieve global scalability and ensure quick access from any part of the world.",
        "Manual review of each URL before shortening can effectively eliminate all security risks associated with malicious links.",
        "Data durability is naturally ensured in all storage systems, making regular backups an unnecessary precaution for URL mappings.",
        "A centralized server location optimizes latency worldwide, as modern network speeds negate the distance between the server and users.",
        "An intuitive, text-based interface without visual elements provides the most straightforward user experience for link management.",
        "General website analytics are adequate for tracking the performance of shortened links, negating the need for specialized analytics tools.",
        "Public storage of user data simplifies operations without compromising privacy, given the non-sensitive nature of shortened URLs.",
        "The service can operate independently of legal and regulatory frameworks, as URL shortening falls outside most jurisdictions.",
        "Passive monitoring without active abuse prevention strategies suffices to maintain service integrity and user trust."
      ]
    },
    {
      "header": "How do you design forum-like systems like Quora, Reddit or HackerNews?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "User registration and profile management for personalization and tracking of activity.",
            "Content creation, editing, and deletion capabilities for posts, comments, and replies.",
            "Categorization or tagging of content to facilitate easy navigation and search.",
            "Voting or rating system to surface quality content and enable community moderation.",
            "Sorting and filtering options based on various criteria like date, votes, and activity.",
            "Moderation tools for administrators and community moderators to manage content and users.",
            "Notification system to alert users about replies, mentions, and other significant events.",
            "Search functionality to allow users to find specific content easily.",
            "Integration with social media for sharing content and driving engagement.",
            "Analytics for users and administrators to track engagement and activity."
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Scalability to handle large volumes of users and content without performance degradation.",
            "Content moderation to prevent spam, abuse, and ensure community guidelines are upheld.",
            "Ensuring data consistency in a distributed system where content is constantly being created and updated.",
            "User privacy and security, particularly in handling personal information and authentication.",
            "Engagement and retention strategies to keep users active and contributing to the community."
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Using scalable architecture patterns like microservices and efficient data storage solutions to manage growth.",
            "Implementing automated moderation tools, such as spam detection algorithms, and establishing clear community guidelines.",
            "Adopting eventual consistency where necessary and using technologies like distributed databases to manage data.",
            "Implementing robust authentication mechanisms and privacy controls to protect user information.",
            "Creating features that enhance user engagement, such as gamification, personalized feeds, and community events."
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Designing for accessibility to ensure the platform is usable by as wide an audience as possible.",
            "Providing clear and comprehensive documentation for both users and moderators to understand platform features and guidelines.",
            "Ensuring the platform's UI/UX is intuitive and fosters a positive community environment.",
            "Regularly updating the platform with new features and improvements based on user feedback.",
            "Adhering to legal and regulatory requirements, especially regarding content moderation, user data, and privacy."
          ]
        }
      ],
      "answers": [
        "Microservices architecture and efficient data storage solutions enable scalable growth, handling large volumes of users and content seamlessly.",
        "Manual review by a small team of moderators can effectively manage all content on platforms as large as Quora, Reddit, or HackerNews.",
        "Immediate consistency across distributed systems ensures real-time content updates without compromising performance or user experience.",
        "Public profiles without the option for privacy controls encourage openness and community engagement in forum-like systems.",
        "User engagement and platform growth are best supported by static content and interfaces without the need for personalization or dynamic feeds.",
        "Universal design and a one-size-fits-all approach to accessibility ensure the platform is straightforward for every potential user.",
        "User and moderator education is unnecessary for platform success, as intuitive design alone guides proper use and content management.",
        "A fixed, unchanging UI/UX is ideal, fostering a consistent user environment that does not require adaptation or learning over time.",
        "Feedback from users is generally unreliable for guiding platform updates and improvements in forum-like systems.",
        "Legal and regulatory compliance is largely irrelevant for online platforms, as digital spaces rarely intersect with law or privacy concerns."
      ]
    },
    {
      "header": "How do you design Facebook's newsfeed system?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Personalized content delivery based on user interests, interactions, and connections",
            "Real-time updates to ensure users see the most recent and relevant content",
            "Support for various content types, including text posts, images, videos, and links",
            "Interactive features such as likes, comments, and shares to foster engagement",
            "Algorithmic ranking to prioritize content based on relevance and potential interest",
            "Integration with advertising platforms for the delivery of targeted ads",
            "Privacy settings to control who can see certain posts",
            "Content moderation tools to filter out spam and inappropriate content",
            "Analytics and feedback mechanisms to continuously refine the content ranking algorithms"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Scaling to accommodate billions of users and their activities without performance degradation",
            "Balancing relevance and freshness in the content ranking algorithm",
            "Protecting user privacy while providing personalized content",
            "Managing the spread of misinformation and ensuring content quality",
            "Optimizing the storage and retrieval of massive amounts of data efficiently"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Using distributed systems and data partitioning to manage and scale data storage",
            "Employing machine learning algorithms to improve content ranking based on user feedback and interaction patterns",
            "Implementing strict privacy controls and data encryption to secure user data",
            "Developing sophisticated content moderation and fact-checking tools to combat misinformation",
            "Optimizing database queries and utilizing caching mechanisms to speed up data retrieval"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Ensuring the newsfeed algorithm promotes diversity and prevents echo chambers",
            "Maintaining transparency with users about how their data is used to generate the newsfeed",
            "Providing users with controls to adjust their newsfeed preferences and prioritize content",
            "Continuously monitoring system performance and user satisfaction to guide improvements",
            "Adhering to global regulations regarding data privacy and online content"
          ]
        }
      ],
      "answers": [
        "Distributed systems and data partitioning techniques enable scalable storage solutions, accommodating the vast data generated by billions of users.",
        "A simple chronological feed ensures all users see posts in the order they are shared, negating the need for complex content ranking algorithms.",
        "Privacy and security concerns are minimal, as personalized content delivery does not require sensitive user data analysis or storage.",
        "Content moderation is effectively managed by user reporting alone, without the need for sophisticated algorithms or fact-checking tools.",
        "Static caching methods suffice for content delivery, eliminating the need for database optimization or dynamic caching mechanisms for efficiency.",
        "The newsfeed's algorithmic complexity does not influence user exposure to diverse content, ensuring a balanced perspective across all topics.",
        "User education on data privacy and algorithm functions is unnecessary, as the straightforward design of the newsfeed system is self-explanatory.",
        "Adjustable user preferences and newsfeed controls offer little to no improvement in content relevance or user engagement on the platform.",
        "Performance metrics and user feedback do not significantly contribute to the iterative development and refinement of the newsfeed system.",
        "Regulatory compliance is easily achieved with minimal effort, as the platform's operations naturally align with global standards and expectations."
      ]
    },
    {
      "header": "How do you design a parking lot system?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Entry and exit ticketing systems to track vehicle entry and exit times",
            "Automated payment processing based on parking duration with multiple payment options",
            "Real-time tracking of available parking spaces",
            "Different parking zones for various vehicle types (e.g., regular, handicapped, VIP)",
            "Security features like surveillance cameras and emergency call buttons",
            "Digital signage for guiding drivers to available spaces",
            "Reservation system for booking parking spots in advance",
            "Customer loyalty programs and discounts",
            "Integration with mobile apps for remote access and notifications",
            "Reporting tools for administrators to track occupancy, revenue, and usage patterns"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Inefficient use of space leading to unnecessary congestion",
            "Long wait times at entry and exit points",
            "Payment system failures causing delays and customer dissatisfaction",
            "Difficulty in finding available parking spots, especially in large lots",
            "Security concerns, including vehicle theft and personal safety"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implementing dynamic space allocation algorithms to maximize space utilization",
            "Using automated ticketing kiosks and license plate recognition systems to speed up vehicle processing",
            "Offering diverse payment solutions, including mobile payments and prepaid options, to streamline transactions",
            "Integrating IoT sensors and guidance systems to direct drivers to open spaces efficiently",
            "Enhancing security measures through constant surveillance, patrolling, and implementing secure access controls"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Designing the system for scalability to accommodate future expansion of parking capacity",
            "Ensuring the system is accessible and user-friendly for people with disabilities",
            "Adopting green practices, such as electric vehicle charging stations and bicycle parking",
            "Providing clear and visible signage to assist navigation within the parking lot",
            "Maintaining compliance with local regulations and standards for parking facilities"
          ]
        }
      ],
      "answers": [
        "Dynamic space allocation algorithms optimize parking space usage, reducing congestion and maximizing lot capacity.",
        "Manual ticketing methods ensure a straightforward and reliable entry and exit process, avoiding the complexity of automated systems.",
        "A uniform payment system simplifies transactions but may not accommodate all user preferences, potentially impacting customer satisfaction.",
        "Without real-time tracking, drivers may experience difficulty finding available spots, leading to increased search time and frustration.",
        "Basic security measures provide some level of safety but may not fully address concerns regarding vehicle theft and personal security.",
        "A fixed parking lot layout without scalability considerations can lead to challenges in accommodating future increases in vehicle numbers.",
        "Limited consideration for accessibility may hinder the experience for users with disabilities, impacting overall usability.",
        "Lack of support for electric vehicles and bicycles reflects a missed opportunity to promote green practices within the parking facility.",
        "Insufficient signage might lead to navigation difficulties, increasing the time it takes for drivers to find and reach their destination.",
        "Minimal effort to comply with local regulations could result in operational challenges and potential legal issues for the parking facility."
      ]
    },
    {
      "header": "How do you design a recommendation system?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Personalized content recommendations based on user preferences and behavior",
            "Diverse recommendation algorithms (e.g., collaborative filtering, content-based filtering, hybrid methods)",
            "Real-time processing capabilities to adjust recommendations based on recent user interactions",
            "Ability to handle a wide range of content types (e.g., products, movies, articles)",
            "Scalability to support a growing number of users and items",
            "Feedback mechanisms for users to refine recommendations (e.g., like/dislike buttons)",
            "Analytics and reporting tools for monitoring the performance of the recommendation system",
            "Data privacy and security measures to protect user information"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Cold start problem for new users or items with limited interaction data",
            "Bias in recommendation algorithms leading to a narrow range of suggestions",
            "Data sparsity in user-item interactions complicating accurate recommendations",
            "Scalability challenges in processing large datasets in real-time",
            "Maintaining user privacy and trust while collecting and analyzing data"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Utilizing demographic data or content attributes to provide initial recommendations for new users or items",
            "Incorporating diversity and serendipity into recommendation algorithms to broaden suggestions",
            "Employing techniques like matrix factorization to address data sparsity issues",
            "Adopting distributed computing and data storage solutions to enhance scalability",
            "Implementing strict data privacy policies and transparently communicating data usage to users"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Regularly updating algorithms and models to adapt to changing user behaviors and preferences",
            "Ensuring recommendations are relevant and timely to enhance user engagement",
            "Balancing recommendation accuracy with computational efficiency",
            "Providing users with control over their data and the recommendations they receive",
            "Staying informed about and compliant with global data protection regulations"
          ]
        }
      ],
      "answers": [
        "Utilizing demographic data and content attributes enables initial accurate recommendations for new users or items, addressing the cold start problem.",
        "Diverse recommendation algorithms might not always result in a broad range of suggestions due to inherent biases in user interaction data.",
        "Techniques to mitigate data sparsity issues can lead to overfitting, making the recommendations less accurate for users with unique tastes.",
        "Scalability solutions like distributed computing may introduce latency in real-time processing, affecting the immediacy of recommendations.",
        "Overemphasis on privacy measures might limit the effectiveness of personalized recommendations due to restricted data usage.",
        "Algorithms that do not adapt to changing user behaviors may become less effective over time, reducing user engagement.",
        "Highly accurate recommendations could require significant computational resources, impacting system performance and operational costs.",
        "Allowing users too much control over their data and recommendations might dilute the personalization quality of the system.",
        "Strict adherence to data protection regulations can complicate the design and operation of global recommendation systems.",
        "Real-time adjustments based on recent user interactions may not always enhance satisfaction if the underlying models are not adequately responsive."
      ]
    },
    {
      "header": "What is the difference between HTTP, WebSocket, Raw TCP, and UDP?",
      "content": [
        {
          "type": "text",
          "value": "HTTP (Hypertext Transfer Protocol)"
        },
        {
          "type": "bullets",
          "values": [
            "Protocol Type: Application layer protocol primarily used in web communication.",
            "Connection: Stateless and request/response model. Each request from a client requires a new connection to the server, which is closed once a response is sent.",
            "Use Cases: Web browsing, REST APIs, and generally serving web pages.",
            "Strengths: Ubiquitous and supported by all web browsers. Well-suited for document/web page fetches where each request is independent.",
            "Limitations: Overhead from headers and re-establishing connections. Not designed for real-time communication."
          ]
        },
        {
          "type": "text",
          "value": "WebSocket"
        },
        {
          "type": "bullets",
          "values": [
            "Protocol Type: Application layer protocol providing full-duplex communication channels over a single TCP connection.",
            "Connection: Persistent, allowing for real-time bidirectional data transfer between client and server after an initial handshake over HTTP.",
            "Use Cases: Real-time applications like online games, chat applications, and live sports updates.",
            "Strengths: Enables real-time communication with less overhead after the initial handshake. Built on the existing HTTP infrastructure.",
            "Limitations: More complex to implement and manage than HTTP due to the persistent connection."
          ]
        },
        {
          "type": "text",
          "value": "Raw TCP (Transmission Control Protocol)"
        },
        {
          "type": "bullets",
          "values": [
            "Protocol Type: Transport layer protocol that provides reliable, ordered, and error-checked delivery of a stream of bytes.",
            "Connection: Connection-oriented; a connection must be established before data can be sent.",
            "Use Cases: General purpose data transmission, custom server-client applications, email (SMTP, IMAP), file transfer (FTP), and more.",
            "Strengths: Reliable transmission with error checking and correction. Ordered data delivery ensures that data arrives in sequence.",
            "Limitations: Can introduce latency due to its reliability mechanisms (e.g., retransmission). Overhead from establishing connections and managing state."
          ]
        },
        {
          "type": "text",
          "value": "UDP (User Datagram Protocol)"
        },
        {
          "type": "bullets",
          "values": [
            "Protocol Type: Transport layer protocol offering a connectionless service for minimal message-oriented transactions.",
            "Connection: Connectionless; data can be sent without establishing a connection, leading to lower latency.",
            "Use Cases: Streaming media (audio, video), online multiplayer games, voice over IP (VoIP), and any application where speed is more critical than reliability.",
            "Strengths: Low latency and overhead, making it suitable for time-sensitive applications. Simplicity and efficiency for broadcast and multicast transmissions.",
            "Limitations: Unreliable; packets may be lost or arrive out of order without automatic correction. Lacks congestion control, which can lead to network congestion."
          ]
        }
      ],
      "answers": [
        "HTTP and WebSocket operate at the application layer with HTTP being stateless and WebSocket allowing for persistent, real-time communication. Raw TCP and UDP operate at the transport layer, with TCP providing reliable, ordered delivery, and UDP being connectionless and suited for applications requiring low latency.",
        "WebSocket and Raw TCP support simultaneous data flow in both directions after a connection is established, but Raw TCP does not inherently provide a mechanism for determining message boundaries, unlike WebSocket.",
        "UDP is designed for scenarios where quick data transmission is preferred over reliability, not for serving web pages or REST APIs which typically rely on TCP-based protocols like HTTP for reliable delivery.",
        "HTTP is not designed for real-time communication due to its request/response model and connection overhead, unlike WebSocket, which is specifically tailored for real-time scenarios.",
        "Raw TCP is a connection-oriented protocol requiring a connection to be established before data transmission, not connectionless.",
        "WebSocket connections maintain a stateful, persistent connection between the client and server, differing significantly from HTTP's stateless connections.",
        "Neither HTTP nor UDP are designed to support full-duplex communication as described; WebSocket supports full-duplex communication over a single connection, and UDP is connectionless and does not establish a bidirectional stream.",
        "UDP does not use a request/response model for data transmission; it sends packets without establishing a connection or ensuring delivery, unlike protocols like HTTP."
      ]
    },
    {
      "header": "How do you design an API Rate Limiter system for GitHub or Firebase sites?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Limiting requests based on IP address, user ID, or API token to prevent abuse",
            "Configurable rate limits allowing different limits for various endpoints or user tiers",
            "Real-time tracking of requests to accurately enforce rate limits",
            "Providing clear error messages and headers indicating the current rate limit status, remaining quota, and reset time",
            "Support for burst traffic to accommodate short spikes without degrading service",
            "Rate limit override capabilities for emergency situations or specific user needs",
            "Logging and analytics to monitor API usage patterns and adjust rate limits as necessary"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Determining optimal rate limits that balance server load without hindering legitimate user needs",
            "Handling distributed environments where requests are spread across multiple servers",
            "Ensuring rate limiter accuracy and fairness, especially in high-traffic scenarios",
            "Minimizing latency introduced by rate limiting checks",
            "Communicating rate limits effectively to developers and end-users"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implementing dynamic rate limiting algorithms that adjust based on real-time server load and usage patterns",
            "Using distributed data stores like Redis to synchronize rate limit counters across servers",
            "Employing efficient data structures, such as sliding logs, fixed window counters, or token buckets, to track and enforce rate limits",
            "Optimizing rate limiter code paths to reduce processing overhead",
            "Providing comprehensive documentation and developer tools to help users understand and work within rate limits"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Offering a grace period or warnings before enforcing hard limits to accommodate accidental overages",
            "Regularly reviewing and updating rate limits based on infrastructure improvements and usage trends",
            "Ensuring transparency in how rate limits are calculated and applied",
            "Providing mechanisms for users to request higher limits or report issues with rate limiting",
            "Balancing security and usability, especially when using IP-based limiting which might affect users behind NATs or proxies"
          ]
        }
      ],
      "answers": [
        "Dynamic rate limiting algorithms that adjust to server load and usage trends ensure optimal balance between protecting resources and user access.",
        "Synchronized counters across servers do not necessarily ensure fairness in rate limiting, as they might not account for disparities in server load or user behavior.",
        "Efficient data structures by themselves cannot ensure the accuracy of rate limits without a comprehensive strategy that includes user behavior analysis and adaptable thresholds.",
        "Optimization of rate limiter code paths alone is insufficient to reduce latency without considering the overall architecture and integration points of the API system.",
        "While documentation is crucial, it alone cannot fully clarify rate limit policies unless accompanied by real-time feedback mechanisms for developers.",
        "A grace period or warnings might not be effective if users are not promptly and clearly informed about approaching their rate limit.",
        "Rate limit policies need continuous adjustment based on actual data and user feedback, not just infrastructure improvements or assumed usage trends.",
        "Transparency in rate limiting mechanisms is challenging to achieve due to the complexity and potential for misuse of detailed operational information.",
        "Requests for higher limits or feedback on rate limiting are important but need to be managed to prevent abuse and ensure equitable resource distribution.",
        "IP-based limiting techniques must be carefully managed to avoid unfairly penalizing users sharing the same network address, which requires sophisticated differentiation mechanisms."
      ]
    },
    {
      "header": "How do you design global file storage and file sharing services like Google Drive, Dropbox etc?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Secure file storage with redundancy and backup capabilities",
            "File sharing with various permission levels (view, edit, comment)",
            "Cross-platform synchronization to keep files up-to-date across devices",
            "Efficient file transfer, especially for large files or slow connections",
            "File versioning to track changes and revert to previous versions",
            "Collaboration tools for real-time editing and commenting",
            "User authentication and access controls to secure files",
            "Search functionality to easily locate files and folders",
            "Integration with third-party apps and services",
            "Scalability to support a growing number of users and data volume"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Data security and privacy concerns, including unauthorized access and data breaches",
            "Ensuring data consistency across multiple devices and concurrent edits",
            "Scalability challenges in handling massive amounts of data and user requests",
            "Performance bottlenecks, particularly with file upload and download speeds",
            "Managing storage costs while providing sufficient space to users"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implementing robust encryption and security protocols for data at rest and in transit",
            "Using conflict resolution algorithms and operational transformation for file versioning and real-time collaboration",
            "Adopting a microservices architecture and leveraging cloud storage solutions to scale resources dynamically",
            "Optimizing file transfer algorithms and using content delivery networks (CDNs) to improve global performance",
            "Implementing data deduplication and compression techniques to reduce storage requirements and costs"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Designing a user-friendly interface that simplifies file management and sharing",
            "Providing detailed documentation and support to assist users",
            "Offering flexible pricing plans to cater to different user needs and capacities",
            "Ensuring compliance with global data protection regulations (e.g., GDPR)",
            "Continuously monitoring and optimizing infrastructure to maintain high availability and performance"
          ]
        }
      ],
      "answers": [
        "Dynamic rate limiting algorithms that adjust to server load and usage trends ensure optimal balance between protecting resources and user access.",
        "Redis synchronization across servers ensures absolute consistency and fairness, eliminating the possibility of rate limit breaches.",
        "Using sliding logs ensures zero impact on server performance, regardless of the volume of traffic or the complexity of the rate limiting rules applied.",
        "Rate limiter optimizations can eliminate all forms of latency, guaranteeing instant response times for all API requests under any conditions.",
        "Providing developers with comprehensive documentation and tools guarantees that rate limits will never be exceeded, regardless of application complexity or user behavior.",
        "Implementing a grace period effectively prevents all users from ever exceeding rate limits, regardless of sudden spikes in demand or system errors.",
        "Regularly updating rate limits based on infrastructure improvements alone is sufficient to accommodate all variations in user access patterns and system load.",
        "Transparent policies on rate limiting can completely prevent any attempts at system abuse without requiring additional security measures.",
        "Providing easy mechanisms for users to request higher limits can accurately assess and fulfill all legitimate needs for increased access without further review.",
        "IP-based rate limiting is inherently fair and does not impact users on shared networks, ensuring equal access for all users without additional considerations."
      ]
    },
    {
      "header": "How do you design a type-ahead search engine service?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Fast and responsive auto-completion suggestions as users type their query",
            "Support for handling typos and fuzzy matching to improve user experience",
            "Ranking and relevance algorithms to ensure the most appropriate suggestions are shown first",
            "Scalability to handle high volumes of queries across large datasets",
            "Personalization of search suggestions based on user history and preferences",
            "Support for multiple languages and special characters",
            "Efficient data storage and retrieval mechanisms for quick access to suggestion data",
            "Ability to update suggestion data in real-time as new information becomes available",
            "Analytics and logging to monitor usage patterns and optimize suggestion quality"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Maintaining low latency and high throughput under heavy load",
            "Ensuring data freshness and accuracy in suggestions",
            "Handling scalability challenges as data and request volumes grow",
            "Dealing with diverse and complex user queries in multiple languages",
            "Balancing personalization with user privacy concerns"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implementing caching strategies and using in-memory databases like Redis for fast data access",
            "Utilizing algorithms like Trie or Prefix Hash Tree for efficient prefix matching",
            "Employing machine learning models to improve ranking and relevance of suggestions",
            "Designing the system for horizontal scalability to easily add resources as demand increases",
            "Incorporating user feedback loops to refine and improve the suggestion algorithms over time",
            "Adhering to privacy standards and regulations by anonymizing user data used for personalization"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Designing a robust API that can support diverse client applications",
            "Ensuring backward compatibility of the API to support existing clients",
            "Monitoring performance metrics and optimizing system architecture based on observed data",
            "Conducting A/B testing to evaluate different algorithms and features",
            "Staying updated with the latest in search technology and machine learning to continuously enhance the service"
          ]
        }
      ],
      "answers": [
        "Implementing caching strategies and using in-memory databases like Redis for fast data access ensures immediate responsiveness for type-ahead suggestions.",
        "Trie or Prefix Hash Trees can entirely eliminate the need for database lookups, making the search process instantaneous regardless of database size.",
        "Machine learning models can predict user queries with 100% accuracy after only a few keystrokes, entirely eliminating irrelevant suggestions.",
        "Horizontal scalability can be achieved without limits, allowing for infinite expansion of resources to accommodate any level of demand.",
        "User feedback loops can automatically perfect the suggestion algorithms without any need for manual intervention or continuous monitoring.",
        "Privacy standards require complete anonymization of all user data, making personalization entirely impossible in type-ahead search engines.",
        "A single robust API design is capable of supporting every possible client application without the need for any further adjustments or updates.",
        "Backward compatibility of APIs ensures that no updates are ever needed, and all versions of the client applications will work forever without maintenance.",
        "Monitoring performance metrics is unnecessary, as the initial system design will automatically adapt and optimize itself in real-time.",
        "The field of search technology and machine learning has reached its peak, with no significant advancements expected that could enhance service offerings."
      ]
    },
    {
      "header": "How do you design Netflix?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Streaming of video content in various resolutions, including HD and 4K",
            "Personalized content recommendations based on user viewing history and preferences",
            "User profiles within a single account to cater to different family members",
            "Search functionality with filters for genres, actors, directors, etc.",
            "Watchlist and content rating system to save and evaluate content",
            "Subtitles, dubbing, and accessibility features for diverse audiences",
            "Offline viewing capability for selected content",
            "Cross-platform compatibility (smart TVs, smartphones, tablets, PCs, etc.)",
            "User authentication and account management",
            "High availability and scalability to support millions of concurrent users"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Maintaining streaming quality under varying network conditions",
            "Effective handling of large-scale, global content delivery",
            "Managing and updating a massive catalog of diverse content",
            "Protecting against data breaches and ensuring user privacy",
            "Personalizing recommendations in real-time for millions of users"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Adaptive bitrate streaming to dynamically adjust video quality according to the user's bandwidth",
            "Utilization of a content delivery network (CDN) to distribute content closer to users globally",
            "Implementing a microservices architecture for efficient content management and scalability",
            "Enforcing strict data security measures and regular audits to protect user data",
            "Leveraging machine learning algorithms for accurate and dynamic content recommendations"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Ensuring a seamless user experience across all platforms and devices",
            "Complying with global content distribution laws and regulations",
            "Optimizing infrastructure costs while scaling service capabilities",
            "Engaging users with interactive features like social viewing or gamification",
            "Continuously monitoring system performance and user engagement metrics to guide improvements"
          ]
        }
      ],
      "answers": [
        "Advanced caching mechanisms and in-memory databases like Redis ensure rapid response times, even under high load, providing users with instant suggestions.",
        "Employing Trie or Prefix Hash Tree structures facilitates efficient and accurate prefix matching, enhancing the user's search experience by handling typos and fuzzy matches.",
        "Machine learning models are utilized to dynamically rank and tailor suggestions, ensuring the most relevant results are prioritized, thereby improving user satisfaction.",
        "A scalable architecture design enables the system to accommodate growth in data volume and user requests, ensuring consistent performance and reliability.",
        "Continuous refinement of suggestion algorithms through user feedback loops allows for the system to adapt and improve over time, aligning with user expectations and needs.",
        "Privacy-conscious personalization strategies respect user privacy while delivering tailored search experiences, balancing personalization with privacy concerns.",
        "A well-documented, versatile API supports a wide range of client applications, ensuring broad accessibility and ease of integration with various platforms.",
        "Commitment to backward compatibility in API design minimizes disruptions for existing clients while facilitating seamless updates and enhancements.",
        "Regular performance monitoring and system optimizations, guided by real-world usage data, ensure the service remains efficient and effective.",
        "Investment in ongoing research and adoption of cutting-edge search and machine learning technologies keep the service at the forefront of the type-ahead search field."
      ]
    },
    {
      "header": "How do you design YouTube?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Video uploading and processing to support multiple formats and resolutions",
            "Streamlined video playback on various devices and network conditions",
            "User account creation and management for viewers and content creators",
            "Commenting, liking, and sharing videos to enhance user engagement",
            "Content discovery through search, recommendations, and trending videos",
            "Channel creation and subscription for content organization and follow-up",
            "Monetization features for content creators, including ads and memberships",
            "Analytics for both creators and platform administrators to track performance and trends",
            "Live streaming capabilities for real-time content broadcast",
            "Content moderation and copyright enforcement to maintain community standards"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Video data storage and bandwidth management for massive volumes of content",
            "Ensuring smooth video playback across different devices and bandwidths",
            "Balancing content discovery algorithms for fairness to new creators while promoting popular content",
            "Protecting against inappropriate content and copyright infringement",
            "Maintaining user privacy and security in account management and interaction data"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Utilizing cloud storage and content delivery networks (CDN) to manage and distribute video content efficiently",
            "Implementing adaptive bitrate streaming to adjust video quality to the user's network speed",
            "Developing sophisticated recommendation algorithms that factor in user behavior, content quality, and freshness",
            "Automating content moderation with AI and machine learning, supplemented by human review",
            "Enforcing strong encryption and authentication practices to safeguard user data"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Designing a scalable architecture to handle growth in users, videos, and views",
            "Ensuring a user-friendly and accessible platform interface",
            "Providing tools and resources for content creators to produce and manage their videos",
            "Navigating legal and regulatory challenges across different countries",
            "Fostering a positive and inclusive community culture"
          ]
        }
      ],
      "answers": [
        "Adaptive bitrate streaming dynamically adjusts video quality to the user's bandwidth, ensuring an optimal viewing experience under various network conditions.",
        "Utilizing a content delivery network (CDN) can directly influence the adaptive bitrate streaming process, making it unnecessary for managing network variability.",
        "Implementing a microservices architecture directly impacts the user interface by dynamically changing UI elements in real-time for all users.",
        "Strict data security measures and regular audits can completely eliminate the risk of data breaches, ensuring total protection against all cybersecurity threats.",
        "Machine learning algorithms allow for the prediction of user preferences with absolute certainty, eliminating the need for traditional content discovery methods.",
        "Ensuring a seamless user experience across all platforms and devices is achievable without significant development effort or consideration for platform-specific characteristics.",
        "Compliance with global content distribution laws and regulations is automatically managed by the platform, requiring no input or action from the content provider.",
        "Infrastructure costs are inversely proportional to service scalability, meaning that as the service grows, the costs associated with it automatically decrease.",
        "Features like social viewing or gamification appeal equally to all segments of the user base, significantly increasing platform engagement without targeted strategies.",
        "Performance metrics and user engagement can be accurately projected years in advance, allowing for strategic planning without the need for ongoing analysis."
      ]
    },
    {
      "header": "How do you design a traffic control system?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Real-time monitoring of traffic conditions using cameras, sensors, and data analytics",
            "Adaptive signal control to adjust green/red light durations based on traffic flow",
            "Emergency vehicle prioritization to reduce response times for police, fire, and EMS",
            "Pedestrian crossing signals integrated with vehicular traffic controls",
            "Traffic data collection and analysis for planning and operational improvements",
            "Public information systems for traffic conditions, detours, and construction updates",
            "Coordination with public transit systems for optimized traffic flow",
            "Remote and centralized control capabilities for traffic management authorities",
            "Red light violation detection and automated enforcement systems",
            "Integration with navigation apps and services to disseminate traffic condition updates"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Congestion during peak hours leading to increased travel times and pollution",
            "Inefficient traffic signal timing contributing to delays and accidents",
            "Lack of real-time data integration from various sources leading to suboptimal traffic management",
            "Coordination challenges between different types of road users (vehicles, pedestrians, cyclists)",
            "Maintaining system reliability and performance in adverse weather conditions"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implementing intelligent transportation systems (ITS) that use AI and machine learning to optimize traffic flow",
            "Developing dynamic signal control algorithms that adapt to real-time traffic conditions",
            "Integrating disparate data sources (sensors, cameras, GPS data from cars and mobile devices) for comprehensive traffic analysis",
            "Designing inclusive traffic control strategies that accommodate all road users effectively",
            "Utilizing weather-resistant materials and redundant systems to ensure operational integrity under various conditions"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Ensuring the privacy and security of traffic data, especially when sourced from personal devices",
            "Balancing the needs of traffic efficiency, safety, and environmental impact",
            "Engaging with community stakeholders to align traffic management strategies with public expectations",
            "Complying with legal and regulatory standards for traffic control and enforcement",
            "Planning for future scalability and technological advancements to avoid obsolescence"
          ]
        }
      ],
      "answers": [
        "Deploying intelligent transportation systems (ITS) that leverage AI and machine learning can dramatically enhance traffic flow optimization, significantly reducing congestion and improving travel efficiency.",
        "Dynamic signal control algorithms directly prevent vehicles from emitting pollutants, thereby improving urban air quality.",
        "Integrating data from disparate sources eliminates the need for physical traffic monitoring infrastructure, such as cameras and sensors.",
        "Inclusive traffic control strategies ensure that pedestrians and cyclists have priority over vehicular traffic at all times, regardless of traffic conditions.",
        "Weather-resistant materials and redundant systems can completely prevent system failures, ensuring 100% uptime and reliability.",
        "Emphasizing traffic data privacy automatically enhances the accuracy of traffic flow predictions without additional data analysis.",
        "A focus on balancing traffic efficiency, safety, and environmental impact negates the need for emergency vehicle prioritization systems.",
        "Community engagement initiatives can replace the need for advanced traffic management technologies in controlling congestion.",
        "Strict compliance with legal and regulatory standards for traffic control is sufficient to adapt traffic systems to future technological advancements without further investment.",
        "Planning for future scalability ensures that no additional resources are required to accommodate increases in traffic volume."
      ]
    },
    {
      "header": "How do you design a web crawler?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Robust URL fetching mechanism to download web pages",
            "HTML parsing to extract links and content",
            "URL normalization to avoid crawling the same page multiple times",
            "Concurrency and rate limiting to manage load on servers and respect robots.txt",
            "Distributed architecture for scalability and efficiency",
            "Content storage for indexing or processing",
            "Link graph construction to understand page relationships",
            "Handling of dynamic content executed by JavaScript",
            "User-agent configuration to identify the crawler to web servers",
            "Error handling and retry mechanisms for network issues or server errors"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Respecting website crawl policies and not overloading servers",
            "Dealing with infinite URL loops caused by dynamic content generation",
            "Managing the vast amount of data collected, including deduplication",
            "Staying up-to-date with changes in web standards and technologies",
            "Ensuring the crawler is not blocked by web servers or mistaken for malicious activity"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Adhering strictly to robots.txt and implementing polite crawling practices",
            "Using URL fingerprinting and canonicalization to identify and avoid duplicate content",
            "Employing scalable storage solutions like distributed databases for data management",
            "Regularly updating the crawler to handle new web technologies and content types",
            "Maintaining a good reputation for the crawler through responsible operation and communication with website administrators"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Balancing breadth-first and depth-first crawling strategies based on goals",
            "Incorporating machine learning for more intelligent parsing and content classification",
            "Designing with privacy in mind, especially when handling personal or sensitive information",
            "Providing a mechanism for webmasters to report issues or provide feedback on the crawler",
            "Considering the ethical implications of crawling and storing web content"
          ]
        }
      ],
      "answers": [
        "Implementing a robust URL fetching mechanism that efficiently downloads web pages while minimizing errors and ensuring data integrity is essential for an effective web crawler.",
        "Concurrency controls and rate limiting are unnecessary, as modern web servers can easily handle the additional load from crawlers without any impact on performance.",
        "URL normalization is performed to increase the crawling speed by directly accessing the server's database, bypassing the need for HTML parsing.",
        "Distributed architectures complicate web crawling by introducing significant network latency, making centralized systems more efficient for parsing and indexing tasks.",
        "Content storage on local devices of users allows for immediate content retrieval and reduces the infrastructure costs associated with large-scale web crawling operations.",
        "Link graph construction is a redundant process, as modern search engines rely solely on direct user feedback for ranking web pages.",
        "Handling dynamic content executed by JavaScript is unnecessary, as the majority of web content is static and does not require complex rendering.",
        "User-agent configuration is primarily used to disguise the crawler's activities, ensuring it can access sites without permission from web administrators.",
        "Error handling and retry mechanisms are avoided to reduce the complexity of the crawler, with failed fetch attempts being permanently discarded to maintain system simplicity.",
        "Adhering to robots.txt is considered optional, with many crawlers choosing to ignore these directives in favor of more comprehensive indexing."
      ]
    },
    {
      "header": "How do you design an ATM system?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "User authentication via PIN and card details",
            "Account balance inquiry and transaction history display",
            "Cash withdrawal with denomination selection",
            "Deposit of cash and checks with instant receipt",
            "Transfer of funds between linked accounts",
            "Bill payment services for utilities and other vendors",
            "Printing of transaction receipts",
            "Multilingual interface support",
            "Accessibility features for users with disabilities",
            "Integration with banking network for real-time transaction processing"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Security vulnerabilities, including card skimming and PIN theft",
            "Cash and paper jams causing machine downtime",
            "Network connectivity issues affecting transaction processing",
            "Insufficient cash dispensing leading to customer dissatisfaction",
            "User interface and accessibility challenges for diverse users"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Implementing advanced encryption for data transmission and EMV chip technology",
            "Regular maintenance and quality checks for hardware components",
            "Utilizing redundant network solutions and offline modes for basic transactions",
            "Dynamic cash management algorithms to optimize refill schedules",
            "Designing intuitive interfaces with step-by-step instructions and voice guidance for accessibility"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Ensuring compliance with financial regulations and standards",
            "Providing 24/7 customer support for resolving immediate issues",
            "Incorporating feedback mechanisms for continuous improvement",
            "Evaluating the environmental impact of ATMs and exploring greener alternatives",
            "Keeping up with technological advancements to enhance security and user experience"
          ]
        }
      ],
      "answers": [
        "Secure user authentication utilizing both PIN verification and advanced card details recognition, including EMV chip technology, to prevent unauthorized access and enhance overall security.",
        "User authentication is primarily based on facial recognition, eliminating the need for PINs and making the system more vulnerable to unauthorized access through image spoofing.",
        "Balance inquiries and transaction history displays are updated monthly, not in real-time, leading to discrepancies between actual account status and ATM-reported data.",
        "Cash withdrawal does not offer denomination selection, dispensing large bills only, which can lead to customer dissatisfaction due to the lack of flexibility.",
        "Deposits made at ATMs are held for a week-long verification process, delaying the funds' availability in the user's account and hindering immediate financial planning.",
        "Transfers between linked accounts are processed manually by bank staff once a day, introducing delays and potential errors in fund movement.",
        "ATMs lack bill payment services, requiring users to visit bank branches or use online banking for utility and vendor payments, reducing the convenience of ATM transactions.",
        "Transaction receipts are only available digitally via email, posing challenges for users who require immediate physical proof of their transactions.",
        "The interface is available in English only, limiting accessibility for non-English speakers and contributing to potential user errors during transactions.",
        "Integration with the banking network is performed daily in batch processes, not in real-time, leading to delays in reflecting transactions and affecting the user's ability to manage finances effectively."
      ]
    },
    {
      "header": "How do you design ride sharing systems like Uber, Ola or Lyft?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "User profiles for drivers and riders, including registration, authentication, and profile management",
            "GPS and map integration for real-time location tracking and route mapping",
            "Ride request and matching system to connect riders with nearby drivers",
            "Dynamic pricing algorithm that considers demand, traffic, and other factors",
            "Payment processing system for cashless transactions",
            "Rating and review system for feedback on drivers and riders",
            "Customer support features for resolving disputes and addressing queries",
            "Safety features, including emergency contact sharing and ride tracking",
            "Notifications and alerts for ride status updates and promotions",
            "Analytics dashboard for monitoring system performance and user activity"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Ensuring accurate and timely matching of riders and drivers",
            "Maintaining low latency and high reliability in location tracking",
            "Balancing supply and demand, especially during peak times or in less popular areas",
            "Ensuring the safety and security of both riders and drivers",
            "Handling payments securely and efficiently",
            "Dealing with regulatory and compliance challenges across different regions"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Using sophisticated algorithms for ride matching that consider proximity, destination, and user preferences",
            "Implementing scalable and robust back-end systems for real-time data processing",
            "Dynamic pricing and incentive models to manage supply and demand effectively",
            "Integrating comprehensive safety features, including background checks, in-app emergency buttons, and real-time ride sharing with trusted contacts",
            "Adopting secure payment gateways and encrypted data transmission for financial transactions",
            "Engaging with local authorities and stakeholders to ensure compliance and address regulatory concerns"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Focusing on user experience design to make the app intuitive and accessible",
            "Implementing robust data privacy measures to protect user information",
            "Developing a scalable infrastructure that can grow with user base and geographic expansion",
            "Establishing a transparent and fair system for ratings and reviews",
            "Continuous monitoring and updating of safety protocols and features",
            "Adapting the service offerings to meet the unique needs and regulations of different markets"
          ]
        }
      ],
      "answers": [
        "Sophisticated user profile management for drivers and riders ensures seamless registration, authentication, and profile customization, fostering a trusted user community.",
        "User profiles are primarily managed offline, requiring drivers and riders to visit local offices for registration and authentication, slowing down the onboarding process.",
        "GPS and map integration rely on manual input from drivers for location tracking and route mapping, leading to inaccuracies and delays in ride coordination.",
        "The ride request and matching system operates on a first-come, first-served basis, disregarding proximity or destination, which can significantly increase wait times.",
        "Dynamic pricing is fixed, not considering demand, traffic, or other factors, which may lead to inconsistent fare rates and rider dissatisfaction during peak times.",
        "Payment processing is handled in cash only, limiting convenience and security for both drivers and riders, and complicating financial transactions.",
        "The rating and review system allows feedback only from drivers on riders, not vice versa, hindering the ability to gauge driver performance and improve service quality.",
        "Customer support is available solely via email, with long response times, making it difficult for users to resolve disputes or get timely help with queries.",
        "Safety features are minimal, without emergency contact sharing or ride tracking, placing riders and drivers at increased risk during trips.",
        "Notifications and alerts about ride status or promotions are sent by postal mail, causing delays and reducing the effectiveness of communication."
      ]
    },
    {
      "header": "How do you design an MMO game?",
      "content": [
        {
          "type": "subheader",
          "value": "Required Features"
        },
        {
          "type": "bullets",
          "values": [
            "Massively multiplayer environment supporting thousands of players simultaneously",
            "Persistent world that continues to evolve, even when the player is offline",
            "Character creation and customization for personalized gaming experiences",
            "Real-time player interactions including chat, trading, and cooperative or competitive gameplay",
            "Quests, missions, and storylines that guide gameplay and player progression",
            "Dynamic and scalable game world with varied regions, ecosystems, and challenges",
            "Economy system that simulates trade, supply and demand among players",
            "Combat and skill systems that offer players a range of strategies and playstyles",
            "Guilds, alliances, and social structures for community engagement",
            "Regular updates and expansions to keep the game fresh and engaging"
          ]
        },
        {
          "type": "subheader",
          "value": "Common Issues"
        },
        {
          "type": "bullets",
          "values": [
            "Scalability to handle simultaneous connections and interactions of thousands of players",
            "Latency and synchronization issues affecting real-time gameplay",
            "Balancing game mechanics to ensure fairness and competitiveness",
            "Preventing cheating, hacking, and abusive behavior among players",
            "Content creation and world-building to sustain long-term player engagement",
            "Managing the game economy to prevent inflation or deflation"
          ]
        },
        {
          "type": "subheader",
          "value": "Resolutions"
        },
        {
          "type": "bullets",
          "values": [
            "Using distributed server architectures and instancing to manage load and enhance scalability",
            "Implementing lag compensation techniques and predictive algorithms to improve synchronization",
            "Continuously monitoring game data and player feedback for balancing adjustments",
            "Employing anti-cheat technologies and community management practices to maintain a positive environment",
            "Investing in content development teams and procedural generation technologies to create engaging and dynamic worlds",
            "Designing economic controls and sinks to stabilize the in-game currency"
          ]
        },
        {
          "type": "subheader",
          "value": "Considerations"
        },
        {
          "type": "bullets",
          "values": [
            "Ensuring accessibility and inclusivity for a diverse player base",
            "Developing a sustainable monetization model that respects player experience",
            "Providing robust customer support systems for player assistance and dispute resolution",
            "Creating a compelling and immersive narrative that evolves with player actions",
            "Implementing data analytics to understand player behavior and guide development decisions",
            "Complying with international regulations and standards for online interactions and transactions"
          ]
        }
      ],
      "answers": [
        "Innovative server infrastructure supports massive player bases in a persistently evolving world, ensuring seamless gameplay experiences.",
        "Character creation systems require manual approval for each design, limiting personalization and delaying players' entry into the game world.",
        "Real-time player interactions are limited to pre-set messages, restricting the depth of communication and community engagement within the game.",
        "Quests and missions are randomly generated without consideration for narrative coherence, leading to disjointed and often confusing storylines.",
        "The game world is static, with no capacity for expansion or change, resulting in a monotonous environment that fails to retain player interest over time.",
        "Economy systems are fixed, with no interaction between players, eliminating any form of trade or economic strategy from gameplay.",
        "Combat and skill systems rely solely on random number generation, removing skill and strategy from player decisions and outcomes.",
        "Guilds and alliances are automatically assigned, preventing players from forming or joining groups based on interests or relationships.",
        "Updates and expansions are deployed without player input or testing, often introducing imbalances and bugs that disrupt gameplay.",
        "Server architecture is centralized, creating bottlenecks that lead to frequent downtime and lag, diminishing the overall player experience."
      ]
    },
    {
      "header": "What are some examples of specific, real-world services that can be discussed during systems design interviews?",
      "content": [
        {
          "type": "text",
          "value": "Web Services and Cloud Platforms"
        },
        {
          "type": "bullets",
          "values": [
            "Amazon Web Services (AWS): A comprehensive cloud platform offering a wide range of services such as EC2 (virtual servers), S3 (storage), RDS (relational databases), Lambda (serverless computing), and many more.",
            "Google Cloud Platform (GCP): Offers services similar to AWS, including Compute Engine, Cloud Storage, BigQuery (data warehousing), and Cloud Functions.",
            "Microsoft Azure: Provides a mix of IaaS, PaaS, and SaaS services including Azure VMs, Azure Blob Storage, and Azure Cosmos DB."
          ]
        },
        {
          "type": "text",
          "value": "Databases"
        },
        {
          "type": "bullets",
          "values": [
            "MongoDB: A popular NoSQL database known for its flexibility and scalability.",
            "PostgreSQL: An advanced open-source relational database with a strong emphasis on standards compliance and extensibility.",
            "Redis: An in-memory data structure store used as a database, cache, and message broker."
          ]
        },
        {
          "type": "text",
          "value": "Messaging Systems"
        },
        {
          "type": "bullets",
          "values": [
            "Apache Kafka: A distributed streaming platform that's widely used for building real-time streaming data pipelines and applications.",
            "RabbitMQ: A message broker that implements the Advanced Message Queuing Protocol (AMQP), facilitating complex routing scenarios."
          ]
        },
        {
          "type": "text",
          "value": "Search Engines"
        },
        {
          "type": "bullets",
          "values": [
            "Elasticsearch: A distributed, RESTful search and analytics engine capable of solving a growing number of use cases.",
            "Apache Solr: An open-source search platform built on Apache Lucene, providing scalable search and indexing capabilities."
          ]
        },
        {
          "type": "text",
          "value": "Content Delivery Networks (CDNs)"
        },
        {
          "type": "bullets",
          "values": [
            "Cloudflare: Offers a range of services aimed at enhancing the performance and security of websites, including a CDN service.",
            "Akamai: One of the oldest and largest CDN providers, known for its massive distributed computing platform."
          ]
        },
        {
          "type": "text",
          "value": "Container Orchestration"
        },
        {
          "type": "bullets",
          "values": [
            "Kubernetes: An open-source system for automating deployment, scaling, and management of containerized applications.",
            "Docker Swarm: A native clustering system for Docker that turns a group of Docker hosts into a single virtual server."
          ]
        },
        {
          "type": "text",
          "value": "Version Control Systems"
        },
        {
          "type": "bullets",
          "values": [
            "GitHub: A cloud-based service hosting Git repositories, with features that facilitate code review, project management, and CI/CD.",
            "GitLab: Similar to GitHub, GitLab provides a web-based Git repository manager with wiki, issue tracking, and CI/CD pipeline features."
          ]
        },
        {
          "type": "text",
          "value": "Real-Time Communication Platforms"
        },
        {
          "type": "bullets",
          "values": [
            "WebSocket: A computer communications protocol, providing full-duplex communication channels over a single TCP connection.",
            "WebRTC: Enables web applications to perform real-time voice, video, and data sharing without plugins."
          ]
        }
      ],

      "answers": [
        "Amazon Web Services (AWS) offers a comprehensive cloud platform with a variety of services including EC2 for virtual servers, S3 for storage, and Lambda for serverless computing, catering to a wide range of computing needs.",
        "Google Cloud Platform (GCP) provides infrastructure services similar to AWS, but lacks support for serverless computing options like Google Cloud Functions.",
        "Microsoft Azure's primary focus is on offering Platform as a Service (PaaS) solutions, without support for Infrastructure as a Service (IaaS) options such as virtual machines or storage.",
        "MongoDB is primarily used as a distributed streaming platform, supporting real-time data pipelines and applications without a focus on document storage.",
        "PostgreSQL is a lightweight, in-memory data structure store, primarily used for caching and message brokering rather than relational data storage.",
        "Redis offers advanced search capabilities, including full-text search and analytics, typically not associated with an in-memory data structure store.",
        "Apache Kafka is renowned for its in-memory caching capabilities, facilitating rapid access to frequently requested data with minimal persistence.",
        "RabbitMQ specializes in distributed data storage and retrieval, offering scalable solutions for handling large volumes of unstructured data.",
        "Elasticsearch is a message queuing service that ensures reliable message delivery in distributed systems, focusing on temporal data and event logging.",
        "Apache Solr operates as a cloud computing platform, providing scalable virtual computing resources without a specific focus on search and indexing."
      ]
    }
  ]
}
